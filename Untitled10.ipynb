{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3yJ0sbluB//AXUZKWG2om",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanpharamut/KKU_data_mining/blob/master/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xPcfv5sQDou",
        "outputId": "f4e6a21f-f1dd-40a4-9a1e-727ccca087ee"
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/facebookresearch/consistent_depth.git\n",
        "%cd consistent_depth\n",
        "!git submodule update --init --recursive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'consistent_depth'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 57 (delta 0), reused 52 (delta 0), pack-reused 5\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n",
            "/content/consistent_depth\n",
            "Submodule 'monodepth/mannequin_challenge' (https://github.com/roxanneluo/mannequinchallenge.git) registered for path 'monodepth/mannequin_challenge'\n",
            "Submodule 'monodepth/midas_v2' (https://github.com/roxanneluo/MiDaS-1.git) registered for path 'monodepth/midas_v2'\n",
            "Submodule 'monodepth/monodepth2' (https://github.com/roxanneluo/monodepth2.git) registered for path 'monodepth/monodepth2'\n",
            "Submodule 'third_party/OpticalFlowToolkit' (https://github.com/roxanneluo/OpticalFlowToolkit.git) registered for path 'third_party/OpticalFlowToolkit'\n",
            "Submodule 'third_party/colmap' (https://github.com/colmap/colmap.git) registered for path 'third_party/colmap'\n",
            "Submodule 'flownet2' (https://github.com/roxanneluo/flownet2-pytorch.git) registered for path 'third_party/flownet2'\n",
            "Cloning into '/content/consistent_depth/monodepth/mannequin_challenge'...\n",
            "Cloning into '/content/consistent_depth/monodepth/midas_v2'...\n",
            "Cloning into '/content/consistent_depth/monodepth/monodepth2'...\n",
            "Cloning into '/content/consistent_depth/third_party/OpticalFlowToolkit'...\n",
            "Cloning into '/content/consistent_depth/third_party/colmap'...\n",
            "Cloning into '/content/consistent_depth/third_party/flownet2'...\n",
            "Submodule path 'monodepth/mannequin_challenge': checked out '826c33d4a9f50eb9bfd4b0eba731d893ccbbfa31'\n",
            "Submodule path 'monodepth/midas_v2': checked out '6cae4a6d3198efae398f07220aae4804c2f517ed'\n",
            "Submodule path 'monodepth/monodepth2': checked out '22d073ab881c1e3a8cfcdd147467d80b1d103327'\n",
            "Submodule path 'third_party/OpticalFlowToolkit': checked out '21e7eccd4f979f56ed4dc6e3ba03a5d90edb9bd7'\n",
            "Submodule path 'third_party/colmap': checked out '64d625a996a052cdf4f3f9151e9a5af7b8d6a5e1'\n",
            "Submodule 'doc/_build/html' (https://github.com/colmap/colmap.github.io.git) registered for path 'third_party/colmap/doc/_build/html'\n",
            "Cloning into '/content/consistent_depth/third_party/colmap/doc/_build/html'...\n",
            "Submodule path 'third_party/colmap/doc/_build/html': checked out '751fd602f81d3bb52b45bc3f4757113c18f3cd00'\n",
            "Submodule path 'third_party/flownet2': checked out 'e99b5cc2b8dde129cb0273ff43501da59fb9731e'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbqoNmc0QFSv",
        "outputId": "1e5b032c-afb6-4783-fdd3-b4e7616653a4"
      },
      "source": [
        "!./scripts/download_model.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allexport      \toff\n",
            "braceexpand    \ton\n",
            "emacs          \toff\n",
            "errexit        \ton\n",
            "errtrace       \toff\n",
            "functrace      \toff\n",
            "hashall        \ton\n",
            "histexpand     \toff\n",
            "history        \toff\n",
            "ignoreeof      \toff\n",
            "interactive-comments\ton\n",
            "keyword        \toff\n",
            "monitor        \toff\n",
            "noclobber      \toff\n",
            "noexec         \toff\n",
            "noglob         \toff\n",
            "nolog          \toff\n",
            "notify         \toff\n",
            "nounset        \toff\n",
            "onecmd         \toff\n",
            "physical       \toff\n",
            "pipefail       \toff\n",
            "posix          \toff\n",
            "privileged     \toff\n",
            "verbose        \toff\n",
            "vi             \toff\n",
            "xtrace         \ton\n",
            "++ mkdir -p checkpoints\n",
            "++ gdown 'https://drive.google.com/uc?id=1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da' -O checkpoints/flownet2.pth\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da\n",
            "To: /content/consistent_depth/checkpoints/flownet2.pth\n",
            "650MB [00:07, 83.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38_V7O-9QNDJ",
        "outputId": "f45a041f-3958-4099-b077-82d682a5e550"
      },
      "source": [
        "!./scripts/download_demo.sh results/ayush"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allexport      \toff\n",
            "braceexpand    \ton\n",
            "emacs          \toff\n",
            "errexit        \ton\n",
            "errtrace       \toff\n",
            "functrace      \toff\n",
            "hashall        \ton\n",
            "histexpand     \toff\n",
            "history        \toff\n",
            "ignoreeof      \toff\n",
            "interactive-comments\ton\n",
            "keyword        \toff\n",
            "monitor        \toff\n",
            "noclobber      \toff\n",
            "noexec         \toff\n",
            "noglob         \toff\n",
            "nolog          \toff\n",
            "notify         \toff\n",
            "nounset        \toff\n",
            "onecmd         \toff\n",
            "physical       \toff\n",
            "pipefail       \toff\n",
            "posix          \toff\n",
            "privileged     \toff\n",
            "verbose        \toff\n",
            "vi             \toff\n",
            "xtrace         \ton\n",
            "++ results_dir=results/ayush\n",
            "++ mkdir -p data/videos/\n",
            "++ wget 'https://www.dropbox.com/s/9a2kb7flg3o1eb5/ayush_color.mp4?dl=1' -O data/videos/ayush.mp4\n",
            "--2021-07-15 04:11:46--  https://www.dropbox.com/s/9a2kb7flg3o1eb5/ayush_color.mp4?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/9a2kb7flg3o1eb5/ayush_color.mp4 [following]\n",
            "--2021-07-15 04:11:46--  https://www.dropbox.com/s/dl/9a2kb7flg3o1eb5/ayush_color.mp4\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc19d10a880538a830089852a5d9.dl.dropboxusercontent.com/cd/0/get/BSWYkbTZooo8bmgmUK7uqfOpzKGLMiWUiyzKBhpf4wzw_fkaHj7kIk6Et-hyd45vsBhIrseQMF4SeGW2jCzzvisW22suWf_9yTH3h7GnOFq81MneWb_sZFBG-LgQoosc3KNjRIJGKwYyGiVbxZ429Ws0/file?dl=1# [following]\n",
            "--2021-07-15 04:11:47--  https://uc19d10a880538a830089852a5d9.dl.dropboxusercontent.com/cd/0/get/BSWYkbTZooo8bmgmUK7uqfOpzKGLMiWUiyzKBhpf4wzw_fkaHj7kIk6Et-hyd45vsBhIrseQMF4SeGW2jCzzvisW22suWf_9yTH3h7GnOFq81MneWb_sZFBG-LgQoosc3KNjRIJGKwYyGiVbxZ429Ws0/file?dl=1\n",
            "Resolving uc19d10a880538a830089852a5d9.dl.dropboxusercontent.com (uc19d10a880538a830089852a5d9.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc19d10a880538a830089852a5d9.dl.dropboxusercontent.com (uc19d10a880538a830089852a5d9.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52605435 (50M) [application/binary]\n",
            "Saving to: ‘data/videos/ayush.mp4’\n",
            "\n",
            "data/videos/ayush.m 100%[===================>]  50.17M   197MB/s    in 0.3s    \n",
            "\n",
            "2021-07-15 04:11:48 (197 MB/s) - ‘data/videos/ayush.mp4’ saved [52605435/52605435]\n",
            "\n",
            "++ mkdir -p results/ayush\n",
            "++ wget 'https://www.dropbox.com/s/7mbvu60qbs7hzod/ayush_colmap.zip?dl=1' -O results/ayush/ayush_colmap.zip\n",
            "--2021-07-15 04:11:48--  https://www.dropbox.com/s/7mbvu60qbs7hzod/ayush_colmap.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/7mbvu60qbs7hzod/ayush_colmap.zip [following]\n",
            "--2021-07-15 04:11:48--  https://www.dropbox.com/s/dl/7mbvu60qbs7hzod/ayush_colmap.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8da85981fdee38b320e970738a.dl.dropboxusercontent.com/cd/0/get/BSWBCFq5t6UxfRLd5iwa6cql0W0gP6DaIhA4uFY60hdq2HH9XkfgzyRQaRDQyvtsGY0x-FcwVv-90o2VSsUn1Nlv3KnvKCBoM1qvDi5Qc8GLfGUwm4OV9Q8VKcEiFdNhKMEoGuBTAT0JiwlZKzs5GcEX/file?dl=1# [following]\n",
            "--2021-07-15 04:11:48--  https://uc8da85981fdee38b320e970738a.dl.dropboxusercontent.com/cd/0/get/BSWBCFq5t6UxfRLd5iwa6cql0W0gP6DaIhA4uFY60hdq2HH9XkfgzyRQaRDQyvtsGY0x-FcwVv-90o2VSsUn1Nlv3KnvKCBoM1qvDi5Qc8GLfGUwm4OV9Q8VKcEiFdNhKMEoGuBTAT0JiwlZKzs5GcEX/file?dl=1\n",
            "Resolving uc8da85981fdee38b320e970738a.dl.dropboxusercontent.com (uc8da85981fdee38b320e970738a.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc8da85981fdee38b320e970738a.dl.dropboxusercontent.com (uc8da85981fdee38b320e970738a.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31811430 (30M) [application/binary]\n",
            "Saving to: ‘results/ayush/ayush_colmap.zip’\n",
            "\n",
            "results/ayush/ayush 100%[===================>]  30.34M   173MB/s    in 0.2s    \n",
            "\n",
            "2021-07-15 04:11:49 (173 MB/s) - ‘results/ayush/ayush_colmap.zip’ saved [31811430/31811430]\n",
            "\n",
            "++ unzip results/ayush/ayush_colmap.zip -d results/ayush\n",
            "Archive:  results/ayush/ayush_colmap.zip\n",
            "   creating: results/ayush/colmap_dense/\n",
            "  inflating: results/ayush/colmap_dense/metadata.npz  \n",
            "   creating: results/ayush/__MACOSX/\n",
            "   creating: results/ayush/__MACOSX/colmap_dense/\n",
            "  inflating: results/ayush/__MACOSX/colmap_dense/._metadata.npz  \n",
            "  inflating: results/ayush/__MACOSX/._colmap_dense  \n",
            "   creating: results/ayush/depth_colmap_dense/\n",
            "   creating: results/ayush/depth_colmap_dense/depth/\n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000022.raw  \n",
            "   creating: results/ayush/__MACOSX/depth_colmap_dense/\n",
            "   creating: results/ayush/__MACOSX/depth_colmap_dense/depth/\n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000022.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000036.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000036.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000088.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000088.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000063.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000063.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000077.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000077.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000076.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000076.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000062.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000062.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000089.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000089.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000037.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000037.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000023.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000023.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000009.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000009.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000035.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000035.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000021.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000021.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000048.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000048.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000074.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000074.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000060.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000060.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000061.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000061.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000075.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000075.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000049.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000049.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000020.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000020.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000034.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000034.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000008.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000008.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000030.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000030.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000024.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000024.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000018.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000018.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000071.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000071.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000065.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000065.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000059.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000059.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000058.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000058.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000064.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000064.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000070.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000070.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000019.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000019.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000025.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000025.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000031.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000031.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000027.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000027.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000033.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000033.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000066.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000066.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000072.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000072.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000073.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000073.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000067.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000067.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000032.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000032.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000026.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000026.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000082.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000082.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000069.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000069.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000041.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000041.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000055.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000055.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000028.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000028.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000000.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000000.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000014.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000014.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000015.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000015.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000001.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000001.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000029.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000029.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000054.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000054.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000040.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000040.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000068.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000068.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000083.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000083.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000081.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000081.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000056.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000056.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000042.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000042.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000017.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000017.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000003.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000003.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000002.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000002.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000016.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000016.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000043.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000043.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000057.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000057.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000080.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000080.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000090.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000090.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000084.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000084.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000053.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000053.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000047.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000047.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000012.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000012.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000006.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000006.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000007.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000007.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000013.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000013.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000046.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000046.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000052.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000052.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000085.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000085.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000091.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000091.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000087.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000087.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000044.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000044.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000050.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000050.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000078.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000078.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000005.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000005.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000011.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000011.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000039.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000039.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000038.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000038.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000010.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000010.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000004.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000004.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000079.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000079.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000051.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000051.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000045.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000045.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000086.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000086.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000048.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000048.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000060.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000060.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000074.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000074.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000009.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000009.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000021.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000021.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000035.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000035.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000034.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000034.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000020.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000020.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000008.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000008.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000075.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000075.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000061.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000061.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000049.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000049.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000088.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000088.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000077.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000077.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000063.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000063.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000036.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000036.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000022.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000022.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000023.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000023.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000037.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000037.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000062.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000062.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000076.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000076.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000089.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000089.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000072.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000072.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000066.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000066.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000033.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000033.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000027.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000027.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000026.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000026.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000032.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000032.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000067.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000067.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000073.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000073.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000065.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000065.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000071.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000071.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000059.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000059.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000024.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000024.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000030.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000030.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000018.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000018.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000019.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000019.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000031.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000031.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000025.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000025.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000058.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000058.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000070.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000070.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000064.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000064.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000003.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000003.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000017.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000017.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000081.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000081.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000042.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000042.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000056.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000056.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000057.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000057.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000043.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000043.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000080.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000080.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000016.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000016.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000002.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000002.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000028.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000028.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000014.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000014.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000000.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000000.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000082.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000082.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000069.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000069.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000055.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000055.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000041.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000041.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000040.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000040.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000054.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000054.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000068.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000068.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000083.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000083.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000001.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000001.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000015.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000015.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000029.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000029.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000011.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000011.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000005.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000005.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000039.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000039.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000087.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000087.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000050.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000050.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000044.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000044.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000078.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000078.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000079.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000079.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000045.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000045.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000051.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000051.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000086.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000086.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000038.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000038.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000004.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000004.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000010.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000010.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000006.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000006.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000012.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000012.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000084.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000084.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000090.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000090.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000047.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000047.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000053.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000053.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000052.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000052.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000046.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000046.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000091.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000091.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000085.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000085.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000013.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000013.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000007.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000007.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/._depth  \n",
            "  inflating: results/ayush/__MACOSX/._depth_colmap_dense  \n",
            "++ rm results/ayush/ayush_colmap.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8z3N6KMQKnS",
        "outputId": "afaa53ca-a051-4cb1-dfaa-4aa5273eaab0"
      },
      "source": [
        "!./scripts/install.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.17.2)\n",
            "Collecting opencv-contrib-python==3.4.2.16\n",
            "  Downloading opencv_contrib_python-3.4.2.16-cp36-cp36m-manylinux1_x86_64.whl (30.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.6 MB 22 kB/s \n",
            "\u001b[?25hCollecting torch==1.4.0+cu100\n",
            "  Downloading https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (723.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 723.9 MB 12 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0+cu100\n",
            "  Downloading https://download.pytorch.org/whl/cu100/torchvision-0.5.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.19.2)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 62.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboardX in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (1.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 14)) (4.61.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp36-cp36m-manylinux1_x86_64.whl (35 kB)\n",
            "Collecting wget\n",
            "  Using cached wget-3.2.zip (10 kB)\n",
            "Collecting gdown\n",
            "  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypng\n",
            "  Using cached pypng-0.0.20.tar.gz (649 kB)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/site-packages (from torchvision==0.5.0+cu100->-r requirements.txt (line 7)) (8.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from torchvision==0.5.0+cu100->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.6/site-packages (from h5py->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (2020.9.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.9.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (2.4.7)\n",
            "Collecting decorator<5,>=4.3\n",
            "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (52.0.0.post20210125)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (3.17.3)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.33.0-py2.py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 69.7 MB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 59.9 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 70.4 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
            "Collecting absl-py>=0.4\n",
            "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (2.25.1)\n",
            "Collecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.38.1-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 67.0 MB/s \n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 70.7 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.6.1-py3-none-any.whl (17 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (1.26.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (2021.5.30)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (4.0.0)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard->-r requirements.txt (line 9)) (0.8)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (1.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->-r requirements.txt (line 9)) (3.10.0.0)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.5.0-py3-none-any.whl (5.7 kB)\n",
            "Building wheels for collected packages: wget, gdown, pypng\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=05034245f58924966c6536a93f7c693c3087fe1ddf31b87867102e540d2a9d66\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/1d/93/c863ee832230df5cfc25ca497b3e88e0ee3ea9e44adc46ac62\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9034 sha256=fbc24dcac269f88b8a127845e8e89cfa15a357ff46f2a78b3482c1d990b2d33e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/87/bd/09b16161b149fd6711ac76b5420d78ed58bd6a320e892117c3\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-py3-none-any.whl size=67162 sha256=b83a0ee54b87b692562d438c4f00b8fef35b96a29c6c0a1fa5cd8b8f146258a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/d3/cf/b82186f85e4c9d159bc4233fbd37607e766c241b78b09f1e8f\n",
            "Successfully built wget gdown pypng\n",
            "Installing collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, decorator, werkzeug, torch, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, filelock, absl-py, wget, torchvision, tensorboard, setproctitle, pypng, opencv-contrib-python, gdown, colorama\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.0.9\n",
            "    Uninstalling decorator-5.0.9:\n",
            "      Successfully uninstalled decorator-5.0.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0\n",
            "    Uninstalling torch-1.9.0:\n",
            "      Successfully uninstalled torch-1.9.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.4.2\n",
            "    Uninstalling torchvision-0.4.2:\n",
            "      Successfully uninstalled torchvision-0.4.2\n",
            "Successfully installed absl-py-0.13.0 cachetools-4.2.2 colorama-0.4.4 decorator-4.4.2 filelock-3.0.12 gdown-3.13.0 google-auth-1.33.0 google-auth-oauthlib-0.4.4 grpcio-1.38.1 importlib-metadata-4.6.1 markdown-3.3.4 oauthlib-3.1.1 opencv-contrib-python-3.4.2.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 pypng-0.0.20 requests-oauthlib-1.3.0 rsa-4.7.2 setproctitle-1.2.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 torch-1.4.0+cu100 torchvision-0.5.0+cu100 werkzeug-2.0.1 wget-3.2 zipp-3.5.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "/content/consistent_depth/third_party/flownet2 /content/consistent_depth\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating correlation_cuda.egg-info\n",
            "writing correlation_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to correlation_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to correlation_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'correlation_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c correlation_cuda.cc -o build/temp.linux-x86_64-3.6/correlation_cuda.o -std=c++11 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c correlation_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/correlation_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:328:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:400:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:469:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:748:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:819:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:887:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1177:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1252:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1324:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:343:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:415:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:487:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:781:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:852:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:923:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1228:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1303:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1378:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:344:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:416:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:488:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:782:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:853:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:924:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1229:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1304:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1379:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/correlation_cuda.o build/temp.linux-x86_64-3.6/correlation_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/correlation_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/correlation_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for correlation_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/correlation_cuda.py to correlation_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.correlation_cuda.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.6/site-packages/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting correlation_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n",
            "Adding correlation-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.6/site-packages/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for correlation-cuda==0.0.0\n",
            "Finished processing dependencies for correlation-cuda==0.0.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating resample2d_cuda.egg-info\n",
            "writing resample2d_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to resample2d_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to resample2d_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'resample2d_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c resample2d_cuda.cc -o build/temp.linux-x86_64-3.6/resample2d_cuda.o -std=c++11 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c resample2d_kernel.cu -o build/temp.linux-x86_64-3.6/resample2d_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_forward(at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:173:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:225:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:277:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_backward(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/resample2d_cuda.o build/temp.linux-x86_64-3.6/resample2d_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/resample2d_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/resample2d_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for resample2d_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/resample2d_cuda.py to resample2d_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.resample2d_cuda.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.6/site-packages/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n",
            "Adding resample2d-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.6/site-packages/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for resample2d-cuda==0.0.0\n",
            "Finished processing dependencies for resample2d-cuda==0.0.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating channelnorm_cuda.egg-info\n",
            "writing channelnorm_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to channelnorm_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to channelnorm_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'channelnorm_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c channelnorm_cuda.cc -o build/temp.linux-x86_64-3.6/channelnorm_cuda.o -std=c++11 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c channelnorm_kernel.cu -o build/temp.linux-x86_64-3.6/channelnorm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:366:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:421:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:717:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:771:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1078:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1136:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:368:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:423:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:482:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:549:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:855:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:909:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:967:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1033:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1350:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1408:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1470:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1540:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/channelnorm_cuda.o build/temp.linux-x86_64-3.6/channelnorm_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/channelnorm_cuda.py to channelnorm_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.channelnorm_cuda.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.6/site-packages/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n",
            "Adding channelnorm-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.6/site-packages/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for channelnorm-cuda==0.0.0\n",
            "Finished processing dependencies for channelnorm-cuda==0.0.0\n",
            "/content/consistent_depth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5icPAOAYQPDP",
        "outputId": "1da6547d-5a13-45f8-9dd3-e9ef872c8119"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9675 sha256=8e9d8a96e3f7e32c1795a34361ab4974392153dc3bfaf82871238d1f2759d276\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oFN3IGTQbut",
        "outputId": "bc6d4e3b-6f4f-4684-fc71-d13e6939e233"
      },
      "source": [
        "!pip install pypng"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\r\u001b[K     |▌                               | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 23.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 26.4MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 29.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 26.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 28.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 27.8MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 26.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 92kB 27.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 112kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 163kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 184kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 204kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 225kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 245kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 256kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 276kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 296kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 317kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 327kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 337kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 348kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 358kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 368kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 378kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 389kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 409kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 419kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 430kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 440kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 450kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 460kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 471kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 481kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 491kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 501kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 512kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 522kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 532kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 542kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 552kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 563kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 573kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 583kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 593kB 28.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 604kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 614kB 28.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 624kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 634kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 645kB 28.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 655kB 28.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pypng\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp37-none-any.whl size=67179 sha256=b7c6a04d4bc1ae78db033f7172a19f94605d73552196d8a4dfaecbe2e81d57d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "Successfully built pypng\n",
            "Installing collected packages: pypng\n",
            "Successfully installed pypng-0.0.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q53n1dAqQd_w",
        "outputId": "bc160ed8-8bc4-4edb-9449-f65338ffe528"
      },
      "source": [
        "!which python # should return /usr/local/bin/python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0WEdqaXQfCN",
        "outputId": "57ff4b5f-d120-4508-f46a-ee90b9a5f159"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3fTtaPiQf-Z",
        "outputId": "be998da8-73e8-4711-84e7-188e550a2ad5"
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zt9BzRXQg2i",
        "outputId": "069e2b89-caef-44e0-ec52-f3cf6a969182"
      },
      "source": [
        "%env PYTHONPATH="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrFvhYi8QiWi",
        "outputId": "ea30491b-7ef6-4d02-94ca-060b8b28498a"
      },
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "installation finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2021-07-15 04:13:30--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2021-07-15 04:13:30--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 32.2M 2s\n",
            "    50K .......... .......... .......... .......... ..........  0% 28.3M 2s\n",
            "   100K .......... .......... .......... .......... ..........  0% 41.7M 2s\n",
            "   150K .......... .......... .......... .......... ..........  0% 91.1M 1s\n",
            "   200K .......... .......... .......... .......... ..........  0%  100M 1s\n",
            "   250K .......... .......... .......... .......... ..........  0% 62.0M 1s\n",
            "   300K .......... .......... .......... .......... ..........  0%  108M 1s\n",
            "   350K .......... .......... .......... .......... ..........  0% 98.2M 1s\n",
            "   400K .......... .......... .......... .......... ..........  0% 91.4M 1s\n",
            "   450K .......... .......... .......... .......... ..........  0%  132M 1s\n",
            "   500K .......... .......... .......... .......... ..........  0%  129M 1s\n",
            "   550K .......... .......... .......... .......... ..........  1% 67.8M 1s\n",
            "   600K .......... .......... .......... .......... ..........  1% 98.2M 1s\n",
            "   650K .......... .......... .......... .......... ..........  1%  196M 1s\n",
            "   700K .......... .......... .......... .......... ..........  1%  222M 1s\n",
            "   750K .......... .......... .......... .......... ..........  1% 45.0M 1s\n",
            "   800K .......... .......... .......... .......... ..........  1%  221M 1s\n",
            "   850K .......... .......... .......... .......... ..........  1%  221M 1s\n",
            "   900K .......... .......... .......... .......... ..........  1%  210M 1s\n",
            "   950K .......... .......... .......... .......... ..........  1% 73.9M 1s\n",
            "  1000K .......... .......... .......... .......... ..........  1%  201M 1s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  227M 1s\n",
            "  1100K .......... .......... .......... .......... ..........  2%  224M 1s\n",
            "  1150K .......... .......... .......... .......... ..........  2%  168M 1s\n",
            "  1200K .......... .......... .......... .......... ..........  2% 76.4M 1s\n",
            "  1250K .......... .......... .......... .......... ..........  2%  108M 1s\n",
            "  1300K .......... .......... .......... .......... ..........  2%  213M 1s\n",
            "  1350K .......... .......... .......... .......... ..........  2% 57.4M 1s\n",
            "  1400K .......... .......... .......... .......... ..........  2%  199M 1s\n",
            "  1450K .......... .......... .......... .......... ..........  2%  221M 1s\n",
            "  1500K .......... .......... .......... .......... ..........  2%  133M 1s\n",
            "  1550K .......... .......... .......... .......... ..........  2%  102M 1s\n",
            "  1600K .......... .......... .......... .......... ..........  2%  208M 1s\n",
            "  1650K .......... .......... .......... .......... ..........  2%  209M 1s\n",
            "  1700K .......... .......... .......... .......... ..........  3%  171M 1s\n",
            "  1750K .......... .......... .......... .......... ..........  3%  198M 1s\n",
            "  1800K .......... .......... .......... .......... ..........  3%  192M 1s\n",
            "  1850K .......... .......... .......... .......... ..........  3%  224M 1s\n",
            "  1900K .......... .......... .......... .......... ..........  3%  227M 1s\n",
            "  1950K .......... .......... .......... .......... ..........  3%  178M 1s\n",
            "  2000K .......... .......... .......... .......... ..........  3%  225M 1s\n",
            "  2050K .......... .......... .......... .......... ..........  3%  236M 1s\n",
            "  2100K .......... .......... .......... .......... ..........  3%  231M 1s\n",
            "  2150K .......... .......... .......... .......... ..........  3%  182M 0s\n",
            "  2200K .......... .......... .......... .......... ..........  3%  225M 0s\n",
            "  2250K .......... .......... .......... .......... ..........  4%  227M 0s\n",
            "  2300K .......... .......... .......... .......... ..........  4%  232M 0s\n",
            "  2350K .......... .......... .......... .......... ..........  4%  180M 0s\n",
            "  2400K .......... .......... .......... .......... ..........  4% 34.4M 0s\n",
            "  2450K .......... .......... .......... .......... ..........  4% 89.6M 0s\n",
            "  2500K .......... .......... .......... .......... ..........  4%  109M 0s\n",
            "  2550K .......... .......... .......... .......... ..........  4%  114M 0s\n",
            "  2600K .......... .......... .......... .......... ..........  4%  205M 0s\n",
            "  2650K .......... .......... .......... .......... ..........  4%  224M 0s\n",
            "  2700K .......... .......... .......... .......... ..........  4%  220M 0s\n",
            "  2750K .......... .......... .......... .......... ..........  4%  195M 0s\n",
            "  2800K .......... .......... .......... .......... ..........  4%  208M 0s\n",
            "  2850K .......... .......... .......... .......... ..........  5%  235M 0s\n",
            "  2900K .......... .......... .......... .......... ..........  5%  228M 0s\n",
            "  2950K .......... .......... .......... .......... ..........  5%  200M 0s\n",
            "  3000K .......... .......... .......... .......... ..........  5%  236M 0s\n",
            "  3050K .......... .......... .......... .......... ..........  5%  223M 0s\n",
            "  3100K .......... .......... .......... .......... ..........  5%  202M 0s\n",
            "  3150K .......... .......... .......... .......... ..........  5%  189M 0s\n",
            "  3200K .......... .......... .......... .......... ..........  5%  228M 0s\n",
            "  3250K .......... .......... .......... .......... ..........  5%  217M 0s\n",
            "  3300K .......... .......... .......... .......... ..........  5%  229M 0s\n",
            "  3350K .......... .......... .......... .......... ..........  5%  205M 0s\n",
            "  3400K .......... .......... .......... .......... ..........  6%  236M 0s\n",
            "  3450K .......... .......... .......... .......... ..........  6%  201M 0s\n",
            "  3500K .......... .......... .......... .......... ..........  6%  230M 0s\n",
            "  3550K .......... .......... .......... .......... ..........  6% 39.7M 0s\n",
            "  3600K .......... .......... .......... .......... ..........  6%  128M 0s\n",
            "  3650K .......... .......... .......... .......... ..........  6%  385M 0s\n",
            "  3700K .......... .......... .......... .......... ..........  6%  312M 0s\n",
            "  3750K .......... .......... .......... .......... ..........  6%  342M 0s\n",
            "  3800K .......... .......... .......... .......... ..........  6%  389M 0s\n",
            "  3850K .......... .......... .......... .......... ..........  6%  359M 0s\n",
            "  3900K .......... .......... .......... .......... ..........  6%  322M 0s\n",
            "  3950K .......... .......... .......... .......... ..........  7% 86.5M 0s\n",
            "  4000K .......... .......... .......... .......... ..........  7%  384M 0s\n",
            "  4050K .......... .......... .......... .......... ..........  7%  363M 0s\n",
            "  4100K .......... .......... .......... .......... ..........  7%  379M 0s\n",
            "  4150K .......... .......... .......... .......... ..........  7%  356M 0s\n",
            "  4200K .......... .......... .......... .......... ..........  7%  376M 0s\n",
            "  4250K .......... .......... .......... .......... ..........  7% 23.6M 0s\n",
            "  4300K .......... .......... .......... .......... ..........  7%  221M 0s\n",
            "  4350K .......... .......... .......... .......... ..........  7%  178M 0s\n",
            "  4400K .......... .......... .......... .......... ..........  7%  232M 0s\n",
            "  4450K .......... .......... .......... .......... ..........  7%  231M 0s\n",
            "  4500K .......... .......... .......... .......... ..........  7% 41.7M 0s\n",
            "  4550K .......... .......... .......... .......... ..........  8%  200M 0s\n",
            "  4600K .......... .......... .......... .......... ..........  8%  227M 0s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  234M 0s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  224M 0s\n",
            "  4750K .......... .......... .......... .......... ..........  8%  180M 0s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  226M 0s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  230M 0s\n",
            "  4900K .......... .......... .......... .......... ..........  8%  235M 0s\n",
            "  4950K .......... .......... .......... .......... ..........  8%  196M 0s\n",
            "  5000K .......... .......... .......... .......... ..........  8%  229M 0s\n",
            "  5050K .......... .......... .......... .......... ..........  8%  208M 0s\n",
            "  5100K .......... .......... .......... .......... ..........  9% 69.6M 0s\n",
            "  5150K .......... .......... .......... .......... ..........  9% 54.7M 0s\n",
            "  5200K .......... .......... .......... .......... ..........  9%  224M 0s\n",
            "  5250K .......... .......... .......... .......... ..........  9%  225M 0s\n",
            "  5300K .......... .......... .......... .......... ..........  9%  231M 0s\n",
            "  5350K .......... .......... .......... .......... ..........  9%  198M 0s\n",
            "  5400K .......... .......... .......... .......... ..........  9%  228M 0s\n",
            "  5450K .......... .......... .......... .......... ..........  9%  220M 0s\n",
            "  5500K .......... .......... .......... .......... ..........  9%  234M 0s\n",
            "  5550K .......... .......... .......... .......... ..........  9%  201M 0s\n",
            "  5600K .......... .......... .......... .......... ..........  9% 14.3M 0s\n",
            "  5650K .......... .......... .......... .......... ..........  9%  223M 0s\n",
            "  5700K .......... .......... .......... .......... .......... 10%  224M 0s\n",
            "  5750K .......... .......... .......... .......... .......... 10%  198M 0s\n",
            "  5800K .......... .......... .......... .......... .......... 10%  231M 0s\n",
            "  5850K .......... .......... .......... .......... .......... 10%  224M 0s\n",
            "  5900K .......... .......... .......... .......... .......... 10% 17.9M 0s\n",
            "  5950K .......... .......... .......... .......... .......... 10%  169M 0s\n",
            "  6000K .......... .......... .......... .......... .......... 10%  231M 0s\n",
            "  6050K .......... .......... .......... .......... .......... 10%  238M 0s\n",
            "  6100K .......... .......... .......... .......... .......... 10% 45.4M 0s\n",
            "  6150K .......... .......... .......... .......... .......... 10%  146M 0s\n",
            "  6200K .......... .......... .......... .......... .......... 10% 82.6M 0s\n",
            "  6250K .......... .......... .......... .......... .......... 11%  212M 0s\n",
            "  6300K .......... .......... .......... .......... .......... 11%  232M 0s\n",
            "  6350K .......... .......... .......... .......... .......... 11% 60.0M 0s\n",
            "  6400K .......... .......... .......... .......... .......... 11%  226M 0s\n",
            "  6450K .......... .......... .......... .......... .......... 11%  216M 0s\n",
            "  6500K .......... .......... .......... .......... .......... 11%  234M 0s\n",
            "  6550K .......... .......... .......... .......... .......... 11%  201M 0s\n",
            "  6600K .......... .......... .......... .......... .......... 11%  227M 0s\n",
            "  6650K .......... .......... .......... .......... .......... 11% 29.3M 0s\n",
            "  6700K .......... .......... .......... .......... .......... 11% 63.0M 0s\n",
            "  6750K .......... .......... .......... .......... .......... 11%  169M 0s\n",
            "  6800K .......... .......... .......... .......... .......... 11%  207M 0s\n",
            "  6850K .......... .......... .......... .......... .......... 12%  211M 0s\n",
            "  6900K .......... .......... .......... .......... .......... 12%  220M 0s\n",
            "  6950K .......... .......... .......... .......... .......... 12%  202M 0s\n",
            "  7000K .......... .......... .......... .......... .......... 12%  212M 0s\n",
            "  7050K .......... .......... .......... .......... .......... 12%  241M 0s\n",
            "  7100K .......... .......... .......... .......... .......... 12% 34.2M 0s\n",
            "  7150K .......... .......... .......... .......... .......... 12%  118M 0s\n",
            "  7200K .......... .......... .......... .......... .......... 12%  225M 0s\n",
            "  7250K .......... .......... .......... .......... .......... 12%  229M 0s\n",
            "  7300K .......... .......... .......... .......... .......... 12%  234M 0s\n",
            "  7350K .......... .......... .......... .......... .......... 12%  180M 0s\n",
            "  7400K .......... .......... .......... .......... .......... 13% 20.6M 0s\n",
            "  7450K .......... .......... .......... .......... .......... 13%  231M 0s\n",
            "  7500K .......... .......... .......... .......... .......... 13%  229M 0s\n",
            "  7550K .......... .......... .......... .......... .......... 13%  182M 0s\n",
            "  7600K .......... .......... .......... .......... .......... 13%  235M 0s\n",
            "  7650K .......... .......... .......... .......... .......... 13% 28.1M 0s\n",
            "  7700K .......... .......... .......... .......... .......... 13%  225M 0s\n",
            "  7750K .......... .......... .......... .......... .......... 13% 72.7M 0s\n",
            "  7800K .......... .......... .......... .......... .......... 13%  224M 0s\n",
            "  7850K .......... .......... .......... .......... .......... 13%  230M 0s\n",
            "  7900K .......... .......... .......... .......... .......... 13%  231M 0s\n",
            "  7950K .......... .......... .......... .......... .......... 14%  191M 0s\n",
            "  8000K .......... .......... .......... .......... .......... 14%  221M 0s\n",
            "  8050K .......... .......... .......... .......... .......... 14% 35.9M 0s\n",
            "  8100K .......... .......... .......... .......... .......... 14%  230M 0s\n",
            "  8150K .......... .......... .......... .......... .......... 14% 18.0M 0s\n",
            "  8200K .......... .......... .......... .......... .......... 14%  233M 0s\n",
            "  8250K .......... .......... .......... .......... .......... 14% 79.5M 0s\n",
            "  8300K .......... .......... .......... .......... .......... 14% 77.9M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 14%  125M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 14%  212M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 14%  232M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 14%  230M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 15%  200M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 15%  216M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 15%  234M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 15% 8.78M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 15%  188M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 15%  224M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 15%  233M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 15% 29.1M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 15%  206M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 15%  260M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 15%  222M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 16%  233M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 16%  184M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 16%  227M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 16%  214M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 16%  232M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 16%  202M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 16%  230M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 16%  217M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 16%  211M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 16%  163M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 16%  221M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 16%  240M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 17% 20.4M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 17%  223M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 17%  229M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 17%  232M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 17%  223M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 17%  182M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 17%  232M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 17%  239M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 17%  225M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 17%  213M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 17% 10.1M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 18%  216M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 18%  227M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 18%  197M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 18%  245M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 18% 12.1M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 18%  340M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 18%  315M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 18%  363M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 18%  379M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 18%  377M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 18%  338M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 19%  389M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 19%  296M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 19%  379M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 19%  158M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 19% 53.0M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 19%  317M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 19%  251M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 19%  189M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 19%  223M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 19%  200M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 19%  233M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 19%  196M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 20%  231M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 20%  219M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 20%  232M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 20%  194M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 20%  223M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 20%  238M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 20%  227M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 20% 8.84M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 20%  232M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 20%  226M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 20%  242M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 21%  193M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 21% 50.3M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 21% 32.9M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 21%  225M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 21%  192M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 21%  219M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 21%  232M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 21%  216M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 21%  108M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 21% 75.1M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 21%  190M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 21% 69.8M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 22%  188M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 22%  219M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 22%  230M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 22%  223M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 22%  196M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 22%  213M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 22%  219M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 22%  216M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 22%  209M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 22%  227M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 22%  217M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 23%  225M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 23%  198M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 23%  218M 0s\n",
            " 13250K .......... .......... .......... .......... .......... 23%  215M 0s\n",
            " 13300K .......... .......... .......... .......... .......... 23%  225M 0s\n",
            " 13350K .......... .......... .......... .......... .......... 23%  191M 0s\n",
            " 13400K .......... .......... .......... .......... .......... 23%  231M 0s\n",
            " 13450K .......... .......... .......... .......... .......... 23%  231M 0s\n",
            " 13500K .......... .......... .......... .......... .......... 23% 42.7M 0s\n",
            " 13550K .......... .......... .......... .......... .......... 23%  192M 0s\n",
            " 13600K .......... .......... .......... .......... .......... 23%  220M 0s\n",
            " 13650K .......... .......... .......... .......... .......... 23%  240M 0s\n",
            " 13700K .......... .......... .......... .......... .......... 24%  234M 0s\n",
            " 13750K .......... .......... .......... .......... .......... 24%  172M 0s\n",
            " 13800K .......... .......... .......... .......... .......... 24%  230M 0s\n",
            " 13850K .......... .......... .......... .......... .......... 24% 68.7M 0s\n",
            " 13900K .......... .......... .......... .......... .......... 24%  224M 0s\n",
            " 13950K .......... .......... .......... .......... .......... 24%  185M 0s\n",
            " 14000K .......... .......... .......... .......... .......... 24%  224M 0s\n",
            " 14050K .......... .......... .......... .......... .......... 24%  214M 0s\n",
            " 14100K .......... .......... .......... .......... .......... 24%  222M 0s\n",
            " 14150K .......... .......... .......... .......... .......... 24%  204M 0s\n",
            " 14200K .......... .......... .......... .......... .......... 24%  230M 0s\n",
            " 14250K .......... .......... .......... .......... .......... 25%  215M 0s\n",
            " 14300K .......... .......... .......... .......... .......... 25%  236M 0s\n",
            " 14350K .......... .......... .......... .......... .......... 25%  190M 0s\n",
            " 14400K .......... .......... .......... .......... .......... 25%  231M 0s\n",
            " 14450K .......... .......... .......... .......... .......... 25%  230M 0s\n",
            " 14500K .......... .......... .......... .......... .......... 25%  214M 0s\n",
            " 14550K .......... .......... .......... .......... .......... 25% 61.1M 0s\n",
            " 14600K .......... .......... .......... .......... .......... 25%  217M 0s\n",
            " 14650K .......... .......... .......... .......... .......... 25%  234M 0s\n",
            " 14700K .......... .......... .......... .......... .......... 25%  226M 0s\n",
            " 14750K .......... .......... .......... .......... .......... 25%  189M 0s\n",
            " 14800K .......... .......... .......... .......... .......... 26% 92.3M 0s\n",
            " 14850K .......... .......... .......... .......... .......... 26% 85.5M 0s\n",
            " 14900K .......... .......... .......... .......... .......... 26% 76.0M 0s\n",
            " 14950K .......... .......... .......... .......... .......... 26%  195M 0s\n",
            " 15000K .......... .......... .......... .......... .......... 26%  214M 0s\n",
            " 15050K .......... .......... .......... .......... .......... 26%  232M 0s\n",
            " 15100K .......... .......... .......... .......... .......... 26%  226M 0s\n",
            " 15150K .......... .......... .......... .......... .......... 26%  197M 0s\n",
            " 15200K .......... .......... .......... .......... .......... 26%  223M 0s\n",
            " 15250K .......... .......... .......... .......... .......... 26%  223M 0s\n",
            " 15300K .......... .......... .......... .......... .......... 26%  231M 0s\n",
            " 15350K .......... .......... .......... .......... .......... 26% 12.4M 0s\n",
            " 15400K .......... .......... .......... .......... .......... 27%  208M 0s\n",
            " 15450K .......... .......... .......... .......... .......... 27%  227M 0s\n",
            " 15500K .......... .......... .......... .......... .......... 27%  232M 0s\n",
            " 15550K .......... .......... .......... .......... .......... 27% 18.9M 0s\n",
            " 15600K .......... .......... .......... .......... .......... 27%  223M 0s\n",
            " 15650K .......... .......... .......... .......... .......... 27%  230M 0s\n",
            " 15700K .......... .......... .......... .......... .......... 27%  219M 0s\n",
            " 15750K .......... .......... .......... .......... .......... 27%  128M 0s\n",
            " 15800K .......... .......... .......... .......... .......... 27%  223M 0s\n",
            " 15850K .......... .......... .......... .......... .......... 27%  217M 0s\n",
            " 15900K .......... .......... .......... .......... .......... 27% 63.1M 0s\n",
            " 15950K .......... .......... .......... .......... .......... 28% 74.6M 0s\n",
            " 16000K .......... .......... .......... .......... .......... 28%  120M 0s\n",
            " 16050K .......... .......... .......... .......... .......... 28%  219M 0s\n",
            " 16100K .......... .......... .......... .......... .......... 28%  233M 0s\n",
            " 16150K .......... .......... .......... .......... .......... 28%  188M 0s\n",
            " 16200K .......... .......... .......... .......... .......... 28%  232M 0s\n",
            " 16250K .......... .......... .......... .......... .......... 28%  211M 0s\n",
            " 16300K .......... .......... .......... .......... .......... 28%  229M 0s\n",
            " 16350K .......... .......... .......... .......... .......... 28%  194M 0s\n",
            " 16400K .......... .......... .......... .......... .......... 28%  233M 0s\n",
            " 16450K .......... .......... .......... .......... .......... 28%  233M 0s\n",
            " 16500K .......... .......... .......... .......... .......... 28%  218M 0s\n",
            " 16550K .......... .......... .......... .......... .......... 29%  209M 0s\n",
            " 16600K .......... .......... .......... .......... .......... 29%  226M 0s\n",
            " 16650K .......... .......... .......... .......... .......... 29%  222M 0s\n",
            " 16700K .......... .......... .......... .......... .......... 29%  236M 0s\n",
            " 16750K .......... .......... .......... .......... .......... 29%  171M 0s\n",
            " 16800K .......... .......... .......... .......... .......... 29%  210M 0s\n",
            " 16850K .......... .......... .......... .......... .......... 29% 15.1M 0s\n",
            " 16900K .......... .......... .......... .......... .......... 29%  188M 0s\n",
            " 16950K .......... .......... .......... .......... .......... 29%  185M 0s\n",
            " 17000K .......... .......... .......... .......... .......... 29%  239M 0s\n",
            " 17050K .......... .......... .......... .......... .......... 29%  278M 0s\n",
            " 17100K .......... .......... .......... .......... .......... 30%  272M 0s\n",
            " 17150K .......... .......... .......... .......... .......... 30%  209M 0s\n",
            " 17200K .......... .......... .......... .......... .......... 30%  225M 0s\n",
            " 17250K .......... .......... .......... .......... .......... 30%  189M 0s\n",
            " 17300K .......... .......... .......... .......... .......... 30%  163M 0s\n",
            " 17350K .......... .......... .......... .......... .......... 30%  221M 0s\n",
            " 17400K .......... .......... .......... .......... .......... 30%  266M 0s\n",
            " 17450K .......... .......... .......... .......... .......... 30% 5.82M 0s\n",
            " 17500K .......... .......... .......... .......... .......... 30% 82.7M 0s\n",
            " 17550K .......... .......... .......... .......... .......... 30%  115M 0s\n",
            " 17600K .......... .......... .......... .......... .......... 30%  122M 0s\n",
            " 17650K .......... .......... .......... .......... .......... 30%  139M 0s\n",
            " 17700K .......... .......... .......... .......... .......... 31%  143M 0s\n",
            " 17750K .......... .......... .......... .......... .......... 31% 86.9M 0s\n",
            " 17800K .......... .......... .......... .......... .......... 31%  220M 0s\n",
            " 17850K .......... .......... .......... .......... .......... 31%  232M 0s\n",
            " 17900K .......... .......... .......... .......... .......... 31%  179M 0s\n",
            " 17950K .......... .......... .......... .......... .......... 31%  144M 0s\n",
            " 18000K .......... .......... .......... .......... .......... 31% 34.7M 0s\n",
            " 18050K .......... .......... .......... .......... .......... 31%  130M 0s\n",
            " 18100K .......... .......... .......... .......... .......... 31% 80.3M 0s\n",
            " 18150K .......... .......... .......... .......... .......... 31% 85.2M 0s\n",
            " 18200K .......... .......... .......... .......... .......... 31%  223M 0s\n",
            " 18250K .......... .......... .......... .......... .......... 32%  231M 0s\n",
            " 18300K .......... .......... .......... .......... .......... 32%  229M 0s\n",
            " 18350K .......... .......... .......... .......... .......... 32%  186M 0s\n",
            " 18400K .......... .......... .......... .......... .......... 32%  215M 0s\n",
            " 18450K .......... .......... .......... .......... .......... 32%  233M 0s\n",
            " 18500K .......... .......... .......... .......... .......... 32% 76.2M 0s\n",
            " 18550K .......... .......... .......... .......... .......... 32%  195M 0s\n",
            " 18600K .......... .......... .......... .......... .......... 32%  234M 0s\n",
            " 18650K .......... .......... .......... .......... .......... 32%  234M 0s\n",
            " 18700K .......... .......... .......... .......... .......... 32% 98.2M 0s\n",
            " 18750K .......... .......... .......... .......... .......... 32%  108M 0s\n",
            " 18800K .......... .......... .......... .......... .......... 33%  196M 0s\n",
            " 18850K .......... .......... .......... .......... .......... 33%  221M 0s\n",
            " 18900K .......... .......... .......... .......... .......... 33%  215M 0s\n",
            " 18950K .......... .......... .......... .......... .......... 33% 18.8M 0s\n",
            " 19000K .......... .......... .......... .......... .......... 33%  127M 0s\n",
            " 19050K .......... .......... .......... .......... .......... 33% 89.6M 0s\n",
            " 19100K .......... .......... .......... .......... .......... 33%  161M 0s\n",
            " 19150K .......... .......... .......... .......... .......... 33% 94.2M 0s\n",
            " 19200K .......... .......... .......... .......... .......... 33% 48.8M 0s\n",
            " 19250K .......... .......... .......... .......... .......... 33% 56.8M 0s\n",
            " 19300K .......... .......... .......... .......... .......... 33% 78.5M 0s\n",
            " 19350K .......... .......... .......... .......... .......... 33%  117M 0s\n",
            " 19400K .......... .......... .......... .......... .......... 34% 97.6M 0s\n",
            " 19450K .......... .......... .......... .......... .......... 34% 89.6M 0s\n",
            " 19500K .......... .......... .......... .......... .......... 34%  211M 0s\n",
            " 19550K .......... .......... .......... .......... .......... 34%  171M 0s\n",
            " 19600K .......... .......... .......... .......... .......... 34%  172M 0s\n",
            " 19650K .......... .......... .......... .......... .......... 34%  223M 0s\n",
            " 19700K .......... .......... .......... .......... .......... 34%  214M 0s\n",
            " 19750K .......... .......... .......... .......... .......... 34% 70.5M 0s\n",
            " 19800K .......... .......... .......... .......... .......... 34%  218M 0s\n",
            " 19850K .......... .......... .......... .......... .......... 34%  229M 0s\n",
            " 19900K .......... .......... .......... .......... .......... 34% 86.4M 0s\n",
            " 19950K .......... .......... .......... .......... .......... 35%  106M 0s\n",
            " 20000K .......... .......... .......... .......... .......... 35%  224M 0s\n",
            " 20050K .......... .......... .......... .......... .......... 35%  194M 0s\n",
            " 20100K .......... .......... .......... .......... .......... 35%  224M 0s\n",
            " 20150K .......... .......... .......... .......... .......... 35%  181M 0s\n",
            " 20200K .......... .......... .......... .......... .......... 35%  223M 0s\n",
            " 20250K .......... .......... .......... .......... .......... 35%  231M 0s\n",
            " 20300K .......... .......... .......... .......... .......... 35%  220M 0s\n",
            " 20350K .......... .......... .......... .......... .......... 35%  186M 0s\n",
            " 20400K .......... .......... .......... .......... .......... 35%  200M 0s\n",
            " 20450K .......... .......... .......... .......... .......... 35%  222M 0s\n",
            " 20500K .......... .......... .......... .......... .......... 35%  227M 0s\n",
            " 20550K .......... .......... .......... .......... .......... 36%  188M 0s\n",
            " 20600K .......... .......... .......... .......... .......... 36%  227M 0s\n",
            " 20650K .......... .......... .......... .......... .......... 36%  215M 0s\n",
            " 20700K .......... .......... .......... .......... .......... 36%  224M 0s\n",
            " 20750K .......... .......... .......... .......... .......... 36%  184M 0s\n",
            " 20800K .......... .......... .......... .......... .......... 36%  221M 0s\n",
            " 20850K .......... .......... .......... .......... .......... 36%  221M 0s\n",
            " 20900K .......... .......... .......... .......... .......... 36%  229M 0s\n",
            " 20950K .......... .......... .......... .......... .......... 36%  206M 0s\n",
            " 21000K .......... .......... .......... .......... .......... 36%  218M 0s\n",
            " 21050K .......... .......... .......... .......... .......... 36%  223M 0s\n",
            " 21100K .......... .......... .......... .......... .......... 37%  221M 0s\n",
            " 21150K .......... .......... .......... .......... .......... 37%  193M 0s\n",
            " 21200K .......... .......... .......... .......... .......... 37%  225M 0s\n",
            " 21250K .......... .......... .......... .......... .......... 37%  236M 0s\n",
            " 21300K .......... .......... .......... .......... .......... 37%  224M 0s\n",
            " 21350K .......... .......... .......... .......... .......... 37%  188M 0s\n",
            " 21400K .......... .......... .......... .......... .......... 37%  229M 0s\n",
            " 21450K .......... .......... .......... .......... .......... 37% 48.0M 0s\n",
            " 21500K .......... .......... .......... .......... .......... 37%  102M 0s\n",
            " 21550K .......... .......... .......... .......... .......... 37% 38.0M 0s\n",
            " 21600K .......... .......... .......... .......... .......... 37%  225M 0s\n",
            " 21650K .......... .......... .......... .......... .......... 38%  218M 0s\n",
            " 21700K .......... .......... .......... .......... .......... 38%  222M 0s\n",
            " 21750K .......... .......... .......... .......... .......... 38%  204M 0s\n",
            " 21800K .......... .......... .......... .......... .......... 38%  222M 0s\n",
            " 21850K .......... .......... .......... .......... .......... 38%  223M 0s\n",
            " 21900K .......... .......... .......... .......... .......... 38%  231M 0s\n",
            " 21950K .......... .......... .......... .......... .......... 38%  195M 0s\n",
            " 22000K .......... .......... .......... .......... .......... 38% 21.3M 0s\n",
            " 22050K .......... .......... .......... .......... .......... 38% 79.9M 0s\n",
            " 22100K .......... .......... .......... .......... .......... 38% 75.3M 0s\n",
            " 22150K .......... .......... .......... .......... .......... 38% 66.3M 0s\n",
            " 22200K .......... .......... .......... .......... .......... 38% 78.5M 0s\n",
            " 22250K .......... .......... .......... .......... .......... 39% 67.6M 0s\n",
            " 22300K .......... .......... .......... .......... .......... 39% 44.1M 0s\n",
            " 22350K .......... .......... .......... .......... .......... 39% 59.4M 0s\n",
            " 22400K .......... .......... .......... .......... .......... 39% 54.2M 0s\n",
            " 22450K .......... .......... .......... .......... .......... 39%  144M 0s\n",
            " 22500K .......... .......... .......... .......... .......... 39%  218M 0s\n",
            " 22550K .......... .......... .......... .......... .......... 39%  186M 0s\n",
            " 22600K .......... .......... .......... .......... .......... 39%  218M 0s\n",
            " 22650K .......... .......... .......... .......... .......... 39%  221M 0s\n",
            " 22700K .......... .......... .......... .......... .......... 39%  225M 0s\n",
            " 22750K .......... .......... .......... .......... .......... 39%  207M 0s\n",
            " 22800K .......... .......... .......... .......... .......... 40%  215M 0s\n",
            " 22850K .......... .......... .......... .......... .......... 40%  201M 0s\n",
            " 22900K .......... .......... .......... .......... .......... 40%  221M 0s\n",
            " 22950K .......... .......... .......... .......... .......... 40%  189M 0s\n",
            " 23000K .......... .......... .......... .......... .......... 40%  231M 0s\n",
            " 23050K .......... .......... .......... .......... .......... 40%  211M 0s\n",
            " 23100K .......... .......... .......... .......... .......... 40%  221M 0s\n",
            " 23150K .......... .......... .......... .......... .......... 40%  200M 0s\n",
            " 23200K .......... .......... .......... .......... .......... 40%  230M 0s\n",
            " 23250K .......... .......... .......... .......... .......... 40%  220M 0s\n",
            " 23300K .......... .......... .......... .......... .......... 40%  223M 0s\n",
            " 23350K .......... .......... .......... .......... .......... 40%  180M 0s\n",
            " 23400K .......... .......... .......... .......... .......... 41%  224M 0s\n",
            " 23450K .......... .......... .......... .......... .......... 41%  226M 0s\n",
            " 23500K .......... .......... .......... .......... .......... 41%  216M 0s\n",
            " 23550K .......... .......... .......... .......... .......... 41%  195M 0s\n",
            " 23600K .......... .......... .......... .......... .......... 41%  225M 0s\n",
            " 23650K .......... .......... .......... .......... .......... 41%  234M 0s\n",
            " 23700K .......... .......... .......... .......... .......... 41%  193M 0s\n",
            " 23750K .......... .......... .......... .......... .......... 41%  191M 0s\n",
            " 23800K .......... .......... .......... .......... .......... 41%  219M 0s\n",
            " 23850K .......... .......... .......... .......... .......... 41%  230M 0s\n",
            " 23900K .......... .......... .......... .......... .......... 41%  231M 0s\n",
            " 23950K .......... .......... .......... .......... .......... 42%  197M 0s\n",
            " 24000K .......... .......... .......... .......... .......... 42%  220M 0s\n",
            " 24050K .......... .......... .......... .......... .......... 42% 9.21M 0s\n",
            " 24100K .......... .......... .......... .......... .......... 42% 89.2M 0s\n",
            " 24150K .......... .......... .......... .......... .......... 42% 62.8M 0s\n",
            " 24200K .......... .......... .......... .......... .......... 42%  197M 0s\n",
            " 24250K .......... .......... .......... .......... .......... 42%  211M 0s\n",
            " 24300K .......... .......... .......... .......... .......... 42%  216M 0s\n",
            " 24350K .......... .......... .......... .......... .......... 42%  169M 0s\n",
            " 24400K .......... .......... .......... .......... .......... 42%  214M 0s\n",
            " 24450K .......... .......... .......... .......... .......... 42%  203M 0s\n",
            " 24500K .......... .......... .......... .......... .......... 42%  234M 0s\n",
            " 24550K .......... .......... .......... .......... .......... 43%  200M 0s\n",
            " 24600K .......... .......... .......... .......... .......... 43%  230M 0s\n",
            " 24650K .......... .......... .......... .......... .......... 43%  225M 0s\n",
            " 24700K .......... .......... .......... .......... .......... 43%  229M 0s\n",
            " 24750K .......... .......... .......... .......... .......... 43% 94.2M 0s\n",
            " 24800K .......... .......... .......... .......... .......... 43% 77.1M 0s\n",
            " 24850K .......... .......... .......... .......... .......... 43%  118M 0s\n",
            " 24900K .......... .......... .......... .......... .......... 43% 49.7M 0s\n",
            " 24950K .......... .......... .......... .......... .......... 43%  104M 0s\n",
            " 25000K .......... .......... .......... .......... .......... 43% 88.7M 0s\n",
            " 25050K .......... .......... .......... .......... .......... 43% 70.5M 0s\n",
            " 25100K .......... .......... .......... .......... .......... 44% 55.4M 0s\n",
            " 25150K .......... .......... .......... .......... .......... 44% 59.4M 0s\n",
            " 25200K .......... .......... .......... .......... .......... 44% 54.5M 0s\n",
            " 25250K .......... .......... .......... .......... .......... 44%  224M 0s\n",
            " 25300K .......... .......... .......... .......... .......... 44%  225M 0s\n",
            " 25350K .......... .......... .......... .......... .......... 44%  123M 0s\n",
            " 25400K .......... .......... .......... .......... .......... 44%  222M 0s\n",
            " 25450K .......... .......... .......... .......... .......... 44%  167M 0s\n",
            " 25500K .......... .......... .......... .......... .......... 44%  139M 0s\n",
            " 25550K .......... .......... .......... .......... .......... 44%  167M 0s\n",
            " 25600K .......... .......... .......... .......... .......... 44%  205M 0s\n",
            " 25650K .......... .......... .......... .......... .......... 45%  102M 0s\n",
            " 25700K .......... .......... .......... .......... .......... 45%  139M 0s\n",
            " 25750K .......... .......... .......... .......... .......... 45%  131M 0s\n",
            " 25800K .......... .......... .......... .......... .......... 45%  208M 0s\n",
            " 25850K .......... .......... .......... .......... .......... 45%  208M 0s\n",
            " 25900K .......... .......... .......... .......... .......... 45%  228M 0s\n",
            " 25950K .......... .......... .......... .......... .......... 45%  103M 0s\n",
            " 26000K .......... .......... .......... .......... .......... 45%  141M 0s\n",
            " 26050K .......... .......... .......... .......... .......... 45%  222M 0s\n",
            " 26100K .......... .......... .......... .......... .......... 45%  223M 0s\n",
            " 26150K .......... .......... .......... .......... .......... 45%  196M 0s\n",
            " 26200K .......... .......... .......... .......... .......... 45%  225M 0s\n",
            " 26250K .......... .......... .......... .......... .......... 46% 50.2M 0s\n",
            " 26300K .......... .......... .......... .......... .......... 46%  229M 0s\n",
            " 26350K .......... .......... .......... .......... .......... 46%  184M 0s\n",
            " 26400K .......... .......... .......... .......... .......... 46%  212M 0s\n",
            " 26450K .......... .......... .......... .......... .......... 46% 97.9M 0s\n",
            " 26500K .......... .......... .......... .......... .......... 46%  232M 0s\n",
            " 26550K .......... .......... .......... .......... .......... 46%  201M 0s\n",
            " 26600K .......... .......... .......... .......... .......... 46%  212M 0s\n",
            " 26650K .......... .......... .......... .......... .......... 46%  223M 0s\n",
            " 26700K .......... .......... .......... .......... .......... 46%  203M 0s\n",
            " 26750K .......... .......... .......... .......... .......... 46%  186M 0s\n",
            " 26800K .......... .......... .......... .......... .......... 47%  212M 0s\n",
            " 26850K .......... .......... .......... .......... .......... 47% 72.0M 0s\n",
            " 26900K .......... .......... .......... .......... .......... 47% 96.1M 0s\n",
            " 26950K .......... .......... .......... .......... .......... 47%  193M 0s\n",
            " 27000K .......... .......... .......... .......... .......... 47%  225M 0s\n",
            " 27050K .......... .......... .......... .......... .......... 47%  218M 0s\n",
            " 27100K .......... .......... .......... .......... .......... 47% 63.3M 0s\n",
            " 27150K .......... .......... .......... .......... .......... 47%  114M 0s\n",
            " 27200K .......... .......... .......... .......... .......... 47%  222M 0s\n",
            " 27250K .......... .......... .......... .......... .......... 47%  231M 0s\n",
            " 27300K .......... .......... .......... .......... .......... 47% 90.1M 0s\n",
            " 27350K .......... .......... .......... .......... .......... 47%  125M 0s\n",
            " 27400K .......... .......... .......... .......... .......... 48%  133M 0s\n",
            " 27450K .......... .......... .......... .......... .......... 48%  211M 0s\n",
            " 27500K .......... .......... .......... .......... .......... 48%  227M 0s\n",
            " 27550K .......... .......... .......... .......... .......... 48%  192M 0s\n",
            " 27600K .......... .......... .......... .......... .......... 48% 87.1M 0s\n",
            " 27650K .......... .......... .......... .......... .......... 48%  202M 0s\n",
            " 27700K .......... .......... .......... .......... .......... 48%  123M 0s\n",
            " 27750K .......... .......... .......... .......... .......... 48%  192M 0s\n",
            " 27800K .......... .......... .......... .......... .......... 48%  229M 0s\n",
            " 27850K .......... .......... .......... .......... .......... 48% 78.0M 0s\n",
            " 27900K .......... .......... .......... .......... .......... 48%  216M 0s\n",
            " 27950K .......... .......... .......... .......... .......... 49%  112M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49%  202M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49%  217M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49% 90.5M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49%  205M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49% 89.6M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49%  207M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49%  226M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49% 95.8M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49%  225M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49%  144M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50%  113M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50%  180M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50% 86.7M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50%  212M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50% 99.5M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50%  196M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50%  139M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50%  135M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50%  199M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50%  207M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50%  129M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50%  225M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51%  214M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51%  109M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51% 64.7M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51%  224M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51%  234M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51%  186M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51% 74.2M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51%  146M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51%  207M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51%  184M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51%  228M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52% 70.8M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52%  205M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52%  118M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52%  226M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52%  217M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52%  161M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52%  179M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52% 65.8M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52%  214M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52%  221M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52%  129M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52%  229M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53% 93.7M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53%  224M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53%  186M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53%  100M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53%  230M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53%  145M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53%  108M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53%  222M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53%  102M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53%  228M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53%  182M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54%  148M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54%  206M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54% 66.8M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54% 99.7M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54%  103M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54%  132M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54%  210M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54%  198M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54%  135M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54%  226M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54% 98.2M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54%  173M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55%  224M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55%  223M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55% 77.8M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55% 97.9M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55%  225M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55%  209M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55%  231M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55% 77.4M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55%  131M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55%  217M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55%  221M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56%  206M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56% 79.2M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56% 97.6M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56%  202M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56%  170M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56%  228M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56% 96.8M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56%  118M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56%  106M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56%  202M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56%  101M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57%  204M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57% 93.2M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57%  225M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57%  215M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57%  230M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57% 89.8M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57% 86.7M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57%  200M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57%  205M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57%  180M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57% 75.8M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57%  201M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58%  116M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58%  197M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58%  138M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58%  149M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58%  228M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58%  189M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58%  108M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58% 82.2M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58%  225M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58%  196M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58%  229M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59%  220M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59%  139M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59% 97.2M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59%  221M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59% 97.9M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59%  215M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59%  204M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59%  113M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59%  217M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59%  227M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59%  167M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59%  212M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60%  209M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60%  212M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60%  194M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60%  204M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60%  224M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60% 74.8M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60% 87.1M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60% 82.9M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60%  208M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60% 73.3M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60%  104M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61%  133M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61% 65.7M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61% 97.6M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61%  121M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61%  206M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61%  121M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61%  206M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61%  150M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61%  192M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61%  226M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61%  222M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61%  191M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62%  205M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62%  226M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62%  188M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62%  177M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62%  200M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62%  213M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62%  208M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62%  169M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62%  196M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62%  204M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  217M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63%  208M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  228M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  219M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63% 76.2M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63% 92.6M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63%  206M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  108M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63% 82.5M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63% 64.2M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63% 55.9M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63% 63.5M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64% 76.0M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64% 88.6M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  213M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64%  134M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64%  214M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64%  129M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64%  127M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64%  133M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64%  124M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64%  111M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64%  130M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64%  204M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65%  122M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65%  113M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65%  165M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65%  202M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65%  200M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65%  173M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65%  210M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65%  202M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65%  209M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65%  189M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65%  229M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66%  233M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66%  215M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66%  187M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66%  226M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66%  234M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66%  217M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66%  197M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66%  223M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66%  114M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66% 58.5M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66%  174M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66%  219M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67%  222M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67%  230M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67%  191M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67%  214M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67%  202M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67%  227M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67%  186M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67%  219M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67%  232M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67%  216M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67% 70.7M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68%  162M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68% 75.4M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68%  219M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68%  187M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68% 66.1M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68%  220M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68%  228M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68%  191M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68%  222M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68%  224M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68%  230M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69%  185M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69%  227M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69%  224M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69%  231M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69%  211M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69%  218M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69%  223M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69%  225M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69%  195M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69%  220M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69%  234M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69% 22.1M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70% 92.6M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70%  228M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70%  227M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70%  237M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70% 14.2M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70% 99.0M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70%  227M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70%  221M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70%  208M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70%  226M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70%  224M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71% 86.6M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71%  143M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71%  217M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71%  207M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71%  216M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71% 13.3M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71%  207M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71%  226M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71%  230M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71% 16.9M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71% 62.8M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71%  145M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72%  230M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72%  199M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72%  228M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72%  227M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72%  234M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72% 24.2M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72%  131M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72%  224M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72%  224M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72%  211M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72% 39.8M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73% 59.7M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73%  210M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73%  193M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73% 73.4M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73%  200M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73%  215M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73%  187M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73% 51.3M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73%  235M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73%  219M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73%  190M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73%  216M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74%  225M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74%  218M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74%  205M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74%  224M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74%  223M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74%  236M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74%  172M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74%  201M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74%  233M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74%  226M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74% 70.4M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75% 93.0M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75%  232M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75%  186M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75%  177M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75% 37.7M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75%  218M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75%  230M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75%  211M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75% 67.1M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75% 95.2M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75%  211M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76%  188M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76%  233M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76%  237M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76%  215M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76%  200M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76%  232M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76%  234M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76%  226M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76% 71.3M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76% 63.3M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76% 62.0M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76% 47.4M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77%  196M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77%  221M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77%  226M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77%  229M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77%  185M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77%  230M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77%  228M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77%  220M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77%  210M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77%  227M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77%  232M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78%  214M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78%  191M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78% 76.8M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78%  221M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78%  228M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78%  197M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78%  230M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78%  207M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78%  230M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78%  189M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78%  231M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78% 25.0M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79%  107M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79%  205M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79%  227M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79%  227M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79% 32.4M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79% 35.1M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79%  109M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79%  229M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79%  223M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79%  209M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79%  221M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80%  225M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80%  225M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80%  193M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80%  235M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80%  222M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80%  236M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80%  200M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80%  229M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80%  224M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80%  236M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80% 62.6M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81%  217M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81%  162M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81% 41.1M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81% 40.2M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81% 55.4M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81% 58.2M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81% 63.2M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81%  188M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81%  140M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81%  142M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81%  219M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81%  121M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82%  226M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82%  223M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82%  100M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82%  122M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82%  133M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82%  235M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82%  217M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82% 99.5M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82%  215M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82%  147M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82%  371M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83%  293M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83%  381M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83%  361M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83%  370M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83%  282M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83%  214M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83% 61.6M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83%  225M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83%  181M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83%  227M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83%  231M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83%  220M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84% 51.5M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84%  107M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84%  220M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84% 70.7M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84%  184M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84%  213M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84%  222M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84%  101M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84%  128M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84%  133M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84%  231M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85%  228M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85% 65.2M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85%  261M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85%  103M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85%  232M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85%  224M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85%  201M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85%  151M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85%  239M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85%  207M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85%  250M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85%  242M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86%  262M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86%  315M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86%  379M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86%  382M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86%  337M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86%  251M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86% 78.6M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86%  210M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86%  230M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86%  218M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86%  236M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87%  241M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87%  243M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87%  195M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87%  217M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87%  265M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87%  252M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87%  206M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87%  207M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87%  202M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87%  217M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87% 68.1M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88%  212M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88%  239M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88%  198M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88%  179M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88%  224M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88%  229M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88%  219M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88%  189M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88% 32.8M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88%  210M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88%  228M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88%  201M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89%  240M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89% 13.8M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89% 53.8M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89%  119M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89% 55.7M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89%  233M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89%  228M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89% 41.8M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89% 84.1M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89%  211M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89%  223M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90%  190M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90%  223M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90%  233M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90% 73.0M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90%  210M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90%  227M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90%  224M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90% 72.1M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90%  181M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90%  227M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90%  226M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90%  233M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91%  201M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91%  219M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91%  222M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91%  224M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91%  193M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91% 75.4M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91%  213M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91%  224M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91%  199M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91%  224M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91%  230M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92%  220M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92%  183M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92%  233M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92%  223M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92%  232M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92%  192M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92%  228M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92%  230M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92%  231M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92%  193M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92% 27.7M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92% 40.8M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93%  214M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93%  203M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93%  155M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93% 86.5M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93%  212M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93%  191M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93%  233M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93%  224M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93%  221M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93%  198M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93%  234M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94%  199M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94%  230M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94%  184M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94%  224M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94%  219M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94%  220M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94% 74.9M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94%  210M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94%  226M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94%  224M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94%  191M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95%  224M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95%  218M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95%  218M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95%  210M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95%  232M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95%  223M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95%  228M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95%  183M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95%  230M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95%  221M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95%  229M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95% 37.1M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96%  108M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96%  216M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96%  225M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96%  186M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96%  229M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96%  201M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96%  211M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96%  163M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96%  196M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96%  207M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96%  210M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97%  181M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97%  217M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97%  207M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97% 48.8M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97%  190M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97%  235M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97%  231M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97%  221M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97%  186M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97%  214M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97%  235M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97%  238M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98%  200M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98%  222M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98%  225M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98% 51.0M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98%  177M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98%  223M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98%  234M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98%  223M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98%  206M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98%  233M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98%  226M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99%  232M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99%  187M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99% 38.3M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99%  207M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99%  217M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99%  188M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99% 15.7M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99%  120M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99%  234M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99%  190M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99%  239M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100%  210M=0.5s\n",
            "\n",
            "2021-07-15 04:13:30 (121 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK_Op8KHQj8n",
        "outputId": "589d9aa3-405f-422a-ffe3-4f52175fe48c"
      },
      "source": [
        "!which conda # should return /usr/local/bin/conda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/conda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9BecajqQlnr",
        "outputId": "9192f196-04e4-43b5-bfb8-7312874cc52b"
      },
      "source": [
        "!conda --version # should return 4.5.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conda 4.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tynIqeL9Qmvg",
        "outputId": "5bfbf47e-c1cc-441a-f85c-a6c24e186440"
      },
      "source": [
        "!which python # still returns /usr/local/bin/python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3n-K5hIQobe",
        "outputId": "6a75d8c7-a3d5-4410-d5ed-fcdcdea53fe3"
      },
      "source": [
        "!python --version # now returns Python 3.6.5 :: Anaconda, Inc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CEMF009Qpjb",
        "outputId": "134a0a58-eefe-4c3f-d53b-eea6069f525b"
      },
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    pyopenssl-20.0.1           |     pyhd3eb1b0_1          48 KB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    tqdm-4.61.2                |     pyhd3eb1b0_1          80 KB\n",
            "    readline-8.1               |       h27cfd23_0         464 KB\n",
            "    pip-21.1.3                 |   py36h06a4308_0         2.1 MB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    brotlipy-0.7.0             |py36h27cfd23_1003         349 KB\n",
            "    pysocks-1.7.1              |   py36h06a4308_0          30 KB\n",
            "    ld_impl_linux-64-2.35.1    |       h7274673_9         637 KB\n",
            "    cryptography-3.4.7         |   py36hd23ed53_0         1.0 MB\n",
            "    urllib3-1.26.6             |     pyhd3eb1b0_1         106 KB\n",
            "    setuptools-52.0.0          |   py36h06a4308_0         933 KB\n",
            "    pycosat-0.6.3              |   py36h27cfd23_0         107 KB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    libstdcxx-ng-9.1.0         |       hdf63c60_0         4.0 MB\n",
            "    tk-8.6.10                  |       hbc83047_0         3.2 MB\n",
            "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
            "    ca-certificates-2021.7.5   |       h06a4308_1         119 KB\n",
            "    yaml-0.2.5                 |       h7b6447c_0          87 KB\n",
            "    ruamel_yaml-0.15.100       |   py36h27cfd23_0         268 KB\n",
            "    zlib-1.2.11                |       h7b6447c_3         120 KB\n",
            "    certifi-2021.5.30          |   py36h06a4308_0         141 KB\n",
            "    pycparser-2.20             |             py_2          94 KB\n",
            "    chardet-4.0.0              |py36h06a4308_1003         213 KB\n",
            "    python-3.6.13              |       h12debd9_1        32.5 MB\n",
            "    conda-package-handling-1.7.3|   py36h27cfd23_1         946 KB\n",
            "    openssl-1.1.1k             |       h27cfd23_0         3.8 MB\n",
            "    conda-4.10.3               |   py36h06a4308_0         3.1 MB\n",
            "    cffi-1.14.6                |   py36h400218f_0         224 KB\n",
            "    sqlite-3.36.0              |       hc218d9a_0         1.4 MB\n",
            "    six-1.16.0                 |     pyhd3eb1b0_0          18 KB\n",
            "    idna-2.10                  |     pyhd3eb1b0_0          52 KB\n",
            "    requests-2.25.1            |     pyhd3eb1b0_0          51 KB\n",
            "    wheel-0.36.2               |     pyhd3eb1b0_0          31 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        65.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main               \n",
            "    brotlipy:               0.7.0-py36h27cfd23_1003\n",
            "    conda-package-handling: 1.7.3-py36h27cfd23_1   \n",
            "    ld_impl_linux-64:       2.35.1-h7274673_9      \n",
            "    tqdm:                   4.61.2-pyhd3eb1b0_1    \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates:        2018.03.07-0            --> 2021.7.5-h06a4308_1     \n",
            "    certifi:                2018.4.16-py36_0        --> 2021.5.30-py36h06a4308_0\n",
            "    cffi:                   1.11.5-py36h9745a5d_0   --> 1.14.6-py36h400218f_0   \n",
            "    chardet:                3.0.4-py36h0f667ec_1    --> 4.0.0-py36h06a4308_1003 \n",
            "    conda:                  4.5.4-py36_0            --> 4.10.3-py36h06a4308_0   \n",
            "    cryptography:           2.2.2-py36h14c3975_0    --> 3.4.7-py36hd23ed53_0    \n",
            "    idna:                   2.6-py36h82fb2a8_1      --> 2.10-pyhd3eb1b0_0       \n",
            "    libffi:                 3.2.1-hd88cf55_4        --> 3.3-he6710b0_2          \n",
            "    libgcc-ng:              7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    libstdcxx-ng:           7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    ncurses:                6.1-hf484d3e_0          --> 6.2-he6710b0_1          \n",
            "    openssl:                1.0.2o-h20670df_0       --> 1.1.1k-h27cfd23_0       \n",
            "    pip:                    10.0.1-py36_0           --> 21.1.3-py36h06a4308_0   \n",
            "    pycosat:                0.6.3-py36h0a5515d_0    --> 0.6.3-py36h27cfd23_0    \n",
            "    pycparser:              2.18-py36hf9f622e_1     --> 2.20-py_2               \n",
            "    pyopenssl:              18.0.0-py36_0           --> 20.0.1-pyhd3eb1b0_1     \n",
            "    pysocks:                1.6.8-py36_0            --> 1.7.1-py36h06a4308_0    \n",
            "    python:                 3.6.5-hc3d631a_2        --> 3.6.13-h12debd9_1       \n",
            "    readline:               7.0-ha6073c6_4          --> 8.1-h27cfd23_0          \n",
            "    requests:               2.18.4-py36he2e5f8d_1   --> 2.25.1-pyhd3eb1b0_0     \n",
            "    ruamel_yaml:            0.15.37-py36h14c3975_2  --> 0.15.100-py36h27cfd23_0 \n",
            "    setuptools:             39.2.0-py36_0           --> 52.0.0-py36h06a4308_0   \n",
            "    six:                    1.11.0-py36h372c433_1   --> 1.16.0-pyhd3eb1b0_0     \n",
            "    sqlite:                 3.23.1-he433501_0       --> 3.36.0-hc218d9a_0       \n",
            "    tk:                     8.6.7-hc745277_3        --> 8.6.10-hbc83047_0       \n",
            "    urllib3:                1.22-py36hbe7ace6_0     --> 1.26.6-pyhd3eb1b0_1     \n",
            "    wheel:                  0.31.1-py36_0           --> 0.36.2-pyhd3eb1b0_0     \n",
            "    xz:                     5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0        \n",
            "    yaml:                   0.1.7-had09818_2        --> 0.2.5-h7b6447c_0        \n",
            "    zlib:                   1.2.11-ha838bed_2       --> 1.2.11-h7b6447c_3       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |            1_gnu          22 KB\n",
            "    libgcc-ng-9.3.0            |      h5101ec6_17         4.8 MB\n",
            "    libgomp-9.3.0              |      h5101ec6_17         311 KB\n",
            "    libstdcxx-ng-9.3.0         |      hd4cf53a_17         3.1 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         8.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-4.5-1_gnu\n",
            "  libgomp            pkgs/main/linux-64::libgomp-9.3.0-h5101ec6_17\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  asn1crypto-0.24.0-py36_0\n",
            "  conda-env-2.6.0-h36134e3_1\n",
            "  libedit-3.1.20170329-h6b74fdf_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  libgcc-ng                                9.1.0-hdf63c60_0 --> 9.3.0-h5101ec6_17\n",
            "  libstdcxx-ng                             9.1.0-hdf63c60_0 --> 9.3.0-hd4cf53a_17\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\rlibgomp-9.3.0        | 311 KB    |            |   0% \rlibgomp-9.3.0        | 311 KB    | ########## | 100% \n",
            "\rlibstdcxx-ng-9.3.0   | 3.1 MB    |            |   0% \rlibstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \rlibstdcxx-ng-9.3.0   | 3.1 MB    | ########## | 100% \n",
            "\r_openmp_mutex-4.5    | 22 KB     |            |   0% \r_openmp_mutex-4.5    | 22 KB     | ########## | 100% \n",
            "\rlibgcc-ng-9.3.0      | 4.8 MB    |            |   0% \rlibgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \rlibgcc-ng-9.3.0      | 4.8 MB    | ########## | 100% \n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rpyopenssl-20.0.1     |   48 KB |            |   0% \rpyopenssl-20.0.1     |   48 KB | ########## | 100% \n",
            "\rlibgcc-ng-9.1.0      |  8.1 MB |            |   0% \rlibgcc-ng-9.1.0      |  8.1 MB | #######5   |  75% \rlibgcc-ng-9.1.0      |  8.1 MB | #########7 |  98% \rlibgcc-ng-9.1.0      |  8.1 MB | ########## | 100% \n",
            "\rtqdm-4.61.2          |   80 KB |            |   0% \rtqdm-4.61.2          |   80 KB | ########## | 100% \n",
            "\rreadline-8.1         |  464 KB |            |   0% \rreadline-8.1         |  464 KB | ########## | 100% \n",
            "\rpip-21.1.3           |  2.1 MB |            |   0% \rpip-21.1.3           |  2.1 MB | #######7   |  77% \rpip-21.1.3           |  2.1 MB | #########5 |  95% \rpip-21.1.3           |  2.1 MB | ########## | 100% \n",
            "\rxz-5.2.5             |  438 KB |            |   0% \rxz-5.2.5             |  438 KB | ########## | 100% \n",
            "\rlibffi-3.3           |   54 KB |            |   0% \rlibffi-3.3           |   54 KB | ########## | 100% \n",
            "\rbrotlipy-0.7.0       |  349 KB |            |   0% \rbrotlipy-0.7.0       |  349 KB | ########## | 100% \n",
            "\rpysocks-1.7.1        |   30 KB |            |   0% \rpysocks-1.7.1        |   30 KB | ########## | 100% \n",
            "\rld_impl_linux-64-2.3 |  637 KB |            |   0% \rld_impl_linux-64-2.3 |  637 KB | #########5 |  95% \rld_impl_linux-64-2.3 |  637 KB | ########## | 100% \n",
            "\rcryptography-3.4.7   |  1.0 MB |            |   0% \rcryptography-3.4.7   |  1.0 MB | #######9   |  80% \rcryptography-3.4.7   |  1.0 MB | ########## | 100% \n",
            "\rurllib3-1.26.6       |  106 KB |            |   0% \rurllib3-1.26.6       |  106 KB | ########## | 100% \n",
            "\rsetuptools-52.0.0    |  933 KB |            |   0% \rsetuptools-52.0.0    |  933 KB | ########1  |  81% \rsetuptools-52.0.0    |  933 KB | ########## | 100% \n",
            "\rpycosat-0.6.3        |  107 KB |            |   0% \rpycosat-0.6.3        |  107 KB | ########## | 100% \n",
            "\r_libgcc_mutex-0.1    |    3 KB |            |   0% \r_libgcc_mutex-0.1    |    3 KB | ########## | 100% \n",
            "\rlibstdcxx-ng-9.1.0   |  4.0 MB |            |   0% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #######6   |  77% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #########9 | 100% \rlibstdcxx-ng-9.1.0   |  4.0 MB | ########## | 100% \n",
            "\rtk-8.6.10            |  3.2 MB |            |   0% \rtk-8.6.10            |  3.2 MB | #######7   |  78% \rtk-8.6.10            |  3.2 MB | #########7 |  98% \rtk-8.6.10            |  3.2 MB | ########## | 100% \n",
            "\rncurses-6.2          |  1.1 MB |            |   0% \rncurses-6.2          |  1.1 MB | #######9   |  79% \rncurses-6.2          |  1.1 MB | ########## | 100% \n",
            "\rca-certificates-2021 |  119 KB |            |   0% \rca-certificates-2021 |  119 KB | ########## | 100% \n",
            "\ryaml-0.2.5           |   87 KB |            |   0% \ryaml-0.2.5           |   87 KB | ########## | 100% \n",
            "\rruamel_yaml-0.15.100 |  268 KB |            |   0% \rruamel_yaml-0.15.100 |  268 KB | ########## | 100% \n",
            "\rzlib-1.2.11          |  120 KB |            |   0% \rzlib-1.2.11          |  120 KB | ########## | 100% \n",
            "\rcertifi-2021.5.30    |  141 KB |            |   0% \rcertifi-2021.5.30    |  141 KB | ########## | 100% \n",
            "\rpycparser-2.20       |   94 KB |            |   0% \rpycparser-2.20       |   94 KB | ########## | 100% \n",
            "\rchardet-4.0.0        |  213 KB |            |   0% \rchardet-4.0.0        |  213 KB | ########## | 100% \n",
            "\rpython-3.6.13        | 32.5 MB |            |   0% \rpython-3.6.13        | 32.5 MB | ##6        |  27% \rpython-3.6.13        | 32.5 MB | #####7     |  58% \rpython-3.6.13        | 32.5 MB | #######5   |  75% \rpython-3.6.13        | 32.5 MB | #########  |  91% \rpython-3.6.13        | 32.5 MB | ########## | 100% \n",
            "\rconda-package-handli |  946 KB |            |   0% \rconda-package-handli |  946 KB | ########6  |  86% \rconda-package-handli |  946 KB | ########## | 100% \n",
            "\ropenssl-1.1.1k       |  3.8 MB |            |   0% \ropenssl-1.1.1k       |  3.8 MB | #######7   |  78% \ropenssl-1.1.1k       |  3.8 MB | ########## | 100% \n",
            "\rconda-4.10.3         |  3.1 MB |            |   0% \rconda-4.10.3         |  3.1 MB | ########1  |  81% \rconda-4.10.3         |  3.1 MB | ########## | 100% \n",
            "\rcffi-1.14.6          |  224 KB |            |   0% \rcffi-1.14.6          |  224 KB | ########## | 100% \n",
            "\rsqlite-3.36.0        |  1.4 MB |            |   0% \rsqlite-3.36.0        |  1.4 MB | ########5  |  86% \rsqlite-3.36.0        |  1.4 MB | ########## | 100% \n",
            "\rsix-1.16.0           |   18 KB |            |   0% \rsix-1.16.0           |   18 KB | ########## | 100% \n",
            "\ridna-2.10            |   52 KB |            |   0% \ridna-2.10            |   52 KB | ########## | 100% \n",
            "\rrequests-2.25.1      |   51 KB |            |   0% \rrequests-2.25.1      |   51 KB | ########## | 100% \n",
            "\rwheel-0.36.2         |   31 KB |            |   0% \rwheel-0.36.2         |   31 KB | ########## | 100% \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-gYm_veQqo6",
        "outputId": "78e75988-2e84-4db1-e0e5-9835ae4b23c3"
      },
      "source": [
        "!conda --version # now returns 4.8.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conda 4.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlmlTbA3QtHV",
        "outputId": "4bd732ac-de53-4261-bf58-84c08636341d"
      },
      "source": [
        "!python --version # now returns Python 3.6.10 :: Anaconda, Inc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.13 :: Anaconda, Inc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuHuDstRQuNy",
        "outputId": "f4fc3655-5afa-4279-a9ab-9212a97c064a"
      },
      "source": [
        "import sys\n",
        "sys.path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3ooXNVfQvSM",
        "outputId": "ca10c0dd-8944-46b7-bcaf-be7f9f484618"
      },
      "source": [
        "['',  \n",
        " '/env/python',\n",
        " '/usr/lib/python36.zip',\n",
        " '/usr/lib/python3.6',\n",
        " '/usr/lib/python3.6/lib-dynload',\n",
        " '/usr/local/lib/python3.6/dist-packages', # pre-installed packages\n",
        " '/usr/lib/python3/dist-packages',\n",
        " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
        " '/root/.ipython']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeacIpOGQwoV"
      },
      "source": [
        "!ls /usr/local/lib/python3.6/dist-packages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8dY3MA4Qxx_"
      },
      "source": [
        "import sys\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.6/site-packages\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPmVjQFUQypL",
        "outputId": "23d759c9-3c8e-4edc-c31b-71206619ccc9"
      },
      "source": [
        "!conda install --channel conda-forge featuretools --yes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - featuretools\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bokeh-2.3.3                |   py36h5fab9bb_0         8.3 MB  conda-forge\n",
            "    ca-certificates-2021.5.30  |       ha878542_0         136 KB  conda-forge\n",
            "    certifi-2021.5.30          |   py36h5fab9bb_0         141 KB  conda-forge\n",
            "    click-7.1.2                |     pyh9f0ad1d_0          64 KB  conda-forge\n",
            "    cloudpickle-1.6.0          |             py_0          22 KB  conda-forge\n",
            "    conda-4.10.3               |   py36h5fab9bb_0         3.1 MB  conda-forge\n",
            "    contextvars-2.4            |             py_0          11 KB  conda-forge\n",
            "    cytoolz-0.11.0             |   py36h8f6f2f9_3         393 KB  conda-forge\n",
            "    dask-2021.3.0              |     pyhd8ed1ab_0           4 KB  conda-forge\n",
            "    dask-core-2021.3.0         |     pyhd8ed1ab_0         702 KB  conda-forge\n",
            "    distributed-2021.3.0       |   py36h5fab9bb_0         1.1 MB  conda-forge\n",
            "    featuretools-0.23.3        |     pyhd8ed1ab_0         267 KB  conda-forge\n",
            "    freetype-2.10.4            |       h0708190_1         890 KB  conda-forge\n",
            "    fsspec-2021.7.0            |     pyhd8ed1ab_0          81 KB  conda-forge\n",
            "    heapdict-1.0.1             |             py_0           7 KB  conda-forge\n",
            "    immutables-0.15            |   py36h8f6f2f9_0          70 KB  conda-forge\n",
            "    jbig-2.1                   |    h7f98852_2003          43 KB  conda-forge\n",
            "    jinja2-3.0.1               |     pyhd8ed1ab_0          99 KB  conda-forge\n",
            "    jpeg-9d                    |       h36c2ea0_0         264 KB  conda-forge\n",
            "    lcms2-2.12                 |       hddcbb42_0         443 KB  conda-forge\n",
            "    lerc-2.2.1                 |       h9c3ff4c_0         213 KB  conda-forge\n",
            "    libblas-3.9.0              |       9_openblas          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |       9_openblas          11 KB  conda-forge\n",
            "    libdeflate-1.7             |       h7f98852_5          67 KB  conda-forge\n",
            "    libgfortran-ng-9.3.0       |      hff62375_19          22 KB  conda-forge\n",
            "    libgfortran5-9.3.0         |      hff62375_19         2.0 MB  conda-forge\n",
            "    liblapack-3.9.0            |       9_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.15         |pthreads_h8fe5266_1         9.2 MB  conda-forge\n",
            "    libpng-1.6.37              |       h21135ba_2         306 KB  conda-forge\n",
            "    libtiff-4.3.0              |       hf544144_1         668 KB  conda-forge\n",
            "    libwebp-base-1.2.0         |       h7f98852_2         815 KB  conda-forge\n",
            "    locket-0.2.0               |             py_2           6 KB  conda-forge\n",
            "    lz4-c-1.9.3                |       h9c3ff4c_0         179 KB  conda-forge\n",
            "    markupsafe-2.0.1           |   py36h8f6f2f9_0          22 KB  conda-forge\n",
            "    msgpack-python-1.0.2       |   py36h605e78d_1          91 KB  conda-forge\n",
            "    numpy-1.19.5               |   py36h2aa4a07_1         5.3 MB  conda-forge\n",
            "    olefile-0.46               |     pyh9f0ad1d_1          32 KB  conda-forge\n",
            "    openjpeg-2.4.0             |       hb52868f_1         444 KB  conda-forge\n",
            "    openssl-1.1.1k             |       h7f98852_0         2.1 MB  conda-forge\n",
            "    packaging-21.0             |     pyhd8ed1ab_0          35 KB  conda-forge\n",
            "    pandas-1.1.5               |   py36h284efc9_0        11.3 MB  conda-forge\n",
            "    partd-1.2.0                |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    pillow-8.3.1               |   py36h676a545_0         687 KB  conda-forge\n",
            "    psutil-5.8.0               |   py36h8f6f2f9_1         342 KB  conda-forge\n",
            "    pyparsing-2.4.7            |     pyh9f0ad1d_0          60 KB  conda-forge\n",
            "    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n",
            "    python_abi-3.6             |          2_cp36m           4 KB  conda-forge\n",
            "    pytz-2021.1                |     pyhd8ed1ab_0         239 KB  conda-forge\n",
            "    pyyaml-5.4.1               |   py36h8f6f2f9_0         190 KB  conda-forge\n",
            "    scipy-1.5.3                |   py36h9e8f40b_0        19.1 MB  conda-forge\n",
            "    sortedcontainers-2.4.0     |     pyhd8ed1ab_0          26 KB  conda-forge\n",
            "    tblib-1.7.0                |     pyhd8ed1ab_0          15 KB  conda-forge\n",
            "    toolz-0.11.1               |             py_0          46 KB  conda-forge\n",
            "    tornado-6.1                |   py36h8f6f2f9_1         643 KB  conda-forge\n",
            "    typing_extensions-3.10.0.0 |     pyha770c72_0          28 KB  conda-forge\n",
            "    zict-2.0.0                 |             py_0          10 KB  conda-forge\n",
            "    zstd-1.5.0                 |       ha95c52a_0         490 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        70.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  bokeh              conda-forge/linux-64::bokeh-2.3.3-py36h5fab9bb_0\n",
            "  click              conda-forge/noarch::click-7.1.2-pyh9f0ad1d_0\n",
            "  cloudpickle        conda-forge/noarch::cloudpickle-1.6.0-py_0\n",
            "  contextvars        conda-forge/noarch::contextvars-2.4-py_0\n",
            "  cytoolz            conda-forge/linux-64::cytoolz-0.11.0-py36h8f6f2f9_3\n",
            "  dask               conda-forge/noarch::dask-2021.3.0-pyhd8ed1ab_0\n",
            "  dask-core          conda-forge/noarch::dask-core-2021.3.0-pyhd8ed1ab_0\n",
            "  distributed        conda-forge/linux-64::distributed-2021.3.0-py36h5fab9bb_0\n",
            "  featuretools       conda-forge/noarch::featuretools-0.23.3-pyhd8ed1ab_0\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.4-h0708190_1\n",
            "  fsspec             conda-forge/noarch::fsspec-2021.7.0-pyhd8ed1ab_0\n",
            "  heapdict           conda-forge/noarch::heapdict-1.0.1-py_0\n",
            "  immutables         conda-forge/linux-64::immutables-0.15-py36h8f6f2f9_0\n",
            "  jbig               conda-forge/linux-64::jbig-2.1-h7f98852_2003\n",
            "  jinja2             conda-forge/noarch::jinja2-3.0.1-pyhd8ed1ab_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9d-h36c2ea0_0\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.12-hddcbb42_0\n",
            "  lerc               conda-forge/linux-64::lerc-2.2.1-h9c3ff4c_0\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-9_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-9_openblas\n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.7-h7f98852_5\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-9.3.0-hff62375_19\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-9.3.0-hff62375_19\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-9_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.15-pthreads_h8fe5266_1\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-h21135ba_2\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.3.0-hf544144_1\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.2.0-h7f98852_2\n",
            "  locket             conda-forge/noarch::locket-0.2.0-py_2\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.3-h9c3ff4c_0\n",
            "  markupsafe         conda-forge/linux-64::markupsafe-2.0.1-py36h8f6f2f9_0\n",
            "  msgpack-python     conda-forge/linux-64::msgpack-python-1.0.2-py36h605e78d_1\n",
            "  numpy              conda-forge/linux-64::numpy-1.19.5-py36h2aa4a07_1\n",
            "  olefile            conda-forge/noarch::olefile-0.46-pyh9f0ad1d_1\n",
            "  openjpeg           conda-forge/linux-64::openjpeg-2.4.0-hb52868f_1\n",
            "  packaging          conda-forge/noarch::packaging-21.0-pyhd8ed1ab_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.1.5-py36h284efc9_0\n",
            "  partd              conda-forge/noarch::partd-1.2.0-pyhd8ed1ab_0\n",
            "  pillow             conda-forge/linux-64::pillow-8.3.1-py36h676a545_0\n",
            "  psutil             conda-forge/linux-64::psutil-5.8.0-py36h8f6f2f9_1\n",
            "  pyparsing          conda-forge/noarch::pyparsing-2.4.7-pyh9f0ad1d_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.6-2_cp36m\n",
            "  pytz               conda-forge/noarch::pytz-2021.1-pyhd8ed1ab_0\n",
            "  pyyaml             conda-forge/linux-64::pyyaml-5.4.1-py36h8f6f2f9_0\n",
            "  scipy              conda-forge/linux-64::scipy-1.5.3-py36h9e8f40b_0\n",
            "  sortedcontainers   conda-forge/noarch::sortedcontainers-2.4.0-pyhd8ed1ab_0\n",
            "  tblib              conda-forge/noarch::tblib-1.7.0-pyhd8ed1ab_0\n",
            "  toolz              conda-forge/noarch::toolz-0.11.1-py_0\n",
            "  tornado            conda-forge/linux-64::tornado-6.1-py36h8f6f2f9_1\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-3.10.0.0-pyha770c72_0\n",
            "  zict               conda-forge/noarch::zict-2.0.0-py_0\n",
            "  zstd               conda-forge/linux-64::zstd-1.5.0-ha95c52a_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2021.7.5-h~ --> conda-forge::ca-certificates-2021.5.30-ha878542_0\n",
            "  certifi            pkgs/main::certifi-2021.5.30-py36h06a~ --> conda-forge::certifi-2021.5.30-py36h5fab9bb_0\n",
            "  conda              pkgs/main::conda-4.10.3-py36h06a4308_0 --> conda-forge::conda-4.10.3-py36h5fab9bb_0\n",
            "  openssl              pkgs/main::openssl-1.1.1k-h27cfd23_0 --> conda-forge::openssl-1.1.1k-h7f98852_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00, 11.26it/s]\n",
            "pyparsing-2.4.7      | 60 KB     | : 100% 1.0/1 [00:00<00:00, 22.80it/s]\n",
            "pandas-1.1.5         | 11.3 MB   | : 100% 1.0/1 [00:02<00:00,  2.31s/it]               \n",
            "openjpeg-2.4.0       | 444 KB    | : 100% 1.0/1 [00:00<00:00,  8.83it/s]\n",
            "lz4-c-1.9.3          | 179 KB    | : 100% 1.0/1 [00:00<00:00, 14.53it/s]\n",
            "olefile-0.46         | 32 KB     | : 100% 1.0/1 [00:00<00:00, 25.90it/s]\n",
            "toolz-0.11.1         | 46 KB     | : 100% 1.0/1 [00:00<00:00, 14.58it/s]\n",
            "heapdict-1.0.1       | 7 KB      | : 100% 1.0/1 [00:00<00:00, 30.76it/s]\n",
            "featuretools-0.23.3  | 267 KB    | : 100% 1.0/1 [00:00<00:00,  6.43it/s]\n",
            "cloudpickle-1.6.0    | 22 KB     | : 100% 1.0/1 [00:00<00:00, 26.01it/s]\n",
            "pillow-8.3.1         | 687 KB    | : 100% 1.0/1 [00:00<00:00,  5.83it/s]\n",
            "locket-0.2.0         | 6 KB      | : 100% 1.0/1 [00:00<00:00, 26.71it/s]\n",
            "python-dateutil-2.8. | 240 KB    | : 100% 1.0/1 [00:00<00:00, 17.69it/s]\n",
            "distributed-2021.3.0 | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.31it/s]\n",
            "libblas-3.9.0        | 11 KB     | : 100% 1.0/1 [00:00<00:00, 29.67it/s]\n",
            "libgfortran-ng-9.3.0 | 22 KB     | : 100% 1.0/1 [00:00<00:00, 23.08it/s]\n",
            "markupsafe-2.0.1     | 22 KB     | : 100% 1.0/1 [00:00<00:00, 27.71it/s]\n",
            "tornado-6.1          | 643 KB    | : 100% 1.0/1 [00:00<00:00,  5.33it/s]\n",
            "pyyaml-5.4.1         | 190 KB    | : 100% 1.0/1 [00:00<00:00, 14.73it/s]\n",
            "lerc-2.2.1           | 213 KB    | : 100% 1.0/1 [00:00<00:00, 13.99it/s]\n",
            "immutables-0.15      | 70 KB     | : 100% 1.0/1 [00:00<00:00, 17.04it/s]\n",
            "tblib-1.7.0          | 15 KB     | : 100% 1.0/1 [00:00<00:00, 35.64it/s]\n",
            "msgpack-python-1.0.2 | 91 KB     | : 100% 1.0/1 [00:00<00:00, 22.03it/s]\n",
            "zict-2.0.0           | 10 KB     | : 100% 1.0/1 [00:00<00:00, 27.90it/s]\n",
            "ca-certificates-2021 | 136 KB    | : 100% 1.0/1 [00:00<00:00, 25.64it/s]\n",
            "sortedcontainers-2.4 | 26 KB     | : 100% 1.0/1 [00:00<00:00, 23.90it/s]\n",
            "libdeflate-1.7       | 67 KB     | : 100% 1.0/1 [00:00<00:00, 24.43it/s]\n",
            "contextvars-2.4      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 33.66it/s]\n",
            "partd-1.2.0          | 18 KB     | : 100% 1.0/1 [00:00<00:00, 31.98it/s]\n",
            "fsspec-2021.7.0      | 81 KB     | : 100% 1.0/1 [00:00<00:00, 20.79it/s]\n",
            "jinja2-3.0.1         | 99 KB     | : 100% 1.0/1 [00:00<00:00, 19.06it/s]\n",
            "zstd-1.5.0           | 490 KB    | : 100% 1.0/1 [00:00<00:00,  9.83it/s]\n",
            "packaging-21.0       | 35 KB     | : 100% 1.0/1 [00:00<00:00, 29.83it/s]\n",
            "dask-2021.3.0        | 4 KB      | : 100% 1.0/1 [00:00<00:00, 19.97it/s]\n",
            "bokeh-2.3.3          | 8.3 MB    | : 100% 1.0/1 [00:02<00:00,  2.25s/it]\n",
            "click-7.1.2          | 64 KB     | : 100% 1.0/1 [00:00<00:00, 21.49it/s]\n",
            "libtiff-4.3.0        | 668 KB    | : 100% 1.0/1 [00:00<00:00,  6.59it/s]\n",
            "certifi-2021.5.30    | 141 KB    | : 100% 1.0/1 [00:00<00:00, 21.21it/s]\n",
            "libwebp-base-1.2.0   | 815 KB    | : 100% 1.0/1 [00:00<00:00,  6.46it/s]\n",
            "cytoolz-0.11.0       | 393 KB    | : 100% 1.0/1 [00:00<00:00,  9.79it/s]\n",
            "psutil-5.8.0         | 342 KB    | : 100% 1.0/1 [00:00<00:00,  9.57it/s]\n",
            "openssl-1.1.1k       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.83it/s]\n",
            "jpeg-9d              | 264 KB    | : 100% 1.0/1 [00:00<00:00, 14.39it/s]\n",
            "libopenblas-0.3.15   | 9.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.56s/it]               \n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 37.04it/s]\n",
            "python_abi-3.6       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 36.56it/s]\n",
            "freetype-2.10.4      | 890 KB    | : 100% 1.0/1 [00:00<00:00,  5.59it/s]\n",
            "scipy-1.5.3          | 19.1 MB   | : 100% 1.0/1 [00:03<00:00,  3.27s/it]               \n",
            "typing_extensions-3. | 28 KB     | : 100% 1.0/1 [00:00<00:00, 31.96it/s]\n",
            "jbig-2.1             | 43 KB     | : 100% 1.0/1 [00:00<00:00, 29.36it/s]\n",
            "libpng-1.6.37        | 306 KB    | : 100% 1.0/1 [00:00<00:00, 11.35it/s]\n",
            "libgfortran5-9.3.0   | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.51it/s]\n",
            "conda-4.10.3         | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.53it/s]               \n",
            "lcms2-2.12           | 443 KB    | : 100% 1.0/1 [00:00<00:00,  8.36it/s]\n",
            "numpy-1.19.5         | 5.3 MB    | : 100% 1.0/1 [00:01<00:00,  1.13s/it]\n",
            "dask-core-2021.3.0   | 702 KB    | : 100% 1.0/1 [00:00<00:00,  4.77it/s]\n",
            "pytz-2021.1          | 239 KB    | : 100% 1.0/1 [00:00<00:00,  7.43it/s]\n",
            "Preparing transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y8IZHnaDQ0so",
        "outputId": "303cfd14-4983-4070-9083-17e7dfcdfc9a"
      },
      "source": [
        "!conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch\n",
        "!pip install tensorboardX==1.4\n",
        "!conda install opencv=3.3.1   # just needed for evaluation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch=0.4.1\n",
            "    - torchvision=0.2.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2021.7.5   |       h06a4308_1         113 KB\n",
            "    certifi-2021.5.30          |   py36h06a4308_0         139 KB\n",
            "    intel-openmp-2021.2.0      |     h06a4308_610         1.3 MB\n",
            "    mkl-2021.2.0               |     h06a4308_296       144.3 MB\n",
            "    ninja-1.10.2               |       hff7bd54_1         1.4 MB\n",
            "    pytorch-0.4.1              |py36_py35_py27__9.0.176_7.1.2_2       471.7 MB  pytorch\n",
            "    torchvision-0.2.1          |             py_2          37 KB  pytorch\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       619.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.2.0-h06a4308_610\n",
            "  mkl                pkgs/main/linux-64::mkl-2021.2.0-h06a4308_296\n",
            "  ninja              pkgs/main/linux-64::ninja-1.10.2-hff7bd54_1\n",
            "  pytorch            pytorch/linux-64::pytorch-0.4.1-py36_py35_py27__9.0.176_7.1.2_2\n",
            "  torchvision        pytorch/noarch::torchvision-0.2.1-py_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2021.5.3~ --> pkgs/main::ca-certificates-2021.7.5-h06a4308_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2021.5.30-py36h5~ --> pkgs/main::certifi-2021.5.30-py36h06a4308_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? \n",
            "CondaSystemExit: \n",
            "Operation aborted.  Exiting.\n",
            "\n",
            "^C\n",
            "Collecting tensorboardX==1.4\n",
            "  Downloading tensorboardX-1.4-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.2.0\n",
            "  Downloading protobuf-3.17.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from tensorboardX==1.4) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from tensorboardX==1.4) (1.19.5)\n",
            "Installing collected packages: protobuf, tensorboardX\n",
            "Successfully installed protobuf-3.17.3 tensorboardX-1.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - opencv=3.3.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |              mkl           6 KB\n",
            "    bzip2-1.0.8                |       h7b6447c_0          78 KB\n",
            "    cairo-1.16.0               |       hf32fb01_1         1.0 MB\n",
            "    ffmpeg-3.4                 |       h7985aa0_0         7.1 MB\n",
            "    fontconfig-2.13.1          |       h6c09931_0         250 KB\n",
            "    glib-2.69.0                |       h5202010_0         1.7 MB\n",
            "    graphite2-1.3.14           |       h23475e2_0          99 KB\n",
            "    harfbuzz-1.8.8             |       hffaf4a1_0         507 KB\n",
            "    hdf5-1.10.1                |       h9caa474_1         3.8 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    intel-openmp-2021.2.0      |     h06a4308_610         1.3 MB\n",
            "    jasper-1.900.1             |       hd497a04_4         198 KB\n",
            "    libgfortran-ng-7.5.0       |      ha8ba4b0_17          22 KB\n",
            "    libgfortran4-7.5.0         |      ha8ba4b0_17         995 KB\n",
            "    libopus-1.3.1              |       h7b6447c_0         491 KB\n",
            "    libprotobuf-3.4.1          |       h5b8497f_0         2.4 MB\n",
            "    libuuid-1.0.3              |       h1bed415_2          15 KB\n",
            "    libvpx-1.7.0               |       h439df22_0         1.2 MB\n",
            "    libxcb-1.14                |       h7b6447c_0         505 KB\n",
            "    libxml2-2.9.12             |       h03d6c58_0         1.2 MB\n",
            "    mkl-2020.2                 |              256       138.3 MB\n",
            "    mkl-service-2.3.0          |   py36he8ac12f_0          52 KB\n",
            "    mkl_fft-1.3.0              |   py36h54f3939_0         170 KB\n",
            "    mkl_random-1.1.1           |   py36h0573a6f_0         327 KB\n",
            "    numpy-1.19.2               |   py36h54aff64_0          22 KB\n",
            "    numpy-base-1.19.2          |   py36hfa32c7d_0         4.1 MB\n",
            "    opencv-3.3.1               |   py36h6cbbc71_1        21.2 MB\n",
            "    pcre-8.45                  |       h295c915_0         207 KB\n",
            "    pixman-0.40.0              |       h7b6447c_0         370 KB\n",
            "    scipy-1.5.2                |   py36h0b6359f_0        14.4 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       212.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0\n",
            "  cairo              pkgs/main/linux-64::cairo-1.16.0-hf32fb01_1\n",
            "  ffmpeg             pkgs/main/linux-64::ffmpeg-3.4-h7985aa0_0\n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.13.1-h6c09931_0\n",
            "  glib               pkgs/main/linux-64::glib-2.69.0-h5202010_0\n",
            "  graphite2          pkgs/main/linux-64::graphite2-1.3.14-h23475e2_0\n",
            "  harfbuzz           pkgs/main/linux-64::harfbuzz-1.8.8-hffaf4a1_0\n",
            "  hdf5               pkgs/main/linux-64::hdf5-1.10.1-h9caa474_1\n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.2.0-h06a4308_610\n",
            "  jasper             pkgs/main/linux-64::jasper-1.900.1-hd497a04_4\n",
            "  libgfortran4       pkgs/main/linux-64::libgfortran4-7.5.0-ha8ba4b0_17\n",
            "  libopus            pkgs/main/linux-64::libopus-1.3.1-h7b6447c_0\n",
            "  libprotobuf        pkgs/main/linux-64::libprotobuf-3.4.1-h5b8497f_0\n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.0.3-h1bed415_2\n",
            "  libvpx             pkgs/main/linux-64::libvpx-1.7.0-h439df22_0\n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.14-h7b6447c_0\n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.12-h03d6c58_0\n",
            "  mkl                pkgs/main/linux-64::mkl-2020.2-256\n",
            "  mkl-service        pkgs/main/linux-64::mkl-service-2.3.0-py36he8ac12f_0\n",
            "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.0-py36h54f3939_0\n",
            "  mkl_random         pkgs/main/linux-64::mkl_random-1.1.1-py36h0573a6f_0\n",
            "  numpy-base         pkgs/main/linux-64::numpy-base-1.19.2-py36hfa32c7d_0\n",
            "  opencv             pkgs/main/linux-64::opencv-3.3.1-py36h6cbbc71_1\n",
            "  pcre               pkgs/main/linux-64::pcre-8.45-h295c915_0\n",
            "  pixman             pkgs/main/linux-64::pixman-0.40.0-h7b6447c_0\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  libblas-3.9.0-9_openblas\n",
            "  libcblas-3.9.0-9_openblas\n",
            "  libgfortran5-9.3.0-hff62375_19\n",
            "  liblapack-3.9.0-9_openblas\n",
            "  libopenblas-0.3.15-pthreads_h8fe5266_1\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2021.5.3~ --> pkgs/main::ca-certificates-2021.7.5-h06a4308_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2021.5.30-py36h5~ --> pkgs/main::certifi-2021.5.30-py36h06a4308_0\n",
            "  conda              conda-forge::conda-4.10.3-py36h5fab9b~ --> pkgs/main::conda-4.10.3-py36h06a4308_0\n",
            "  libgfortran-ng     conda-forge::libgfortran-ng-9.3.0-hff~ --> pkgs/main::libgfortran-ng-7.5.0-ha8ba4b0_17\n",
            "  numpy              conda-forge::numpy-1.19.5-py36h2aa4a0~ --> pkgs/main::numpy-1.19.2-py36h54aff64_0\n",
            "  openssl            conda-forge::openssl-1.1.1k-h7f98852_0 --> pkgs/main::openssl-1.1.1k-h27cfd23_0\n",
            "  scipy              conda-forge::scipy-1.5.3-py36h9e8f40b~ --> pkgs/main::scipy-1.5.2-py36h0b6359f_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "pcre-8.45            | 207 KB    | : 100% 1.0/1 [00:00<00:00, 13.75it/s]\n",
            "libxcb-1.14          | 505 KB    | : 100% 1.0/1 [00:00<00:00, 15.37it/s]\n",
            "libopus-1.3.1        | 491 KB    | : 100% 1.0/1 [00:00<00:00, 16.55it/s]\n",
            "fontconfig-2.13.1    | 250 KB    | : 100% 1.0/1 [00:00<00:00, 21.38it/s]\n",
            "libgfortran4-7.5.0   | 995 KB    | : 100% 1.0/1 [00:00<00:00, 15.46it/s]\n",
            "graphite2-1.3.14     | 99 KB     | : 100% 1.0/1 [00:00<00:00, 22.06it/s]\n",
            "hdf5-1.10.1          | 3.8 MB    | : 100% 1.0/1 [00:00<00:00,  6.88it/s]\n",
            "pixman-0.40.0        | 370 KB    | : 100% 1.0/1 [00:00<00:00, 20.14it/s]\n",
            "mkl_fft-1.3.0        | 170 KB    | : 100% 1.0/1 [00:00<00:00, 23.48it/s]\n",
            "numpy-1.19.2         | 22 KB     | : 100% 1.0/1 [00:00<00:00, 27.59it/s]\n",
            "mkl_random-1.1.1     | 327 KB    | : 100% 1.0/1 [00:00<00:00, 20.95it/s]\n",
            "intel-openmp-2021.2. | 1.3 MB    | : 100% 1.0/1 [00:00<00:00, 13.31it/s]\n",
            "jasper-1.900.1       | 198 KB    | : 100% 1.0/1 [00:00<00:00, 22.12it/s]\n",
            "libvpx-1.7.0         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00, 13.86it/s]\n",
            "libxml2-2.9.12       | 1.2 MB    | : 100% 1.0/1 [00:00<00:00, 11.64it/s]\n",
            "harfbuzz-1.8.8       | 507 KB    | : 100% 1.0/1 [00:00<00:00, 16.42it/s]\n",
            "mkl-service-2.3.0    | 52 KB     | : 100% 1.0/1 [00:00<00:00, 28.32it/s]\n",
            "opencv-3.3.1         | 21.2 MB   | : 100% 1.0/1 [00:00<00:00,  1.14it/s]               \n",
            "ffmpeg-3.4           | 7.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.56it/s]               \n",
            "libprotobuf-3.4.1    | 2.4 MB    | : 100% 1.0/1 [00:00<00:00,  6.35it/s]\n",
            "blas-1.0             | 6 KB      | : 100% 1.0/1 [00:00<00:00, 24.42it/s]\n",
            "libgfortran-ng-7.5.0 | 22 KB     | : 100% 1.0/1 [00:00<00:00, 21.95it/s]\n",
            "mkl-2020.2           | 138.3 MB  | : 100% 1.0/1 [00:05<00:00,  5.03s/it]               \n",
            "numpy-base-1.19.2    | 4.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.78it/s]\n",
            "scipy-1.5.2          | 14.4 MB   | : 100% 1.0/1 [00:00<00:00,  1.61it/s]               \n",
            "glib-2.69.0          | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  8.46it/s]\n",
            "cairo-1.16.0         | 1.0 MB    | : 100% 1.0/1 [00:00<00:00, 10.32it/s]\n",
            "libuuid-1.0.3        | 15 KB     | : 100% 1.0/1 [00:00<00:00, 20.75it/s]\n",
            "icu-58.2             | 10.5 MB   | : 100% 1.0/1 [00:01<00:00,  1.17s/it]               \n",
            "bzip2-1.0.8          | 78 KB     | : 100% 1.0/1 [00:00<00:00,  1.34it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gag-tlwgQ14F",
        "outputId": "36af4a0b-069d-418f-abd9-32057a3ff715"
      },
      "source": [
        "!conda install pytorch torchvision opencv\n",
        "!pip install timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - opencv\n",
            "    - pytorch\n",
            "    - torchvision\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _pytorch_select-0.2        |            gpu_0           2 KB\n",
            "    cudatoolkit-10.0.130       |                0       261.2 MB\n",
            "    cudnn-7.6.5                |       cuda10.0_0       165.0 MB\n",
            "    ninja-1.10.2               |       hff7bd54_1         1.4 MB\n",
            "    pytorch-1.3.1              |cuda100py36h53c1284_0       169.0 MB\n",
            "    torchvision-0.4.2          |cuda100py36hecfc37a_0         6.4 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       603.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _pytorch_select    pkgs/main/linux-64::_pytorch_select-0.2-gpu_0\n",
            "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.0.130-0\n",
            "  cudnn              pkgs/main/linux-64::cudnn-7.6.5-cuda10.0_0\n",
            "  ninja              pkgs/main/linux-64::ninja-1.10.2-hff7bd54_1\n",
            "  pytorch            pkgs/main/linux-64::pytorch-1.3.1-cuda100py36h53c1284_0\n",
            "  torchvision        pkgs/main/linux-64::torchvision-0.4.2-cuda100py36hecfc37a_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ninja-1.10.2         | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  8.92it/s]\n",
            "_pytorch_select-0.2  | 2 KB      | : 100% 1.0/1 [00:00<00:00, 25.14it/s]\n",
            "cudnn-7.6.5          | 165.0 MB  | : 100% 1.0/1 [00:03<00:00,  3.76s/it]               \n",
            "pytorch-1.3.1        | 169.0 MB  | : 100% 1.0/1 [00:14<00:00, 14.01s/it]               \n",
            "cudatoolkit-10.0.130 | 261.2 MB  | : 100% 1.0/1 [00:25<00:00, 25.91s/it]               \n",
            "torchvision-0.4.2    | 6.4 MB    | : 100% 1.0/1 [00:00<00:00,  4.20it/s]\n",
            "Preparing transaction: - \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 27.3 MB/s \n",
            "\u001b[?25hCollecting torch>=1.4\n",
            "  Downloading torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 10 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/site-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from torch>=1.4->timm) (3.10.0.0)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/site-packages (from torchvision->timm) (8.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from torchvision->timm) (1.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from torchvision->timm) (1.16.0)\n",
            "Installing collected packages: dataclasses, torch, timm\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.3.1\n",
            "    Uninstalling torch-1.3.1:\n",
            "      Successfully uninstalled torch-1.3.1\n",
            "Successfully installed dataclasses-0.8 timm-0.4.12 torch-1.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot4pklEEQ20-",
        "outputId": "78553b3d-fb9d-40b0-83d1-ea38f6f4f691"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting h5py\n",
            "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 24.6 MB/s \n",
            "\u001b[?25hCollecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/site-packages (from h5py) (1.19.2)\n",
            "Installing collected packages: cached-property, h5py\n",
            "Successfully installed cached-property-1.5.2 h5py-3.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "ezCZ17VnQ3vs",
        "outputId": "3eea7bf3-1d4b-42fb-ac37-46d593417c74"
      },
      "source": [
        "!pip install scikit-image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 142 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.6/site-packages (from scikit-image) (1.19.2)\n",
            "Collecting imageio>=2.3.0\n",
            "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting tifffile>=2019.7.26\n",
            "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 73.1 MB/s \n",
            "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 65.5 MB/s \n",
            "\u001b[?25hCollecting networkx>=2.0\n",
            "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.6/site-packages (from scikit-image) (8.3.1)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/site-packages (from scikit-image) (1.5.2)\n",
            "Collecting matplotlib!=3.0.0,>=2.0.0\n",
            "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.16.0)\n",
            "Collecting decorator<5,>=4.3\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: kiwisolver, decorator, cycler, tifffile, PyWavelets, networkx, matplotlib, imageio, scikit-image\n",
            "Successfully installed PyWavelets-1.1.1 cycler-0.10.0 decorator-4.4.2 imageio-2.9.0 kiwisolver-1.3.1 matplotlib-3.3.4 networkx-2.5.1 scikit-image-0.17.2 tifffile-2020.9.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "decorator",
                  "kiwisolver"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kl-3XnoQ4sj",
        "outputId": "0f385d65-a52e-4ab2-ccc3-3beb467f06a3"
      },
      "source": [
        "!conda install ipykernel\n",
        "!python -m ipykernel install --user"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipykernel\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    decorator-5.0.9            |     pyhd3eb1b0_0          12 KB\n",
            "    ipykernel-5.3.4            |   py36h5ca1d4c_0         181 KB\n",
            "    ipython-7.16.1             |   py36h5ca1d4c_0         999 KB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    jedi-0.17.0                |           py36_0         780 KB\n",
            "    jupyter_client-6.1.12      |     pyhd3eb1b0_0          88 KB\n",
            "    jupyter_core-4.7.1         |   py36h06a4308_0          68 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    parso-0.8.2                |     pyhd3eb1b0_0          69 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    prompt-toolkit-3.0.17      |     pyh06a4308_0         256 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pygments-2.9.0             |     pyhd3eb1b0_0         721 KB\n",
            "    pyzmq-20.0.0               |   py36h2531618_1         438 KB\n",
            "    traitlets-4.3.3            |           py36_0         140 KB\n",
            "    wcwidth-0.2.5              |             py_0          29 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         4.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0\n",
            "  decorator          pkgs/main/noarch::decorator-5.0.9-pyhd3eb1b0_0\n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-5.3.4-py36h5ca1d4c_0\n",
            "  ipython            pkgs/main/linux-64::ipython-7.16.1-py36h5ca1d4c_0\n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1\n",
            "  jedi               pkgs/main/linux-64::jedi-0.17.0-py36_0\n",
            "  jupyter_client     pkgs/main/noarch::jupyter_client-6.1.12-pyhd3eb1b0_0\n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.7.1-py36h06a4308_0\n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0\n",
            "  parso              pkgs/main/noarch::parso-0.8.2-pyhd3eb1b0_0\n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3\n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003\n",
            "  prompt-toolkit     pkgs/main/noarch::prompt-toolkit-3.0.17-pyh06a4308_0\n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2\n",
            "  pygments           pkgs/main/noarch::pygments-2.9.0-pyhd3eb1b0_0\n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-20.0.0-py36h2531618_1\n",
            "  traitlets          pkgs/main/linux-64::traitlets-4.3.3-py36_0\n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-py_0\n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "decorator-5.0.9      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 14.37it/s]\n",
            "parso-0.8.2          | 69 KB     | : 100% 1.0/1 [00:00<00:00, 19.92it/s]\n",
            "ptyprocess-0.7.0     | 17 KB     | : 100% 1.0/1 [00:00<00:00, 23.77it/s]\n",
            "libsodium-1.0.18     | 244 KB    | : 100% 1.0/1 [00:00<00:00, 17.28it/s]\n",
            "pygments-2.9.0       | 721 KB    | : 100% 1.0/1 [00:00<00:00, 12.29it/s]\n",
            "backcall-0.2.0       | 13 KB     | : 100% 1.0/1 [00:00<00:00, 45.18it/s]\n",
            "ipython-7.16.1       | 999 KB    | : 100% 1.0/1 [00:00<00:00,  8.45it/s]\n",
            "pickleshare-0.7.5    | 13 KB     | : 100% 1.0/1 [00:00<00:00, 17.55it/s]\n",
            "wcwidth-0.2.5        | 29 KB     | : 100% 1.0/1 [00:00<00:00, 24.14it/s]\n",
            "jupyter_core-4.7.1   | 68 KB     | : 100% 1.0/1 [00:00<00:00, 18.94it/s]\n",
            "jupyter_client-6.1.1 | 88 KB     | : 100% 1.0/1 [00:00<00:00, 20.42it/s]\n",
            "zeromq-4.3.4         | 331 KB    | : 100% 1.0/1 [00:00<00:00, 19.71it/s]\n",
            "pexpect-4.8.0        | 53 KB     | : 100% 1.0/1 [00:00<00:00, 20.11it/s]\n",
            "jedi-0.17.0          | 780 KB    | : 100% 1.0/1 [00:00<00:00,  4.97it/s]\n",
            "prompt-toolkit-3.0.1 | 256 KB    | : 100% 1.0/1 [00:00<00:00, 14.67it/s]\n",
            "pyzmq-20.0.0         | 438 KB    | : 100% 1.0/1 [00:00<00:00, 13.18it/s]\n",
            "traitlets-4.3.3      | 140 KB    | : 100% 1.0/1 [00:00<00:00, 18.84it/s]\n",
            "ipykernel-5.3.4      | 181 KB    | : 100% 1.0/1 [00:00<00:00, 17.75it/s]\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:00<00:00, 22.64it/s]\n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Installed kernelspec python3 in /root/.local/share/jupyter/kernels/python3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6h2MwT8Q505"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDdGzUHgQ764",
        "outputId": "1db3d3b3-8af3-4db6-acdf-aa1c43ec5bbb"
      },
      "source": [
        "!python main.py --video_file data/videos/ayush.mp4 --path results/ayush \\\n",
        "  --camera_params \"1671.770118, 540, 960\" --camera_model \"SIMPLE_PINHOLE\" \\\n",
        "  --make_video"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "(75, 77): reprojection: 0.416348, disparity: 0.087062\n",
            "(76, 77): reprojection: 0.239172, disparity: 0.071185\n",
            "(76, 78): reprojection: 0.363673, disparity: 0.075604\n",
            "(76, 80): reprojection: 0.472915, disparity: 0.101859\n",
            "(76, 84): reprojection: 0.762114, disparity: 0.114964\n",
            "(77, 78): reprojection: 0.249045, disparity: 0.069877\n",
            "(77, 79): reprojection: 0.338156, disparity: 0.079979\n",
            "(78, 79): reprojection: 0.217948, disparity: 0.073959\n",
            "(78, 80): reprojection: 0.314357, disparity: 0.088129\n",
            "(78, 82): reprojection: 0.541570, disparity: 0.093291\n",
            "(79, 80): reprojection: 0.168284, disparity: 0.071499\n",
            "(79, 81): reprojection: 0.289510, disparity: 0.077499\n",
            "(80, 81): reprojection: 0.166207, disparity: 0.067384\n",
            "(80, 82): reprojection: 0.272871, disparity: 0.076229\n",
            "(80, 84): reprojection: 0.525093, disparity: 0.082406\n",
            "(80, 88): reprojection: 0.876291, disparity: 0.106316\n",
            "(81, 82): reprojection: 0.197586, disparity: 0.068165\n",
            "(81, 83): reprojection: 0.366619, disparity: 0.075429\n",
            "(82, 83): reprojection: 0.188941, disparity: 0.066092\n",
            "(82, 84): reprojection: 0.278601, disparity: 0.077885\n",
            "(82, 86): reprojection: 0.619217, disparity: 0.094981\n",
            "(83, 84): reprojection: 0.165052, disparity: 0.069916\n",
            "(83, 85): reprojection: 0.349856, disparity: 0.084702\n",
            "(84, 85): reprojection: 0.187184, disparity: 0.066412\n",
            "(84, 86): reprojection: 0.325526, disparity: 0.080995\n",
            "(84, 88): reprojection: 0.590811, disparity: 0.096313\n",
            "(85, 86): reprojection: 0.194315, disparity: 0.066461\n",
            "(85, 87): reprojection: 0.332554, disparity: 0.079206\n",
            "(86, 87): reprojection: 0.258896, disparity: 0.067427\n",
            "(86, 88): reprojection: 0.365340, disparity: 0.079034\n",
            "(86, 90): reprojection: 0.490180, disparity: 0.092384\n",
            "(87, 88): reprojection: 0.192754, disparity: 0.067621\n",
            "(87, 89): reprojection: 0.278892, disparity: 0.077762\n",
            "(88, 89): reprojection: 0.130238, disparity: 0.064914\n",
            "(88, 90): reprojection: 0.305305, disparity: 0.073162\n",
            "(89, 90): reprojection: 0.216831, disparity: 0.063182\n",
            "(89, 91): reprojection: 0.449320, disparity: 0.081251\n",
            "(90, 91): reprojection: 0.360790, disparity: 0.071212\n",
            "Mean:     reprojection: 0.360790, disparity: 0.071212\n",
            "Done Validation for epoch 5 (1300 iterations)\n",
            "Epoch = 5, pairs = [[84, 85], [32, 34], [21, 23], [87, 89]], loss = 0.3302445709705353\n",
            "Epoch = 5, pairs = [[6, 10], [3, 4], [57, 58], [46, 50]], loss = 0.483310341835022\n",
            "Epoch = 5, pairs = [[24, 28], [19, 20], [0, 4], [23, 24]], loss = 0.3889331817626953\n",
            "Epoch = 5, pairs = [[74, 76], [68, 72], [79, 81], [8, 10]], loss = 0.6425105333328247\n",
            "Epoch = 5, pairs = [[53, 55], [40, 56], [72, 80], [0, 2]], loss = 0.9118444919586182\n",
            "Epoch = 5, pairs = [[54, 58], [72, 73], [16, 17], [36, 37]], loss = 0.3150646388530731\n",
            "Epoch = 5, pairs = [[48, 52], [24, 32], [32, 33], [15, 16]], loss = 0.5148462057113647\n",
            "Epoch = 5, pairs = [[54, 56], [47, 48], [24, 25], [44, 48]], loss = 0.4504045248031616\n",
            "Epoch = 5, pairs = [[18, 19], [12, 16], [4, 5], [40, 44]], loss = 0.47373324632644653\n",
            "Epoch = 5, pairs = [[10, 11], [20, 24], [81, 83], [63, 64]], loss = 0.37649276852607727\n",
            "Epoch = 5, pairs = [[84, 86], [7, 9], [46, 48], [68, 69]], loss = 0.3295278549194336\n",
            "Epoch = 5, pairs = [[5, 7], [25, 26], [20, 22], [5, 6]], loss = 0.2634050250053406\n",
            "Epoch = 5, pairs = [[1, 2], [31, 33], [58, 60], [9, 10]], loss = 0.3628382980823517\n",
            "Epoch = 5, pairs = [[78, 79], [48, 50], [20, 21], [34, 35]], loss = 0.29098135232925415\n",
            "Epoch = 5, pairs = [[16, 20], [28, 36], [48, 56], [62, 64]], loss = 0.7722547054290771\n",
            "Epoch = 5, pairs = [[55, 57], [3, 5], [2, 4], [10, 12]], loss = 0.31744828820228577\n",
            "Epoch = 5, pairs = [[75, 77], [55, 56], [30, 32], [27, 28]], loss = 0.3380548059940338\n",
            "Epoch = 5, pairs = [[0, 32], [18, 22], [28, 32], [14, 16]], loss = 0.9054595232009888\n",
            "Epoch = 5, pairs = [[67, 69], [72, 88], [78, 80], [35, 37]], loss = 0.6270433068275452\n",
            "Epoch = 5, pairs = [[38, 39], [72, 74], [76, 84], [49, 50]], loss = 0.3845077455043793\n",
            "Epoch = 5, pairs = [[31, 32], [6, 8], [64, 72], [52, 56]], loss = 0.49454158544540405\n",
            "Epoch = 5, pairs = [[13, 15], [73, 75], [41, 43], [88, 90]], loss = 0.3948412835597992\n",
            "Epoch = 5, pairs = [[71, 73], [39, 40], [59, 61], [57, 59]], loss = 0.46339190006256104\n",
            "Epoch = 5, pairs = [[8, 9], [0, 16], [84, 88], [38, 42]], loss = 0.6525592803955078\n",
            "Epoch = 5, pairs = [[77, 79], [37, 39], [62, 66], [89, 91]], loss = 0.45908260345458984\n",
            "Epoch = 5, pairs = [[76, 77], [22, 23], [35, 36], [68, 70]], loss = 0.3262253999710083\n",
            "Epoch = 5, pairs = [[76, 80], [83, 84], [50, 51], [51, 52]], loss = 0.2868504226207733\n",
            "Epoch = 5, pairs = [[36, 40], [90, 91], [33, 34], [37, 38]], loss = 0.33573979139328003\n",
            "Epoch = 5, pairs = [[26, 28], [69, 71], [10, 14], [43, 45]], loss = 0.37966644763946533\n",
            "Epoch = 5, pairs = [[80, 82], [88, 89], [7, 8], [72, 76]], loss = 0.29994064569473267\n",
            "Epoch = 5, pairs = [[52, 54], [39, 41], [85, 87], [87, 88]], loss = 0.3729293942451477\n",
            "Epoch = 5, pairs = [[14, 15], [27, 29], [77, 78], [82, 84]], loss = 0.2970582842826843\n",
            "Epoch = 5, pairs = [[1, 3], [32, 48], [45, 47], [64, 80]], loss = 0.7649827599525452\n",
            "Epoch = 5, pairs = [[70, 72], [54, 55], [17, 19], [40, 41]], loss = 0.40287625789642334\n",
            "Epoch = 5, pairs = [[29, 30], [11, 12], [81, 82], [56, 57]], loss = 0.2470136284828186\n",
            "Epoch = 5, pairs = [[56, 72], [43, 44], [36, 44], [86, 87]], loss = 0.5750763416290283\n",
            "Epoch = 5, pairs = [[60, 68], [58, 62], [50, 54], [2, 6]], loss = 0.5652450323104858\n",
            "Epoch = 5, pairs = [[19, 21], [65, 66], [33, 35], [22, 24]], loss = 0.30117860436439514\n",
            "Epoch = 5, pairs = [[50, 52], [13, 14], [44, 46], [89, 90]], loss = 0.2948607802391052\n",
            "Epoch = 5, pairs = [[56, 60], [66, 67], [80, 84], [34, 36]], loss = 0.4884726405143738\n",
            "Epoch = 5, pairs = [[49, 51], [17, 18], [8, 16], [60, 62]], loss = 0.4550476670265198\n",
            "Epoch = 5, pairs = [[12, 14], [69, 70], [70, 71], [34, 38]], loss = 0.35612359642982483\n",
            "Epoch = 5, pairs = [[2, 3], [74, 75], [78, 82], [42, 43]], loss = 0.29383304715156555\n",
            "Epoch = 5, pairs = [[61, 62], [46, 47], [80, 88], [68, 76]], loss = 0.5441796779632568\n",
            "Epoch = 5, pairs = [[9, 11], [29, 31], [24, 40], [26, 27]], loss = 0.5528762340545654\n",
            "Epoch = 5, pairs = [[15, 17], [8, 12], [16, 32], [16, 48]], loss = 0.9718084931373596\n",
            "Epoch = 5, pairs = [[41, 42], [70, 74], [62, 63], [86, 88]], loss = 0.3582298159599304\n",
            "Epoch = 5, pairs = [[28, 30], [22, 26], [21, 22], [32, 40]], loss = 0.37645605206489563\n",
            "Epoch = 5, pairs = [[82, 83], [56, 64], [48, 64], [76, 78]], loss = 0.7958123087882996\n",
            "Epoch = 5, pairs = [[11, 13], [44, 52], [42, 46], [4, 8]], loss = 0.502575695514679\n",
            "Epoch = 5, pairs = [[63, 65], [6, 7], [25, 27], [20, 28]], loss = 0.3926543593406677\n",
            "Epoch = 5, pairs = [[61, 63], [47, 49], [0, 1], [12, 13]], loss = 0.29495784640312195\n",
            "Epoch = 5, pairs = [[38, 40], [64, 66], [56, 58], [14, 18]], loss = 0.4743115305900574\n",
            "Epoch = 5, pairs = [[45, 46], [64, 68], [44, 45], [85, 86]], loss = 0.31055790185928345\n",
            "Epoch = 5, pairs = [[58, 59], [24, 26], [36, 38], [66, 70]], loss = 0.39950764179229736\n",
            "Epoch = 5, pairs = [[75, 76], [64, 65], [32, 36], [59, 60]], loss = 0.32240474224090576\n",
            "Epoch = 5, pairs = [[51, 53], [32, 64], [82, 86], [79, 80]], loss = 0.702245831489563\n",
            "Epoch = 5, pairs = [[4, 12], [28, 29], [60, 61], [18, 20]], loss = 0.3417413830757141\n",
            "Epoch = 5, pairs = [[16, 18], [52, 60], [23, 25], [53, 54]], loss = 0.45784491300582886\n",
            "Epoch = 5, pairs = [[42, 44], [30, 34], [4, 6], [86, 90]], loss = 0.5027992725372314\n",
            "Epoch = 5, pairs = [[30, 31], [83, 85], [67, 68], [12, 20]], loss = 0.5079306960105896\n",
            "Epoch = 5, pairs = [[73, 74], [48, 49], [52, 53], [60, 64]], loss = 0.39740216732025146\n",
            "Epoch = 5, pairs = [[26, 30], [65, 67], [8, 24], [48, 80]], loss = 1.2601923942565918\n",
            "Epoch = 5, pairs = [[74, 78], [71, 72], [66, 68], [80, 81]], loss = 0.37211090326309204\n",
            "Epoch = 5, pairs = [[16, 24], [40, 42], [40, 48], [0, 8]], loss = 0.9281527400016785\n",
            "Epoch 5 took 84.58s.\n",
            "( 0,  1): reprojection: 0.223675, disparity: 0.078054\n",
            "( 0,  2): reprojection: 0.323509, disparity: 0.082584\n",
            "( 0,  4): reprojection: 0.351223, disparity: 0.108873\n",
            "( 0,  8): reprojection: 0.494846, disparity: 0.137317\n",
            "( 0, 16): reprojection: 1.195464, disparity: 0.120882\n",
            "( 0, 32): reprojection: 2.790145, disparity: 0.181484\n",
            "( 1,  2): reprojection: 0.186684, disparity: 0.063455\n",
            "( 1,  3): reprojection: 0.239740, disparity: 0.072175\n",
            "( 2,  3): reprojection: 0.111291, disparity: 0.063660\n",
            "( 2,  4): reprojection: 0.189145, disparity: 0.076502\n",
            "( 2,  6): reprojection: 0.411172, disparity: 0.089362\n",
            "( 3,  4): reprojection: 0.102785, disparity: 0.067665\n",
            "( 3,  5): reprojection: 0.223384, disparity: 0.079467\n",
            "( 4,  5): reprojection: 0.166540, disparity: 0.062436\n",
            "( 4,  6): reprojection: 0.256463, disparity: 0.070264\n",
            "( 4,  8): reprojection: 0.345777, disparity: 0.088019\n",
            "( 4, 12): reprojection: 0.741423, disparity: 0.109425\n",
            "( 5,  6): reprojection: 0.130596, disparity: 0.060940\n",
            "( 5,  7): reprojection: 0.169204, disparity: 0.071783\n",
            "( 6,  7): reprojection: 0.084233, disparity: 0.061786\n",
            "( 6,  8): reprojection: 0.163256, disparity: 0.068810\n",
            "( 6, 10): reprojection: 0.490394, disparity: 0.090387\n",
            "( 7,  8): reprojection: 0.102072, disparity: 0.060494\n",
            "( 7,  9): reprojection: 0.245251, disparity: 0.070524\n",
            "( 8,  9): reprojection: 0.184053, disparity: 0.063264\n",
            "( 8, 10): reprojection: 0.444235, disparity: 0.072560\n",
            "( 8, 12): reprojection: 0.716373, disparity: 0.098804\n",
            "( 8, 16): reprojection: 0.996146, disparity: 0.113910\n",
            "( 8, 24): reprojection: 1.975751, disparity: 0.137740\n",
            "( 9, 10): reprojection: 0.305654, disparity: 0.063522\n",
            "( 9, 11): reprojection: 0.486734, disparity: 0.071817\n",
            "(10, 11): reprojection: 0.222697, disparity: 0.057400\n",
            "(10, 12): reprojection: 0.323284, disparity: 0.073143\n",
            "(10, 14): reprojection: 0.548353, disparity: 0.090914\n",
            "(11, 12): reprojection: 0.175659, disparity: 0.056055\n",
            "(11, 13): reprojection: 0.312796, disparity: 0.076071\n",
            "(12, 13): reprojection: 0.162804, disparity: 0.060539\n",
            "(12, 14): reprojection: 0.337426, disparity: 0.076796\n",
            "(12, 16): reprojection: 0.498484, disparity: 0.115295\n",
            "(12, 20): reprojection: 1.088165, disparity: 0.097416\n",
            "(13, 14): reprojection: 0.225719, disparity: 0.063643\n",
            "(13, 15): reprojection: 0.247652, disparity: 0.078186\n",
            "(14, 15): reprojection: 0.197541, disparity: 0.067355\n",
            "(14, 16): reprojection: 0.484629, disparity: 0.083520\n",
            "(14, 18): reprojection: 0.534011, disparity: 0.099833\n",
            "(15, 16): reprojection: 0.319784, disparity: 0.073460\n",
            "(15, 17): reprojection: 0.437856, disparity: 0.084508\n",
            "(16, 17): reprojection: 0.177471, disparity: 0.062651\n",
            "(16, 18): reprojection: 0.443631, disparity: 0.084304\n",
            "(16, 20): reprojection: 0.871169, disparity: 0.109169\n",
            "(16, 24): reprojection: 1.405085, disparity: 0.125683\n",
            "(16, 32): reprojection: 1.778508, disparity: 0.145676\n",
            "(16, 48): reprojection: 4.289143, disparity: 0.495814\n",
            "(17, 18): reprojection: 0.390432, disparity: 0.067979\n",
            "(17, 19): reprojection: 0.644968, disparity: 0.093928\n",
            "(18, 19): reprojection: 0.284371, disparity: 0.067534\n",
            "(18, 20): reprojection: 0.458379, disparity: 0.079015\n",
            "(18, 22): reprojection: 0.723151, disparity: 0.090069\n",
            "(19, 20): reprojection: 0.169999, disparity: 0.067176\n",
            "(19, 21): reprojection: 0.383512, disparity: 0.080241\n",
            "(20, 21): reprojection: 0.227465, disparity: 0.066231\n",
            "(20, 22): reprojection: 0.366937, disparity: 0.080101\n",
            "(20, 24): reprojection: 0.599849, disparity: 0.091231\n",
            "(20, 28): reprojection: 0.921411, disparity: 0.141164\n",
            "(21, 22): reprojection: 0.181197, disparity: 0.065869\n",
            "(21, 23): reprojection: 0.292792, disparity: 0.078664\n",
            "(22, 23): reprojection: 0.167210, disparity: 0.074753\n",
            "(22, 24): reprojection: 0.273863, disparity: 0.079870\n",
            "(22, 26): reprojection: 0.591170, disparity: 0.098407\n",
            "(23, 24): reprojection: 0.151484, disparity: 0.065678\n",
            "(23, 25): reprojection: 0.318133, disparity: 0.081156\n",
            "(24, 25): reprojection: 0.186855, disparity: 0.070911\n",
            "(24, 26): reprojection: 0.332709, disparity: 0.076914\n",
            "(24, 28): reprojection: 0.513147, disparity: 0.107289\n",
            "(24, 32): reprojection: 0.886649, disparity: 0.133149\n",
            "(24, 40): reprojection: 1.893205, disparity: 0.244941\n",
            "(25, 26): reprojection: 0.189353, disparity: 0.074342\n",
            "(25, 27): reprojection: 0.351735, disparity: 0.084145\n",
            "(26, 27): reprojection: 0.181974, disparity: 0.097192\n",
            "(26, 28): reprojection: 0.288524, disparity: 0.097006\n",
            "(26, 30): reprojection: 0.551567, disparity: 0.112462\n",
            "(27, 28): reprojection: 0.197866, disparity: 0.065701\n",
            "(27, 29): reprojection: 0.376009, disparity: 0.082033\n",
            "(28, 29): reprojection: 0.231963, disparity: 0.068355\n",
            "(28, 30): reprojection: 0.356290, disparity: 0.091405\n",
            "(28, 32): reprojection: 0.502702, disparity: 0.101592\n",
            "(28, 36): reprojection: 0.838493, disparity: 0.158639\n",
            "(29, 30): reprojection: 0.171551, disparity: 0.069633\n",
            "(29, 31): reprojection: 0.301104, disparity: 0.079690\n",
            "(30, 31): reprojection: 0.204779, disparity: 0.065539\n",
            "(30, 32): reprojection: 0.271700, disparity: 0.081246\n",
            "(30, 34): reprojection: 0.437564, disparity: 0.112636\n",
            "(31, 32): reprojection: 0.157015, disparity: 0.072531\n",
            "(31, 33): reprojection: 0.298676, disparity: 0.082968\n",
            "(32, 33): reprojection: 0.234778, disparity: 0.072519\n",
            "(32, 34): reprojection: 0.250379, disparity: 0.082215\n",
            "(32, 36): reprojection: 0.430328, disparity: 0.119604\n",
            "(32, 40): reprojection: 0.950765, disparity: 0.233831\n",
            "(32, 48): reprojection: 1.942361, disparity: 0.313428\n",
            "(32, 64): reprojection: 5.054023, disparity: 0.314134\n",
            "(33, 34): reprojection: 0.198984, disparity: 0.066470\n",
            "(33, 35): reprojection: 0.306427, disparity: 0.083614\n",
            "(34, 35): reprojection: 0.198447, disparity: 0.068597\n",
            "(34, 36): reprojection: 0.321070, disparity: 0.092436\n",
            "(34, 38): reprojection: 0.544722, disparity: 0.132299\n",
            "(35, 36): reprojection: 0.260883, disparity: 0.076410\n",
            "(35, 37): reprojection: 0.346701, disparity: 0.112667\n",
            "(36, 37): reprojection: 0.270667, disparity: 0.087744\n",
            "(36, 38): reprojection: 0.474978, disparity: 0.090332\n",
            "(36, 40): reprojection: 0.612862, disparity: 0.137496\n",
            "(36, 44): reprojection: 1.120451, disparity: 0.198827\n",
            "(37, 38): reprojection: 0.267921, disparity: 0.067684\n",
            "(37, 39): reprojection: 0.331072, disparity: 0.093465\n",
            "(38, 39): reprojection: 0.223526, disparity: 0.081853\n",
            "(38, 40): reprojection: 0.506518, disparity: 0.100737\n",
            "(38, 42): reprojection: 1.003697, disparity: 0.144602\n",
            "(39, 40): reprojection: 0.333919, disparity: 0.072992\n",
            "(39, 41): reprojection: 0.623921, disparity: 0.097027\n",
            "(40, 41): reprojection: 0.363337, disparity: 0.080756\n",
            "(40, 42): reprojection: 0.613704, disparity: 0.096628\n",
            "(40, 44): reprojection: 0.843332, disparity: 0.118275\n",
            "(40, 48): reprojection: 1.273957, disparity: 0.177992\n",
            "(40, 56): reprojection: 2.431952, disparity: 0.206491\n",
            "(41, 42): reprojection: 0.286734, disparity: 0.076425\n",
            "(41, 43): reprojection: 0.415094, disparity: 0.094179\n",
            "(42, 43): reprojection: 0.224988, disparity: 0.073842\n",
            "(42, 44): reprojection: 0.418014, disparity: 0.088980\n",
            "(42, 46): reprojection: 0.711081, disparity: 0.106795\n",
            "(43, 44): reprojection: 0.216950, disparity: 0.075517\n",
            "(43, 45): reprojection: 0.347164, disparity: 0.094201\n",
            "(44, 45): reprojection: 0.188403, disparity: 0.073464\n",
            "(44, 46): reprojection: 0.380180, disparity: 0.086680\n",
            "(44, 48): reprojection: 0.585910, disparity: 0.129567\n",
            "(44, 52): reprojection: 1.077743, disparity: 0.139591\n",
            "(45, 46): reprojection: 0.233516, disparity: 0.077292\n",
            "(45, 47): reprojection: 0.373212, disparity: 0.106563\n",
            "(46, 47): reprojection: 0.213643, disparity: 0.079865\n",
            "(46, 48): reprojection: 0.414456, disparity: 0.099896\n",
            "(46, 50): reprojection: 0.634701, disparity: 0.104986\n",
            "(47, 48): reprojection: 0.240927, disparity: 0.077532\n",
            "(47, 49): reprojection: 0.354360, disparity: 0.085894\n",
            "(48, 49): reprojection: 0.221689, disparity: 0.087631\n",
            "(48, 50): reprojection: 0.340777, disparity: 0.103218\n",
            "(48, 52): reprojection: 0.600045, disparity: 0.102142\n",
            "(48, 56): reprojection: 1.227410, disparity: 0.124082\n",
            "(48, 64): reprojection: 3.122900, disparity: 0.246296\n",
            "(48, 80): reprojection: 7.698462, disparity: 0.140314\n",
            "(49, 50): reprojection: 0.189388, disparity: 0.079713\n",
            "(49, 51): reprojection: 0.344866, disparity: 0.088612\n",
            "(50, 51): reprojection: 0.198286, disparity: 0.077687\n",
            "(50, 52): reprojection: 0.415243, disparity: 0.084233\n",
            "(50, 54): reprojection: 0.807979, disparity: 0.100052\n",
            "(51, 52): reprojection: 0.251194, disparity: 0.072837\n",
            "(51, 53): reprojection: 0.442930, disparity: 0.082886\n",
            "(52, 53): reprojection: 0.246997, disparity: 0.075556\n",
            "(52, 54): reprojection: 0.400926, disparity: 0.085978\n",
            "(52, 56): reprojection: 0.870439, disparity: 0.095326\n",
            "(52, 60): reprojection: 1.550491, disparity: 0.143175\n",
            "(53, 54): reprojection: 0.211628, disparity: 0.076119\n",
            "(53, 55): reprojection: 0.469100, disparity: 0.089110\n",
            "(54, 55): reprojection: 0.371416, disparity: 0.074421\n",
            "(54, 56): reprojection: 0.715572, disparity: 0.083882\n",
            "(54, 58): reprojection: 0.735157, disparity: 0.096608\n",
            "(55, 56): reprojection: 0.389424, disparity: 0.079605\n",
            "(55, 57): reprojection: 0.502512, disparity: 0.088269\n",
            "(56, 57): reprojection: 0.365560, disparity: 0.080747\n",
            "(56, 58): reprojection: 0.705999, disparity: 0.095877\n",
            "(56, 60): reprojection: 1.135520, disparity: 0.129629\n",
            "(56, 64): reprojection: 1.724294, disparity: 0.162401\n",
            "(56, 72): reprojection: 3.869252, disparity: 0.163744\n",
            "(57, 58): reprojection: 0.431147, disparity: 0.085368\n",
            "(57, 59): reprojection: 0.678164, disparity: 0.101586\n",
            "(58, 59): reprojection: 0.332969, disparity: 0.085212\n",
            "(58, 60): reprojection: 0.518052, disparity: 0.102995\n",
            "(58, 62): reprojection: 0.972640, disparity: 0.131055\n",
            "(59, 60): reprojection: 0.257810, disparity: 0.091863\n",
            "(59, 61): reprojection: 0.444977, disparity: 0.110651\n",
            "(60, 61): reprojection: 0.295071, disparity: 0.089706\n",
            "(60, 62): reprojection: 0.485843, disparity: 0.098720\n",
            "(60, 64): reprojection: 0.951195, disparity: 0.116687\n",
            "(60, 68): reprojection: 1.893498, disparity: 0.142437\n",
            "(61, 62): reprojection: 0.299109, disparity: 0.090296\n",
            "(61, 63): reprojection: 0.526434, disparity: 0.095990\n",
            "(62, 63): reprojection: 0.325978, disparity: 0.083608\n",
            "(62, 64): reprojection: 0.504582, disparity: 0.094383\n",
            "(62, 66): reprojection: 0.895743, disparity: 0.134890\n",
            "(63, 64): reprojection: 0.344285, disparity: 0.104401\n",
            "(63, 65): reprojection: 0.568984, disparity: 0.094756\n",
            "(64, 65): reprojection: 0.302081, disparity: 0.091862\n",
            "(64, 66): reprojection: 0.564911, disparity: 0.090391\n",
            "(64, 68): reprojection: 0.837172, disparity: 0.112662\n",
            "(64, 72): reprojection: 1.770601, disparity: 0.211602\n",
            "(64, 80): reprojection: 3.049512, disparity: 0.201135\n",
            "(65, 66): reprojection: 0.377022, disparity: 0.078363\n",
            "(65, 67): reprojection: 0.651050, disparity: 0.090147\n",
            "(66, 67): reprojection: 0.347683, disparity: 0.084500\n",
            "(66, 68): reprojection: 0.606662, disparity: 0.088576\n",
            "(66, 70): reprojection: 1.059660, disparity: 0.116299\n",
            "(67, 68): reprojection: 0.440571, disparity: 0.079190\n",
            "(67, 69): reprojection: 0.626523, disparity: 0.089062\n",
            "(68, 69): reprojection: 0.328342, disparity: 0.082552\n",
            "(68, 70): reprojection: 0.718645, disparity: 0.100053\n",
            "(68, 72): reprojection: 1.071182, disparity: 0.141320\n",
            "(68, 76): reprojection: 1.802824, disparity: 0.205807\n",
            "(69, 70): reprojection: 0.455463, disparity: 0.079875\n",
            "(69, 71): reprojection: 0.725090, disparity: 0.099084\n",
            "(70, 71): reprojection: 0.338245, disparity: 0.083859\n",
            "(70, 72): reprojection: 0.490541, disparity: 0.094205\n",
            "(70, 74): reprojection: 0.915393, disparity: 0.135522\n",
            "(71, 72): reprojection: 0.348238, disparity: 0.079589\n",
            "(71, 73): reprojection: 0.594129, disparity: 0.092214\n",
            "(72, 73): reprojection: 0.334950, disparity: 0.081793\n",
            "(72, 74): reprojection: 0.527855, disparity: 0.101409\n",
            "(72, 76): reprojection: 0.911412, disparity: 0.128228\n",
            "(72, 80): reprojection: 1.344917, disparity: 0.182875\n",
            "(72, 88): reprojection: 2.520815, disparity: 0.210515\n",
            "(73, 74): reprojection: 0.401218, disparity: 0.096327\n",
            "(73, 75): reprojection: 0.617133, disparity: 0.112812\n",
            "(74, 75): reprojection: 0.318162, disparity: 0.088236\n",
            "(74, 76): reprojection: 0.446245, disparity: 0.095314\n",
            "(74, 78): reprojection: 0.727544, disparity: 0.118393\n",
            "(75, 76): reprojection: 0.314619, disparity: 0.077716\n",
            "(75, 77): reprojection: 0.539474, disparity: 0.095561\n",
            "(76, 77): reprojection: 0.303038, disparity: 0.077787\n",
            "(76, 78): reprojection: 0.475660, disparity: 0.087755\n",
            "(76, 80): reprojection: 0.653416, disparity: 0.109114\n",
            "(76, 84): reprojection: 1.231108, disparity: 0.156536\n",
            "(77, 78): reprojection: 0.291521, disparity: 0.074811\n",
            "(77, 79): reprojection: 0.419104, disparity: 0.088342\n",
            "(78, 79): reprojection: 0.264666, disparity: 0.078933\n",
            "(78, 80): reprojection: 0.397498, disparity: 0.090899\n",
            "(78, 82): reprojection: 0.744210, disparity: 0.123922\n",
            "(79, 80): reprojection: 0.203615, disparity: 0.076999\n",
            "(79, 81): reprojection: 0.369747, disparity: 0.085493\n",
            "(80, 81): reprojection: 0.219144, disparity: 0.074025\n",
            "(80, 82): reprojection: 0.372506, disparity: 0.095618\n",
            "(80, 84): reprojection: 0.748887, disparity: 0.106559\n",
            "(80, 88): reprojection: 1.267876, disparity: 0.133897\n",
            "(81, 82): reprojection: 0.254860, disparity: 0.084911\n",
            "(81, 83): reprojection: 0.478016, disparity: 0.090059\n",
            "(82, 83): reprojection: 0.234626, disparity: 0.075030\n",
            "(82, 84): reprojection: 0.387059, disparity: 0.079721\n",
            "(82, 86): reprojection: 0.742650, disparity: 0.099057\n",
            "(83, 84): reprojection: 0.214515, disparity: 0.073554\n",
            "(83, 85): reprojection: 0.452111, disparity: 0.087460\n",
            "(84, 85): reprojection: 0.224021, disparity: 0.069908\n",
            "(84, 86): reprojection: 0.384781, disparity: 0.083574\n",
            "(84, 88): reprojection: 0.713485, disparity: 0.094286\n",
            "(85, 86): reprojection: 0.212398, disparity: 0.069932\n",
            "(85, 87): reprojection: 0.387317, disparity: 0.081611\n",
            "(86, 87): reprojection: 0.283143, disparity: 0.072335\n",
            "(86, 88): reprojection: 0.436864, disparity: 0.091990\n",
            "(86, 90): reprojection: 0.577168, disparity: 0.095894\n",
            "(87, 88): reprojection: 0.222873, disparity: 0.075821\n",
            "(87, 89): reprojection: 0.330093, disparity: 0.079083\n",
            "(88, 89): reprojection: 0.153794, disparity: 0.068522\n",
            "(88, 90): reprojection: 0.329965, disparity: 0.076472\n",
            "(89, 90): reprojection: 0.218069, disparity: 0.068584\n",
            "(89, 91): reprojection: 0.469584, disparity: 0.091878\n",
            "(90, 91): reprojection: 0.374607, disparity: 0.080853\n",
            "Mean:     reprojection: 0.374607, disparity: 0.080853\n",
            "Done Validation for epoch 6 (1560 iterations)\n",
            "Epoch = 6, pairs = [[58, 59], [84, 88], [8, 24], [45, 47]], loss = 0.8765696287155151\n",
            "Epoch = 6, pairs = [[51, 53], [48, 56], [3, 4], [70, 72]], loss = 0.6721330285072327\n",
            "Epoch = 6, pairs = [[69, 70], [32, 33], [55, 57], [19, 21]], loss = 0.45064347982406616\n",
            "Epoch = 6, pairs = [[52, 53], [0, 4], [35, 36], [1, 2]], loss = 0.3441014885902405\n",
            "Epoch = 6, pairs = [[7, 8], [4, 8], [25, 27], [83, 85]], loss = 0.37658703327178955\n",
            "Epoch = 6, pairs = [[24, 32], [68, 76], [10, 14], [60, 62]], loss = 1.1317362785339355\n",
            "Epoch = 6, pairs = [[85, 87], [52, 56], [9, 11], [56, 60]], loss = 0.6802322864532471\n",
            "Epoch = 6, pairs = [[89, 90], [5, 7], [76, 78], [4, 6]], loss = 0.3528164029121399\n",
            "Epoch = 6, pairs = [[10, 12], [43, 44], [40, 48], [77, 78]], loss = 0.5522257685661316\n",
            "Epoch = 6, pairs = [[14, 18], [48, 64], [84, 85], [65, 66]], loss = 0.8643605709075928\n",
            "Epoch = 6, pairs = [[22, 26], [52, 60], [52, 54], [39, 41]], loss = 0.7133311033248901\n",
            "Epoch = 6, pairs = [[85, 86], [5, 6], [28, 32], [76, 84]], loss = 0.4678252935409546\n",
            "Epoch = 6, pairs = [[16, 32], [72, 76], [28, 36], [4, 12]], loss = 0.7863860130310059\n",
            "Epoch = 6, pairs = [[61, 63], [6, 10], [76, 77], [18, 19]], loss = 0.3913460969924927\n",
            "Epoch = 6, pairs = [[23, 25], [38, 40], [81, 83], [16, 48]], loss = 0.8206391930580139\n",
            "Epoch = 6, pairs = [[32, 34], [0, 32], [44, 45], [26, 27]], loss = 0.6738114356994629\n",
            "Epoch = 6, pairs = [[80, 82], [50, 52], [80, 81], [66, 70]], loss = 0.5026254653930664\n",
            "Epoch = 6, pairs = [[84, 86], [3, 5], [72, 88], [13, 15]], loss = 0.6280252933502197\n",
            "Epoch = 6, pairs = [[32, 48], [64, 80], [28, 29], [88, 89]], loss = 0.8801038265228271\n",
            "Epoch = 6, pairs = [[68, 72], [76, 80], [24, 40], [46, 47]], loss = 0.7526249289512634\n",
            "Epoch = 6, pairs = [[68, 70], [90, 91], [59, 60], [21, 23]], loss = 0.37696516513824463\n",
            "Epoch = 6, pairs = [[59, 61], [34, 38], [10, 11], [48, 80]], loss = 0.8678134679794312\n",
            "Epoch = 6, pairs = [[82, 86], [72, 73], [78, 79], [64, 66]], loss = 0.3937010169029236\n",
            "Epoch = 6, pairs = [[38, 42], [54, 55], [67, 68], [15, 17]], loss = 0.5728368759155273\n",
            "Epoch = 6, pairs = [[2, 4], [87, 89], [14, 15], [22, 24]], loss = 0.312539279460907\n",
            "Epoch = 6, pairs = [[40, 56], [7, 9], [73, 75], [53, 55]], loss = 0.8677151799201965\n",
            "Epoch = 6, pairs = [[51, 52], [58, 62], [74, 75], [12, 16]], loss = 0.5129828453063965\n",
            "Epoch = 6, pairs = [[71, 73], [26, 28], [31, 33], [82, 84]], loss = 0.4607117176055908\n",
            "Epoch = 6, pairs = [[68, 69], [47, 48], [86, 90], [8, 9]], loss = 0.3640827536582947\n",
            "Epoch = 6, pairs = [[20, 21], [28, 30], [78, 80], [24, 25]], loss = 0.3402601480484009\n",
            "Epoch = 6, pairs = [[2, 6], [15, 16], [74, 78], [48, 50]], loss = 0.4933045506477356\n",
            "Epoch = 6, pairs = [[46, 48], [62, 64], [11, 13], [36, 40]], loss = 0.46594950556755066\n",
            "Epoch = 6, pairs = [[81, 82], [18, 20], [6, 7], [20, 22]], loss = 0.3116718530654907\n",
            "Epoch = 6, pairs = [[41, 42], [66, 68], [0, 8], [45, 46]], loss = 0.4258512854576111\n",
            "Epoch = 6, pairs = [[2, 3], [0, 2], [70, 74], [30, 31]], loss = 0.38710933923721313\n",
            "Epoch = 6, pairs = [[43, 45], [77, 79], [44, 46], [32, 64]], loss = 0.856266975402832\n",
            "Epoch = 6, pairs = [[34, 35], [30, 34], [60, 64], [20, 24]], loss = 0.511674165725708\n",
            "Epoch = 6, pairs = [[16, 24], [62, 66], [86, 88], [12, 14]], loss = 0.5718575716018677\n",
            "Epoch = 6, pairs = [[17, 18], [62, 63], [50, 54], [27, 29]], loss = 0.3873863220214844\n",
            "Epoch = 6, pairs = [[12, 13], [33, 35], [41, 43], [60, 68]], loss = 0.454582154750824\n",
            "Epoch = 6, pairs = [[56, 64], [16, 18], [20, 28], [25, 26]], loss = 0.5959025025367737\n",
            "Epoch = 6, pairs = [[49, 50], [67, 69], [0, 1], [88, 90]], loss = 0.33636486530303955\n",
            "Epoch = 6, pairs = [[42, 44], [64, 72], [0, 16], [42, 46]], loss = 0.7399137020111084\n",
            "Epoch = 6, pairs = [[70, 71], [50, 51], [87, 88], [32, 36]], loss = 0.301716148853302\n",
            "Epoch = 6, pairs = [[21, 22], [54, 58], [24, 28], [36, 37]], loss = 0.3462693691253662\n",
            "Epoch = 6, pairs = [[75, 76], [73, 74], [79, 80], [11, 12]], loss = 0.27854669094085693\n",
            "Epoch = 6, pairs = [[46, 50], [64, 68], [80, 88], [40, 42]], loss = 0.6562798619270325\n",
            "Epoch = 6, pairs = [[83, 84], [39, 40], [54, 56], [74, 76]], loss = 0.37661004066467285\n",
            "Epoch = 6, pairs = [[80, 84], [38, 39], [29, 30], [27, 28]], loss = 0.28977513313293457\n",
            "Epoch = 6, pairs = [[13, 14], [35, 37], [19, 20], [71, 72]], loss = 0.26148805022239685\n",
            "Epoch = 6, pairs = [[44, 48], [36, 44], [57, 59], [47, 49]], loss = 0.6057643890380859\n",
            "Epoch = 6, pairs = [[48, 49], [58, 60], [56, 57], [42, 43]], loss = 0.32150527834892273\n",
            "Epoch = 6, pairs = [[40, 44], [30, 32], [37, 38], [24, 26]], loss = 0.39703041315078735\n",
            "Epoch = 6, pairs = [[65, 67], [18, 22], [49, 51], [12, 20]], loss = 0.5539021492004395\n",
            "Epoch = 6, pairs = [[37, 39], [4, 5], [9, 10], [89, 91]], loss = 0.3304976522922516\n",
            "Epoch = 6, pairs = [[33, 34], [66, 67], [57, 58], [72, 80]], loss = 0.4350634217262268\n",
            "Epoch = 6, pairs = [[75, 77], [14, 16], [72, 74], [8, 16]], loss = 0.5289772748947144\n",
            "Epoch = 6, pairs = [[55, 56], [44, 52], [8, 12], [48, 52]], loss = 0.5367931127548218\n",
            "Epoch = 6, pairs = [[69, 71], [17, 19], [61, 62], [32, 40]], loss = 0.49266254901885986\n",
            "Epoch = 6, pairs = [[8, 10], [56, 58], [40, 41], [63, 64]], loss = 0.40735238790512085\n",
            "Epoch = 6, pairs = [[36, 38], [78, 82], [22, 23], [31, 32]], loss = 0.3128195106983185\n",
            "Epoch = 6, pairs = [[23, 24], [16, 17], [56, 72], [26, 30]], loss = 0.47603142261505127\n",
            "Epoch = 6, pairs = [[63, 65], [16, 20], [60, 61], [86, 87]], loss = 0.4113449454307556\n",
            "Epoch = 6, pairs = [[34, 36], [79, 81], [82, 83], [29, 31]], loss = 0.29497936367988586\n",
            "Epoch = 6, pairs = [[53, 54], [6, 8], [1, 3], [64, 65]], loss = 0.25631439685821533\n",
            "Epoch 6 took 85.32s.\n",
            "( 0,  1): reprojection: 0.236160, disparity: 0.062788\n",
            "( 0,  2): reprojection: 0.351569, disparity: 0.060854\n",
            "( 0,  4): reprojection: 0.394656, disparity: 0.073701\n",
            "( 0,  8): reprojection: 0.499370, disparity: 0.079271\n",
            "( 0, 16): reprojection: 0.781647, disparity: 0.083287\n",
            "( 0, 32): reprojection: 1.469233, disparity: 0.144008\n",
            "( 1,  2): reprojection: 0.196698, disparity: 0.048680\n",
            "( 1,  3): reprojection: 0.250374, disparity: 0.056489\n",
            "( 2,  3): reprojection: 0.109076, disparity: 0.049546\n",
            "( 2,  4): reprojection: 0.172857, disparity: 0.058268\n",
            "( 2,  6): reprojection: 0.338264, disparity: 0.066105\n",
            "( 3,  4): reprojection: 0.109373, disparity: 0.052010\n",
            "( 3,  5): reprojection: 0.196239, disparity: 0.056194\n",
            "( 4,  5): reprojection: 0.152667, disparity: 0.050459\n",
            "( 4,  6): reprojection: 0.214476, disparity: 0.052303\n",
            "( 4,  8): reprojection: 0.290084, disparity: 0.065961\n",
            "( 4, 12): reprojection: 0.390961, disparity: 0.078299\n",
            "( 5,  6): reprojection: 0.119056, disparity: 0.049097\n",
            "( 5,  7): reprojection: 0.162296, disparity: 0.052832\n",
            "( 6,  7): reprojection: 0.100619, disparity: 0.046485\n",
            "( 6,  8): reprojection: 0.166149, disparity: 0.052265\n",
            "( 6, 10): reprojection: 0.405456, disparity: 0.061539\n",
            "( 7,  8): reprojection: 0.091494, disparity: 0.047291\n",
            "( 7,  9): reprojection: 0.196832, disparity: 0.053088\n",
            "( 8,  9): reprojection: 0.162745, disparity: 0.046769\n",
            "( 8, 10): reprojection: 0.361073, disparity: 0.053632\n",
            "( 8, 12): reprojection: 0.501249, disparity: 0.063004\n",
            "( 8, 16): reprojection: 0.635878, disparity: 0.077642\n",
            "( 8, 24): reprojection: 0.832290, disparity: 0.100929\n",
            "( 9, 10): reprojection: 0.246778, disparity: 0.048206\n",
            "( 9, 11): reprojection: 0.367201, disparity: 0.054475\n",
            "(10, 11): reprojection: 0.157295, disparity: 0.043154\n",
            "(10, 12): reprojection: 0.197369, disparity: 0.050206\n",
            "(10, 14): reprojection: 0.327003, disparity: 0.066535\n",
            "(11, 12): reprojection: 0.131051, disparity: 0.042870\n",
            "(11, 13): reprojection: 0.207339, disparity: 0.053105\n",
            "(12, 13): reprojection: 0.135156, disparity: 0.044534\n",
            "(12, 14): reprojection: 0.265847, disparity: 0.053414\n",
            "(12, 16): reprojection: 0.440775, disparity: 0.077163\n",
            "(12, 20): reprojection: 0.702068, disparity: 0.087907\n",
            "(13, 14): reprojection: 0.180544, disparity: 0.046129\n",
            "(13, 15): reprojection: 0.249074, disparity: 0.057213\n",
            "(14, 15): reprojection: 0.191884, disparity: 0.048866\n",
            "(14, 16): reprojection: 0.427723, disparity: 0.059942\n",
            "(14, 18): reprojection: 0.366754, disparity: 0.071151\n",
            "(15, 16): reprojection: 0.297875, disparity: 0.051267\n",
            "(15, 17): reprojection: 0.379214, disparity: 0.059520\n",
            "(16, 17): reprojection: 0.141414, disparity: 0.048332\n",
            "(16, 18): reprojection: 0.323632, disparity: 0.061944\n",
            "(16, 20): reprojection: 0.569075, disparity: 0.078917\n",
            "(16, 24): reprojection: 0.840403, disparity: 0.094799\n",
            "(16, 32): reprojection: 1.494101, disparity: 0.138186\n",
            "(16, 48): reprojection: 1.977330, disparity: 0.199747\n",
            "(17, 18): reprojection: 0.311730, disparity: 0.054239\n",
            "(17, 19): reprojection: 0.484047, disparity: 0.063588\n",
            "(18, 19): reprojection: 0.213613, disparity: 0.048068\n",
            "(18, 20): reprojection: 0.296412, disparity: 0.060391\n",
            "(18, 22): reprojection: 0.505315, disparity: 0.070691\n",
            "(19, 20): reprojection: 0.119461, disparity: 0.047502\n",
            "(19, 21): reprojection: 0.251978, disparity: 0.059332\n",
            "(20, 21): reprojection: 0.195693, disparity: 0.050370\n",
            "(20, 22): reprojection: 0.297462, disparity: 0.058012\n",
            "(20, 24): reprojection: 0.396330, disparity: 0.067964\n",
            "(20, 28): reprojection: 0.721220, disparity: 0.095283\n",
            "(21, 22): reprojection: 0.162174, disparity: 0.049238\n",
            "(21, 23): reprojection: 0.252282, disparity: 0.056894\n",
            "(22, 23): reprojection: 0.127568, disparity: 0.049393\n",
            "(22, 24): reprojection: 0.185703, disparity: 0.061688\n",
            "(22, 26): reprojection: 0.334418, disparity: 0.078415\n",
            "(23, 24): reprojection: 0.111045, disparity: 0.052461\n",
            "(23, 25): reprojection: 0.184151, disparity: 0.062096\n",
            "(24, 25): reprojection: 0.141904, disparity: 0.052554\n",
            "(24, 26): reprojection: 0.187485, disparity: 0.061537\n",
            "(24, 28): reprojection: 0.373311, disparity: 0.076142\n",
            "(24, 32): reprojection: 0.782240, disparity: 0.105803\n",
            "(24, 40): reprojection: 1.067092, disparity: 0.119257\n",
            "(25, 26): reprojection: 0.132288, disparity: 0.055062\n",
            "(25, 27): reprojection: 0.230110, disparity: 0.062141\n",
            "(26, 27): reprojection: 0.150660, disparity: 0.053230\n",
            "(26, 28): reprojection: 0.200094, disparity: 0.061412\n",
            "(26, 30): reprojection: 0.368226, disparity: 0.074530\n",
            "(27, 28): reprojection: 0.141300, disparity: 0.052315\n",
            "(27, 29): reprojection: 0.263076, disparity: 0.061757\n",
            "(28, 29): reprojection: 0.182829, disparity: 0.058589\n",
            "(28, 30): reprojection: 0.250974, disparity: 0.070564\n",
            "(28, 32): reprojection: 0.467319, disparity: 0.083951\n",
            "(28, 36): reprojection: 0.781545, disparity: 0.092055\n",
            "(29, 30): reprojection: 0.133044, disparity: 0.052820\n",
            "(29, 31): reprojection: 0.283228, disparity: 0.063353\n",
            "(30, 31): reprojection: 0.204805, disparity: 0.052661\n",
            "(30, 32): reprojection: 0.287494, disparity: 0.062776\n",
            "(30, 34): reprojection: 0.507257, disparity: 0.072298\n",
            "(31, 32): reprojection: 0.136417, disparity: 0.052588\n",
            "(31, 33): reprojection: 0.326140, disparity: 0.058796\n",
            "(32, 33): reprojection: 0.249580, disparity: 0.056077\n",
            "(32, 34): reprojection: 0.268260, disparity: 0.060942\n",
            "(32, 36): reprojection: 0.386795, disparity: 0.072413\n",
            "(32, 40): reprojection: 0.600332, disparity: 0.103209\n",
            "(32, 48): reprojection: 1.134997, disparity: 0.143461\n",
            "(32, 64): reprojection: 1.676318, disparity: 0.185083\n",
            "(33, 34): reprojection: 0.148382, disparity: 0.052889\n",
            "(33, 35): reprojection: 0.267405, disparity: 0.061812\n",
            "(34, 35): reprojection: 0.187054, disparity: 0.051866\n",
            "(34, 36): reprojection: 0.293942, disparity: 0.060059\n",
            "(34, 38): reprojection: 0.404615, disparity: 0.075448\n",
            "(35, 36): reprojection: 0.234541, disparity: 0.056255\n",
            "(35, 37): reprojection: 0.293600, disparity: 0.067504\n",
            "(36, 37): reprojection: 0.268635, disparity: 0.061816\n",
            "(36, 38): reprojection: 0.396351, disparity: 0.066060\n",
            "(36, 40): reprojection: 0.334049, disparity: 0.073376\n",
            "(36, 44): reprojection: 0.751448, disparity: 0.109672\n",
            "(37, 38): reprojection: 0.223532, disparity: 0.055028\n",
            "(37, 39): reprojection: 0.252023, disparity: 0.060940\n",
            "(38, 39): reprojection: 0.193349, disparity: 0.054221\n",
            "(38, 40): reprojection: 0.364332, disparity: 0.063258\n",
            "(38, 42): reprojection: 0.664294, disparity: 0.086769\n",
            "(39, 40): reprojection: 0.278543, disparity: 0.055924\n",
            "(39, 41): reprojection: 0.487302, disparity: 0.066966\n",
            "(40, 41): reprojection: 0.285830, disparity: 0.059822\n",
            "(40, 42): reprojection: 0.413636, disparity: 0.070382\n",
            "(40, 44): reprojection: 0.559677, disparity: 0.082588\n",
            "(40, 48): reprojection: 0.839874, disparity: 0.102346\n",
            "(40, 56): reprojection: 0.891420, disparity: 0.126823\n",
            "(41, 42): reprojection: 0.193594, disparity: 0.063460\n",
            "(41, 43): reprojection: 0.313921, disparity: 0.074211\n",
            "(42, 43): reprojection: 0.179643, disparity: 0.059143\n",
            "(42, 44): reprojection: 0.335797, disparity: 0.068352\n",
            "(42, 46): reprojection: 0.550225, disparity: 0.078639\n",
            "(43, 44): reprojection: 0.213563, disparity: 0.058303\n",
            "(43, 45): reprojection: 0.318322, disparity: 0.070420\n",
            "(44, 45): reprojection: 0.151850, disparity: 0.061383\n",
            "(44, 46): reprojection: 0.292813, disparity: 0.069571\n",
            "(44, 48): reprojection: 0.479075, disparity: 0.084498\n",
            "(44, 52): reprojection: 0.747424, disparity: 0.115730\n",
            "(45, 46): reprojection: 0.208007, disparity: 0.060591\n",
            "(45, 47): reprojection: 0.344506, disparity: 0.073702\n",
            "(46, 47): reprojection: 0.196723, disparity: 0.060379\n",
            "(46, 48): reprojection: 0.298977, disparity: 0.074854\n",
            "(46, 50): reprojection: 0.412731, disparity: 0.086111\n",
            "(47, 48): reprojection: 0.205059, disparity: 0.060955\n",
            "(47, 49): reprojection: 0.265765, disparity: 0.070354\n",
            "(48, 49): reprojection: 0.170159, disparity: 0.063031\n",
            "(48, 50): reprojection: 0.242268, disparity: 0.071010\n",
            "(48, 52): reprojection: 0.400837, disparity: 0.087817\n",
            "(48, 56): reprojection: 0.784337, disparity: 0.109353\n",
            "(48, 64): reprojection: 1.029855, disparity: 0.130427\n",
            "(48, 80): reprojection: 2.200672, disparity: 0.144355\n",
            "(49, 50): reprojection: 0.145363, disparity: 0.063633\n",
            "(49, 51): reprojection: 0.245727, disparity: 0.070467\n",
            "(50, 51): reprojection: 0.158505, disparity: 0.061762\n",
            "(50, 52): reprojection: 0.259831, disparity: 0.068151\n",
            "(50, 54): reprojection: 0.407692, disparity: 0.083088\n",
            "(51, 52): reprojection: 0.170181, disparity: 0.059996\n",
            "(51, 53): reprojection: 0.311522, disparity: 0.068750\n",
            "(52, 53): reprojection: 0.174038, disparity: 0.063143\n",
            "(52, 54): reprojection: 0.263981, disparity: 0.074038\n",
            "(52, 56): reprojection: 0.534236, disparity: 0.075434\n",
            "(52, 60): reprojection: 0.648811, disparity: 0.092943\n",
            "(53, 54): reprojection: 0.135525, disparity: 0.063169\n",
            "(53, 55): reprojection: 0.327449, disparity: 0.073691\n",
            "(54, 55): reprojection: 0.294982, disparity: 0.058998\n",
            "(54, 56): reprojection: 0.558323, disparity: 0.066316\n",
            "(54, 58): reprojection: 0.421264, disparity: 0.074729\n",
            "(55, 56): reprojection: 0.348267, disparity: 0.064132\n",
            "(55, 57): reprojection: 0.392010, disparity: 0.066566\n",
            "(56, 57): reprojection: 0.301087, disparity: 0.063176\n",
            "(56, 58): reprojection: 0.545004, disparity: 0.075429\n",
            "(56, 60): reprojection: 0.686838, disparity: 0.084042\n",
            "(56, 64): reprojection: 0.951796, disparity: 0.093199\n",
            "(56, 72): reprojection: 1.146464, disparity: 0.112481\n",
            "(57, 58): reprojection: 0.353650, disparity: 0.069142\n",
            "(57, 59): reprojection: 0.476478, disparity: 0.074748\n",
            "(58, 59): reprojection: 0.242323, disparity: 0.066702\n",
            "(58, 60): reprojection: 0.389307, disparity: 0.075799\n",
            "(58, 62): reprojection: 0.471412, disparity: 0.092669\n",
            "(59, 60): reprojection: 0.197817, disparity: 0.073095\n",
            "(59, 61): reprojection: 0.335127, disparity: 0.076034\n",
            "(60, 61): reprojection: 0.218946, disparity: 0.072280\n",
            "(60, 62): reprojection: 0.308557, disparity: 0.080465\n",
            "(60, 64): reprojection: 0.503050, disparity: 0.088596\n",
            "(60, 68): reprojection: 0.622532, disparity: 0.105557\n",
            "(61, 62): reprojection: 0.204274, disparity: 0.075335\n",
            "(61, 63): reprojection: 0.300283, disparity: 0.078810\n",
            "(62, 63): reprojection: 0.222251, disparity: 0.070426\n",
            "(62, 64): reprojection: 0.333162, disparity: 0.077030\n",
            "(62, 66): reprojection: 0.535191, disparity: 0.090392\n",
            "(63, 64): reprojection: 0.212600, disparity: 0.071635\n",
            "(63, 65): reprojection: 0.341281, disparity: 0.076792\n",
            "(64, 65): reprojection: 0.224679, disparity: 0.074828\n",
            "(64, 66): reprojection: 0.372635, disparity: 0.074907\n",
            "(64, 68): reprojection: 0.506993, disparity: 0.087655\n",
            "(64, 72): reprojection: 0.891780, disparity: 0.104296\n",
            "(64, 80): reprojection: 1.455036, disparity: 0.123956\n",
            "(65, 66): reprojection: 0.263981, disparity: 0.065489\n",
            "(65, 67): reprojection: 0.403914, disparity: 0.072553\n",
            "(66, 67): reprojection: 0.254409, disparity: 0.067485\n",
            "(66, 68): reprojection: 0.375954, disparity: 0.072991\n",
            "(66, 70): reprojection: 0.474627, disparity: 0.084359\n",
            "(67, 68): reprojection: 0.345263, disparity: 0.067465\n",
            "(67, 69): reprojection: 0.340109, disparity: 0.072379\n",
            "(68, 69): reprojection: 0.204428, disparity: 0.064119\n",
            "(68, 70): reprojection: 0.443955, disparity: 0.071475\n",
            "(68, 72): reprojection: 0.535076, disparity: 0.084670\n",
            "(68, 76): reprojection: 0.740935, disparity: 0.094915\n",
            "(69, 70): reprojection: 0.296758, disparity: 0.063925\n",
            "(69, 71): reprojection: 0.425355, disparity: 0.068490\n",
            "(70, 71): reprojection: 0.209653, disparity: 0.064489\n",
            "(70, 72): reprojection: 0.287494, disparity: 0.069227\n",
            "(70, 74): reprojection: 0.444418, disparity: 0.081360\n",
            "(71, 72): reprojection: 0.225319, disparity: 0.064831\n",
            "(71, 73): reprojection: 0.346338, disparity: 0.072878\n",
            "(72, 73): reprojection: 0.226930, disparity: 0.064685\n",
            "(72, 74): reprojection: 0.314397, disparity: 0.070607\n",
            "(72, 76): reprojection: 0.407997, disparity: 0.078299\n",
            "(72, 80): reprojection: 0.745706, disparity: 0.098472\n",
            "(72, 88): reprojection: 0.976612, disparity: 0.121450\n",
            "(73, 74): reprojection: 0.320952, disparity: 0.065619\n",
            "(73, 75): reprojection: 0.442105, disparity: 0.073605\n",
            "(74, 75): reprojection: 0.239185, disparity: 0.064950\n",
            "(74, 76): reprojection: 0.264831, disparity: 0.067646\n",
            "(74, 78): reprojection: 0.421219, disparity: 0.075998\n",
            "(75, 76): reprojection: 0.221802, disparity: 0.061864\n",
            "(75, 77): reprojection: 0.344801, disparity: 0.068451\n",
            "(76, 77): reprojection: 0.226785, disparity: 0.060539\n",
            "(76, 78): reprojection: 0.350069, disparity: 0.064907\n",
            "(76, 80): reprojection: 0.475624, disparity: 0.076134\n",
            "(76, 84): reprojection: 0.723207, disparity: 0.092777\n",
            "(77, 78): reprojection: 0.226785, disparity: 0.061808\n",
            "(77, 79): reprojection: 0.266153, disparity: 0.067951\n",
            "(78, 79): reprojection: 0.195352, disparity: 0.064057\n",
            "(78, 80): reprojection: 0.253308, disparity: 0.070131\n",
            "(78, 82): reprojection: 0.418229, disparity: 0.075278\n",
            "(79, 80): reprojection: 0.145499, disparity: 0.061115\n",
            "(79, 81): reprojection: 0.244395, disparity: 0.065988\n",
            "(80, 81): reprojection: 0.149200, disparity: 0.061000\n",
            "(80, 82): reprojection: 0.284319, disparity: 0.067904\n",
            "(80, 84): reprojection: 0.478556, disparity: 0.075677\n",
            "(80, 88): reprojection: 0.746109, disparity: 0.092152\n",
            "(81, 82): reprojection: 0.184628, disparity: 0.059594\n",
            "(81, 83): reprojection: 0.286372, disparity: 0.066586\n",
            "(82, 83): reprojection: 0.147842, disparity: 0.058481\n",
            "(82, 84): reprojection: 0.253098, disparity: 0.065868\n",
            "(82, 86): reprojection: 0.452050, disparity: 0.079018\n",
            "(83, 84): reprojection: 0.140947, disparity: 0.059964\n",
            "(83, 85): reprojection: 0.297953, disparity: 0.072383\n",
            "(84, 85): reprojection: 0.174401, disparity: 0.058660\n",
            "(84, 86): reprojection: 0.259801, disparity: 0.065600\n",
            "(84, 88): reprojection: 0.499563, disparity: 0.074836\n",
            "(85, 86): reprojection: 0.165758, disparity: 0.056214\n",
            "(85, 87): reprojection: 0.284720, disparity: 0.064369\n",
            "(86, 87): reprojection: 0.233959, disparity: 0.058347\n",
            "(86, 88): reprojection: 0.333274, disparity: 0.068996\n",
            "(86, 90): reprojection: 0.502976, disparity: 0.075992\n",
            "(87, 88): reprojection: 0.177211, disparity: 0.059830\n",
            "(87, 89): reprojection: 0.281459, disparity: 0.067365\n",
            "(88, 89): reprojection: 0.142757, disparity: 0.055667\n",
            "(88, 90): reprojection: 0.309670, disparity: 0.065610\n",
            "(89, 90): reprojection: 0.214517, disparity: 0.054664\n",
            "(89, 91): reprojection: 0.414166, disparity: 0.073641\n",
            "(90, 91): reprojection: 0.328727, disparity: 0.063181\n",
            "Mean:     reprojection: 0.328727, disparity: 0.063181\n",
            "Done Validation for epoch 7 (1820 iterations)\n",
            "Epoch = 7, pairs = [[13, 15], [56, 64], [58, 59], [22, 24]], loss = 0.458538293838501\n",
            "Epoch = 7, pairs = [[66, 67], [85, 87], [86, 90], [6, 10]], loss = 0.43230536580085754\n",
            "Epoch = 7, pairs = [[76, 84], [78, 79], [78, 80], [52, 53]], loss = 0.40781575441360474\n",
            "Epoch = 7, pairs = [[50, 52], [35, 37], [15, 17], [30, 32]], loss = 0.35729408264160156\n",
            "Epoch = 7, pairs = [[5, 7], [45, 46], [64, 72], [76, 77]], loss = 0.4006224274635315\n",
            "Epoch = 7, pairs = [[35, 36], [67, 68], [54, 55], [38, 40]], loss = 0.37395697832107544\n",
            "Epoch = 7, pairs = [[14, 15], [83, 84], [24, 26], [26, 27]], loss = 0.21974238753318787\n",
            "Epoch = 7, pairs = [[28, 32], [51, 53], [46, 47], [63, 64]], loss = 0.3568810522556305\n",
            "Epoch = 7, pairs = [[56, 58], [13, 14], [64, 68], [8, 24]], loss = 0.6245758533477783\n",
            "Epoch = 7, pairs = [[28, 30], [17, 19], [19, 21], [8, 12]], loss = 0.43479838967323303\n",
            "Epoch = 7, pairs = [[82, 84], [74, 75], [9, 11], [79, 81]], loss = 0.3368135690689087\n",
            "Epoch = 7, pairs = [[37, 38], [47, 48], [11, 13], [49, 51]], loss = 0.30908212065696716\n",
            "Epoch = 7, pairs = [[43, 45], [12, 20], [68, 72], [78, 82]], loss = 0.6721053719520569\n",
            "Epoch = 7, pairs = [[86, 88], [16, 18], [40, 48], [16, 48]], loss = 0.8858064413070679\n",
            "Epoch = 7, pairs = [[36, 40], [25, 27], [59, 61], [65, 66]], loss = 0.3716050982475281\n",
            "Epoch = 7, pairs = [[82, 86], [40, 41], [87, 88], [16, 17]], loss = 0.32554200291633606\n",
            "Epoch = 7, pairs = [[48, 64], [10, 14], [68, 76], [56, 60]], loss = 1.0644655227661133\n",
            "Epoch = 7, pairs = [[31, 32], [32, 34], [56, 57], [39, 41]], loss = 0.374648779630661\n",
            "Epoch = 7, pairs = [[60, 64], [16, 32], [40, 42], [23, 25]], loss = 0.7324162125587463\n",
            "Epoch = 7, pairs = [[57, 59], [47, 49], [69, 70], [33, 35]], loss = 0.3963737487792969\n",
            "Epoch = 7, pairs = [[49, 50], [48, 49], [83, 85], [37, 39]], loss = 0.2861378788948059\n",
            "Epoch = 7, pairs = [[82, 83], [89, 91], [6, 8], [44, 48]], loss = 0.35649481415748596\n",
            "Epoch = 7, pairs = [[10, 12], [80, 88], [31, 33], [32, 64]], loss = 0.7217862010002136\n",
            "Epoch = 7, pairs = [[80, 82], [12, 13], [21, 22], [59, 60]], loss = 0.2466707080602646\n",
            "Epoch = 7, pairs = [[60, 61], [39, 40], [5, 6], [77, 79]], loss = 0.2867805063724518\n",
            "Epoch = 7, pairs = [[25, 26], [34, 35], [24, 32], [70, 72]], loss = 0.38076868653297424\n",
            "Epoch = 7, pairs = [[60, 62], [53, 54], [24, 28], [70, 74]], loss = 0.37652337551116943\n",
            "Epoch = 7, pairs = [[22, 23], [18, 19], [29, 30], [53, 55]], loss = 0.2574284076690674\n",
            "Epoch = 7, pairs = [[42, 44], [62, 64], [42, 46], [64, 80]], loss = 0.7446483373641968\n",
            "Epoch = 7, pairs = [[24, 25], [32, 40], [73, 74], [4, 5]], loss = 0.35357430577278137\n",
            "Epoch = 7, pairs = [[0, 1], [12, 16], [21, 23], [4, 8]], loss = 0.35563042759895325\n",
            "Epoch = 7, pairs = [[40, 56], [36, 37], [74, 78], [26, 28]], loss = 0.5475118160247803\n",
            "Epoch = 7, pairs = [[60, 68], [38, 39], [23, 24], [3, 5]], loss = 0.3551819622516632\n",
            "Epoch = 7, pairs = [[4, 12], [87, 89], [52, 54], [44, 45]], loss = 0.34944015741348267\n",
            "Epoch = 7, pairs = [[43, 44], [63, 65], [44, 46], [14, 18]], loss = 0.3709736764431\n",
            "Epoch = 7, pairs = [[67, 69], [86, 87], [89, 90], [72, 76]], loss = 0.37738773226737976\n",
            "Epoch = 7, pairs = [[73, 75], [88, 90], [3, 4], [45, 47]], loss = 0.3660205602645874\n",
            "Epoch = 7, pairs = [[48, 50], [52, 56], [58, 60], [66, 68]], loss = 0.5012397766113281\n",
            "Epoch = 7, pairs = [[32, 33], [16, 24], [0, 4], [80, 81]], loss = 0.507028341293335\n",
            "Epoch = 7, pairs = [[18, 20], [88, 89], [72, 73], [72, 80]], loss = 0.3885340988636017\n",
            "Epoch = 7, pairs = [[46, 48], [34, 36], [84, 85], [70, 71]], loss = 0.3042297959327698\n",
            "Epoch = 7, pairs = [[20, 21], [50, 51], [81, 83], [10, 11]], loss = 0.2530180811882019\n",
            "Epoch = 7, pairs = [[11, 12], [2, 3], [32, 36], [29, 31]], loss = 0.2829229235649109\n",
            "Epoch = 7, pairs = [[46, 50], [2, 4], [2, 6], [48, 80]], loss = 0.8707504272460938\n",
            "Epoch = 7, pairs = [[69, 71], [0, 8], [55, 56], [15, 16]], loss = 0.44300100207328796\n",
            "Epoch = 7, pairs = [[8, 9], [38, 42], [24, 40], [76, 78]], loss = 0.6485949158668518\n",
            "Epoch = 7, pairs = [[52, 60], [79, 80], [75, 76], [80, 84]], loss = 0.5033572316169739\n",
            "Epoch = 7, pairs = [[48, 52], [66, 70], [76, 80], [8, 16]], loss = 0.6044190526008606\n",
            "Epoch = 7, pairs = [[72, 74], [36, 44], [41, 43], [64, 66]], loss = 0.5645983219146729\n",
            "Epoch = 7, pairs = [[26, 30], [56, 72], [17, 18], [50, 54]], loss = 0.8385002017021179\n",
            "Epoch = 7, pairs = [[48, 56], [90, 91], [71, 72], [71, 73]], loss = 0.5115983486175537\n",
            "Epoch = 7, pairs = [[54, 58], [84, 88], [61, 62], [42, 43]], loss = 0.4068024158477783\n",
            "Epoch = 7, pairs = [[20, 28], [12, 14], [7, 9], [14, 16]], loss = 0.42391878366470337\n",
            "Epoch = 7, pairs = [[84, 86], [0, 2], [57, 58], [74, 76]], loss = 0.37543338537216187\n",
            "Epoch = 7, pairs = [[61, 63], [1, 3], [75, 77], [9, 10]], loss = 0.3620736002922058\n",
            "Epoch = 7, pairs = [[30, 31], [18, 22], [62, 66], [7, 8]], loss = 0.3947594165802002\n",
            "Epoch = 7, pairs = [[81, 82], [62, 63], [1, 2], [27, 29]], loss = 0.2833356261253357\n",
            "Epoch = 7, pairs = [[30, 34], [6, 7], [33, 34], [72, 88]], loss = 0.4866705536842346\n",
            "Epoch = 7, pairs = [[41, 42], [27, 28], [55, 57], [68, 70]], loss = 0.34921205043792725\n",
            "Epoch = 7, pairs = [[8, 10], [54, 56], [40, 44], [16, 20]], loss = 0.5790213346481323\n",
            "Epoch = 7, pairs = [[20, 24], [65, 67], [68, 69], [36, 38]], loss = 0.4147515296936035\n",
            "Epoch = 7, pairs = [[64, 65], [51, 52], [0, 16], [28, 36]], loss = 0.610709547996521\n",
            "Epoch = 7, pairs = [[4, 6], [58, 62], [85, 86], [77, 78]], loss = 0.3480529487133026\n",
            "Epoch = 7, pairs = [[0, 32], [34, 38], [20, 22], [22, 26]], loss = 0.6586514711380005\n",
            "Epoch = 7, pairs = [[44, 52], [19, 20], [32, 48], [28, 29]], loss = 0.5528315305709839\n",
            "Epoch 7 took 84.35s.\n",
            "( 0,  1): reprojection: 0.228904, disparity: 0.065299\n",
            "( 0,  2): reprojection: 0.334235, disparity: 0.066966\n",
            "( 0,  4): reprojection: 0.362227, disparity: 0.083530\n",
            "( 0,  8): reprojection: 0.436255, disparity: 0.082373\n",
            "( 0, 16): reprojection: 0.678182, disparity: 0.089857\n",
            "( 0, 32): reprojection: 1.030679, disparity: 0.120339\n",
            "( 1,  2): reprojection: 0.191525, disparity: 0.047839\n",
            "( 1,  3): reprojection: 0.240833, disparity: 0.053464\n",
            "( 2,  3): reprojection: 0.107656, disparity: 0.047948\n",
            "( 2,  4): reprojection: 0.170424, disparity: 0.056309\n",
            "( 2,  6): reprojection: 0.351380, disparity: 0.065044\n",
            "( 3,  4): reprojection: 0.103901, disparity: 0.051904\n",
            "( 3,  5): reprojection: 0.202575, disparity: 0.057623\n",
            "( 4,  5): reprojection: 0.157897, disparity: 0.048590\n",
            "( 4,  6): reprojection: 0.228818, disparity: 0.050456\n",
            "( 4,  8): reprojection: 0.287083, disparity: 0.062358\n",
            "( 4, 12): reprojection: 0.506058, disparity: 0.080688\n",
            "( 5,  6): reprojection: 0.119213, disparity: 0.048579\n",
            "( 5,  7): reprojection: 0.148780, disparity: 0.052423\n",
            "( 6,  7): reprojection: 0.087991, disparity: 0.047413\n",
            "( 6,  8): reprojection: 0.150046, disparity: 0.052479\n",
            "( 6, 10): reprojection: 0.415177, disparity: 0.063315\n",
            "( 7,  8): reprojection: 0.088900, disparity: 0.047667\n",
            "( 7,  9): reprojection: 0.209706, disparity: 0.053174\n",
            "( 8,  9): reprojection: 0.169510, disparity: 0.048707\n",
            "( 8, 10): reprojection: 0.387065, disparity: 0.054356\n",
            "( 8, 12): reprojection: 0.573972, disparity: 0.067240\n",
            "( 8, 16): reprojection: 0.684857, disparity: 0.075586\n",
            "( 8, 24): reprojection: 0.938212, disparity: 0.095192\n",
            "( 9, 10): reprojection: 0.268900, disparity: 0.049937\n",
            "( 9, 11): reprojection: 0.406117, disparity: 0.055809\n",
            "(10, 11): reprojection: 0.182961, disparity: 0.044163\n",
            "(10, 12): reprojection: 0.227732, disparity: 0.052090\n",
            "(10, 14): reprojection: 0.368368, disparity: 0.068586\n",
            "(11, 12): reprojection: 0.142011, disparity: 0.044009\n",
            "(11, 13): reprojection: 0.228679, disparity: 0.054012\n",
            "(12, 13): reprojection: 0.126655, disparity: 0.045613\n",
            "(12, 14): reprojection: 0.262877, disparity: 0.055670\n",
            "(12, 16): reprojection: 0.341694, disparity: 0.077906\n",
            "(12, 20): reprojection: 0.621755, disparity: 0.083827\n",
            "(13, 14): reprojection: 0.193039, disparity: 0.048709\n",
            "(13, 15): reprojection: 0.192587, disparity: 0.058001\n",
            "(14, 15): reprojection: 0.173761, disparity: 0.049581\n",
            "(14, 16): reprojection: 0.411322, disparity: 0.058206\n",
            "(14, 18): reprojection: 0.311609, disparity: 0.071876\n",
            "(15, 16): reprojection: 0.288184, disparity: 0.050966\n",
            "(15, 17): reprojection: 0.363250, disparity: 0.061829\n",
            "(16, 17): reprojection: 0.138272, disparity: 0.049823\n",
            "(16, 18): reprojection: 0.339580, disparity: 0.061547\n",
            "(16, 20): reprojection: 0.632167, disparity: 0.074540\n",
            "(16, 24): reprojection: 0.871149, disparity: 0.091709\n",
            "(16, 32): reprojection: 1.014641, disparity: 0.116175\n",
            "(16, 48): reprojection: 1.309171, disparity: 0.185427\n",
            "(17, 18): reprojection: 0.325521, disparity: 0.053907\n",
            "(17, 19): reprojection: 0.517106, disparity: 0.063167\n",
            "(18, 19): reprojection: 0.233199, disparity: 0.048884\n",
            "(18, 20): reprojection: 0.350866, disparity: 0.059016\n",
            "(18, 22): reprojection: 0.504405, disparity: 0.068746\n",
            "(19, 20): reprojection: 0.129744, disparity: 0.049191\n",
            "(19, 21): reprojection: 0.291645, disparity: 0.060431\n",
            "(20, 21): reprojection: 0.186729, disparity: 0.050968\n",
            "(20, 22): reprojection: 0.277097, disparity: 0.061426\n",
            "(20, 24): reprojection: 0.338303, disparity: 0.071276\n",
            "(20, 28): reprojection: 0.474035, disparity: 0.099822\n",
            "(21, 22): reprojection: 0.139413, disparity: 0.052753\n",
            "(21, 23): reprojection: 0.194273, disparity: 0.061184\n",
            "(22, 23): reprojection: 0.121649, disparity: 0.051756\n",
            "(22, 24): reprojection: 0.174321, disparity: 0.060762\n",
            "(22, 26): reprojection: 0.384079, disparity: 0.076395\n",
            "(23, 24): reprojection: 0.108492, disparity: 0.051153\n",
            "(23, 25): reprojection: 0.222936, disparity: 0.062909\n",
            "(24, 25): reprojection: 0.138913, disparity: 0.054084\n",
            "(24, 26): reprojection: 0.211221, disparity: 0.062216\n",
            "(24, 28): reprojection: 0.343192, disparity: 0.074308\n",
            "(24, 32): reprojection: 0.539574, disparity: 0.090972\n",
            "(24, 40): reprojection: 0.913690, disparity: 0.128218\n",
            "(25, 26): reprojection: 0.148229, disparity: 0.058860\n",
            "(25, 27): reprojection: 0.238042, disparity: 0.064975\n",
            "(26, 27): reprojection: 0.144114, disparity: 0.057806\n",
            "(26, 28): reprojection: 0.211076, disparity: 0.062994\n",
            "(26, 30): reprojection: 0.356218, disparity: 0.073695\n",
            "(27, 28): reprojection: 0.132194, disparity: 0.054578\n",
            "(27, 29): reprojection: 0.258527, disparity: 0.061857\n",
            "(28, 29): reprojection: 0.176810, disparity: 0.056010\n",
            "(28, 30): reprojection: 0.263187, disparity: 0.066412\n",
            "(28, 32): reprojection: 0.349290, disparity: 0.075185\n",
            "(28, 36): reprojection: 0.514405, disparity: 0.105680\n",
            "(29, 30): reprojection: 0.136096, disparity: 0.054237\n",
            "(29, 31): reprojection: 0.238208, disparity: 0.060474\n",
            "(30, 31): reprojection: 0.178296, disparity: 0.054611\n",
            "(30, 32): reprojection: 0.226764, disparity: 0.064068\n",
            "(30, 34): reprojection: 0.375022, disparity: 0.078259\n",
            "(31, 32): reprojection: 0.128997, disparity: 0.053743\n",
            "(31, 33): reprojection: 0.271668, disparity: 0.062807\n",
            "(32, 33): reprojection: 0.220391, disparity: 0.058156\n",
            "(32, 34): reprojection: 0.216416, disparity: 0.062172\n",
            "(32, 36): reprojection: 0.306171, disparity: 0.078117\n",
            "(32, 40): reprojection: 0.444901, disparity: 0.121040\n",
            "(32, 48): reprojection: 0.937602, disparity: 0.152620\n",
            "(32, 64): reprojection: 1.309160, disparity: 0.193927\n",
            "(33, 34): reprojection: 0.161729, disparity: 0.054117\n",
            "(33, 35): reprojection: 0.247347, disparity: 0.062268\n",
            "(34, 35): reprojection: 0.171064, disparity: 0.053052\n",
            "(34, 36): reprojection: 0.252252, disparity: 0.065344\n",
            "(34, 38): reprojection: 0.336527, disparity: 0.077637\n",
            "(35, 36): reprojection: 0.223471, disparity: 0.059344\n",
            "(35, 37): reprojection: 0.270619, disparity: 0.078164\n",
            "(36, 37): reprojection: 0.252635, disparity: 0.067272\n",
            "(36, 38): reprojection: 0.388853, disparity: 0.070764\n",
            "(36, 40): reprojection: 0.353902, disparity: 0.084040\n",
            "(36, 44): reprojection: 0.699050, disparity: 0.114506\n",
            "(37, 38): reprojection: 0.227005, disparity: 0.062527\n",
            "(37, 39): reprojection: 0.241603, disparity: 0.070149\n",
            "(38, 39): reprojection: 0.195038, disparity: 0.061745\n",
            "(38, 40): reprojection: 0.384074, disparity: 0.074360\n",
            "(38, 42): reprojection: 0.752652, disparity: 0.100001\n",
            "(39, 40): reprojection: 0.291983, disparity: 0.062349\n",
            "(39, 41): reprojection: 0.524683, disparity: 0.079366\n",
            "(40, 41): reprojection: 0.322928, disparity: 0.071454\n",
            "(40, 42): reprojection: 0.487714, disparity: 0.084648\n",
            "(40, 44): reprojection: 0.647560, disparity: 0.097594\n",
            "(40, 48): reprojection: 0.876706, disparity: 0.129981\n",
            "(40, 56): reprojection: 0.818169, disparity: 0.194063\n",
            "(41, 42): reprojection: 0.215688, disparity: 0.065885\n",
            "(41, 43): reprojection: 0.336893, disparity: 0.078532\n",
            "(42, 43): reprojection: 0.192831, disparity: 0.064031\n",
            "(42, 44): reprojection: 0.351255, disparity: 0.076429\n",
            "(42, 46): reprojection: 0.577922, disparity: 0.101105\n",
            "(43, 44): reprojection: 0.196320, disparity: 0.065503\n",
            "(43, 45): reprojection: 0.295177, disparity: 0.080462\n",
            "(44, 45): reprojection: 0.149507, disparity: 0.072468\n",
            "(44, 46): reprojection: 0.298427, disparity: 0.093959\n",
            "(44, 48): reprojection: 0.427839, disparity: 0.114713\n",
            "(44, 52): reprojection: 0.669267, disparity: 0.116567\n",
            "(45, 46): reprojection: 0.205298, disparity: 0.070396\n",
            "(45, 47): reprojection: 0.330064, disparity: 0.087505\n",
            "(46, 47): reprojection: 0.196182, disparity: 0.069342\n",
            "(46, 48): reprojection: 0.317989, disparity: 0.093539\n",
            "(46, 50): reprojection: 0.436986, disparity: 0.102001\n",
            "(47, 48): reprojection: 0.205642, disparity: 0.069938\n",
            "(47, 49): reprojection: 0.261707, disparity: 0.087607\n",
            "(48, 49): reprojection: 0.166507, disparity: 0.076379\n",
            "(48, 50): reprojection: 0.224198, disparity: 0.090896\n",
            "(48, 52): reprojection: 0.388876, disparity: 0.106468\n",
            "(48, 56): reprojection: 0.669360, disparity: 0.129527\n",
            "(48, 64): reprojection: 1.081520, disparity: 0.210601\n",
            "(48, 80): reprojection: 1.083622, disparity: 0.123728\n",
            "(49, 50): reprojection: 0.134081, disparity: 0.071019\n",
            "(49, 51): reprojection: 0.228042, disparity: 0.079660\n",
            "(50, 51): reprojection: 0.146648, disparity: 0.065354\n",
            "(50, 52): reprojection: 0.271517, disparity: 0.074953\n",
            "(50, 54): reprojection: 0.479122, disparity: 0.109699\n",
            "(51, 52): reprojection: 0.188409, disparity: 0.065151\n",
            "(51, 53): reprojection: 0.336099, disparity: 0.077643\n",
            "(52, 53): reprojection: 0.188601, disparity: 0.075700\n",
            "(52, 54): reprojection: 0.281737, disparity: 0.093145\n",
            "(52, 56): reprojection: 0.555491, disparity: 0.099580\n",
            "(52, 60): reprojection: 0.690277, disparity: 0.168888\n",
            "(53, 54): reprojection: 0.140820, disparity: 0.071900\n",
            "(53, 55): reprojection: 0.339827, disparity: 0.083913\n",
            "(54, 55): reprojection: 0.319551, disparity: 0.062620\n",
            "(54, 56): reprojection: 0.562371, disparity: 0.072747\n",
            "(54, 58): reprojection: 0.377112, disparity: 0.087214\n",
            "(55, 56): reprojection: 0.340815, disparity: 0.066230\n",
            "(55, 57): reprojection: 0.352785, disparity: 0.076519\n",
            "(56, 57): reprojection: 0.307574, disparity: 0.069563\n",
            "(56, 58): reprojection: 0.570041, disparity: 0.089872\n",
            "(56, 60): reprojection: 0.711226, disparity: 0.140240\n",
            "(56, 64): reprojection: 0.899348, disparity: 0.135554\n",
            "(56, 72): reprojection: 0.989246, disparity: 0.142297\n",
            "(57, 58): reprojection: 0.366952, disparity: 0.076814\n",
            "(57, 59): reprojection: 0.510225, disparity: 0.107404\n",
            "(58, 59): reprojection: 0.267184, disparity: 0.085706\n",
            "(58, 60): reprojection: 0.382517, disparity: 0.091044\n",
            "(58, 62): reprojection: 0.518461, disparity: 0.129573\n",
            "(59, 60): reprojection: 0.192793, disparity: 0.077134\n",
            "(59, 61): reprojection: 0.317889, disparity: 0.084642\n",
            "(60, 61): reprojection: 0.225549, disparity: 0.077684\n",
            "(60, 62): reprojection: 0.318317, disparity: 0.086728\n",
            "(60, 64): reprojection: 0.574359, disparity: 0.094895\n",
            "(60, 68): reprojection: 0.753107, disparity: 0.110873\n",
            "(61, 62): reprojection: 0.236374, disparity: 0.077235\n",
            "(61, 63): reprojection: 0.333193, disparity: 0.079545\n",
            "(62, 63): reprojection: 0.249143, disparity: 0.070482\n",
            "(62, 64): reprojection: 0.365836, disparity: 0.076834\n",
            "(62, 66): reprojection: 0.498032, disparity: 0.097153\n",
            "(63, 64): reprojection: 0.220534, disparity: 0.075494\n",
            "(63, 65): reprojection: 0.333418, disparity: 0.075756\n",
            "(64, 65): reprojection: 0.212626, disparity: 0.074410\n",
            "(64, 66): reprojection: 0.364178, disparity: 0.074706\n",
            "(64, 68): reprojection: 0.507371, disparity: 0.087099\n",
            "(64, 72): reprojection: 0.848269, disparity: 0.103384\n",
            "(64, 80): reprojection: 0.998169, disparity: 0.126605\n",
            "(65, 66): reprojection: 0.298028, disparity: 0.064665\n",
            "(65, 67): reprojection: 0.449772, disparity: 0.072709\n",
            "(66, 67): reprojection: 0.260540, disparity: 0.068770\n",
            "(66, 68): reprojection: 0.399587, disparity: 0.070170\n",
            "(66, 70): reprojection: 0.520525, disparity: 0.080708\n",
            "(67, 68): reprojection: 0.360244, disparity: 0.065623\n",
            "(67, 69): reprojection: 0.405652, disparity: 0.071380\n",
            "(68, 69): reprojection: 0.216280, disparity: 0.064568\n",
            "(68, 70): reprojection: 0.488647, disparity: 0.071907\n",
            "(68, 72): reprojection: 0.536621, disparity: 0.086675\n",
            "(68, 76): reprojection: 0.746304, disparity: 0.100048\n",
            "(69, 70): reprojection: 0.329822, disparity: 0.064637\n",
            "(69, 71): reprojection: 0.501226, disparity: 0.073375\n",
            "(70, 71): reprojection: 0.217458, disparity: 0.066061\n",
            "(70, 72): reprojection: 0.281104, disparity: 0.069029\n",
            "(70, 74): reprojection: 0.400016, disparity: 0.082249\n",
            "(71, 72): reprojection: 0.240569, disparity: 0.064947\n",
            "(71, 73): reprojection: 0.378490, disparity: 0.072946\n",
            "(72, 73): reprojection: 0.243253, disparity: 0.066710\n",
            "(72, 74): reprojection: 0.316473, disparity: 0.074353\n",
            "(72, 76): reprojection: 0.500338, disparity: 0.087291\n",
            "(72, 80): reprojection: 0.531418, disparity: 0.109568\n",
            "(72, 88): reprojection: 0.959501, disparity: 0.106495\n",
            "(73, 74): reprojection: 0.322680, disparity: 0.069412\n",
            "(73, 75): reprojection: 0.434866, disparity: 0.074165\n",
            "(74, 75): reprojection: 0.232353, disparity: 0.065744\n",
            "(74, 76): reprojection: 0.244050, disparity: 0.068806\n",
            "(74, 78): reprojection: 0.358078, disparity: 0.080823\n",
            "(75, 76): reprojection: 0.230089, disparity: 0.063459\n",
            "(75, 77): reprojection: 0.362261, disparity: 0.073169\n",
            "(76, 77): reprojection: 0.221941, disparity: 0.061104\n",
            "(76, 78): reprojection: 0.323912, disparity: 0.065068\n",
            "(76, 80): reprojection: 0.390466, disparity: 0.079974\n",
            "(76, 84): reprojection: 0.554957, disparity: 0.093927\n",
            "(77, 78): reprojection: 0.223274, disparity: 0.062009\n",
            "(77, 79): reprojection: 0.276614, disparity: 0.067574\n",
            "(78, 79): reprojection: 0.201510, disparity: 0.065546\n",
            "(78, 80): reprojection: 0.264446, disparity: 0.070395\n",
            "(78, 82): reprojection: 0.431576, disparity: 0.077500\n",
            "(79, 80): reprojection: 0.149267, disparity: 0.060878\n",
            "(79, 81): reprojection: 0.241472, disparity: 0.066863\n",
            "(80, 81): reprojection: 0.144123, disparity: 0.061121\n",
            "(80, 82): reprojection: 0.259255, disparity: 0.066135\n",
            "(80, 84): reprojection: 0.460076, disparity: 0.076501\n",
            "(80, 88): reprojection: 0.765690, disparity: 0.096689\n",
            "(81, 82): reprojection: 0.188011, disparity: 0.060145\n",
            "(81, 83): reprojection: 0.316207, disparity: 0.067169\n",
            "(82, 83): reprojection: 0.163917, disparity: 0.058218\n",
            "(82, 84): reprojection: 0.249797, disparity: 0.067480\n",
            "(82, 86): reprojection: 0.512305, disparity: 0.078385\n",
            "(83, 84): reprojection: 0.142972, disparity: 0.062378\n",
            "(83, 85): reprojection: 0.314777, disparity: 0.074618\n",
            "(84, 85): reprojection: 0.180306, disparity: 0.057358\n",
            "(84, 86): reprojection: 0.286684, disparity: 0.067073\n",
            "(84, 88): reprojection: 0.524952, disparity: 0.076991\n",
            "(85, 86): reprojection: 0.174414, disparity: 0.056243\n",
            "(85, 87): reprojection: 0.301165, disparity: 0.065695\n",
            "(86, 87): reprojection: 0.246608, disparity: 0.058307\n",
            "(86, 88): reprojection: 0.345702, disparity: 0.070736\n",
            "(86, 90): reprojection: 0.479490, disparity: 0.073694\n",
            "(87, 88): reprojection: 0.180997, disparity: 0.060219\n",
            "(87, 89): reprojection: 0.270753, disparity: 0.067342\n",
            "(88, 89): reprojection: 0.132164, disparity: 0.056278\n",
            "(88, 90): reprojection: 0.302288, disparity: 0.062113\n",
            "(89, 90): reprojection: 0.213932, disparity: 0.054427\n",
            "(89, 91): reprojection: 0.435117, disparity: 0.081137\n",
            "(90, 91): reprojection: 0.346084, disparity: 0.069914\n",
            "Mean:     reprojection: 0.346084, disparity: 0.069914\n",
            "Done Validation for epoch 8 (2080 iterations)\n",
            "Epoch = 8, pairs = [[21, 22], [35, 37], [72, 76], [63, 64]], loss = 0.33764058351516724\n",
            "Epoch = 8, pairs = [[33, 35], [59, 61], [62, 64], [86, 90]], loss = 0.44385629892349243\n",
            "Epoch = 8, pairs = [[84, 85], [34, 38], [44, 52], [40, 42]], loss = 0.5212537050247192\n",
            "Epoch = 8, pairs = [[41, 42], [24, 32], [5, 7], [68, 76]], loss = 0.518801212310791\n",
            "Epoch = 8, pairs = [[26, 28], [65, 67], [24, 26], [78, 80]], loss = 0.3745374083518982\n",
            "Epoch = 8, pairs = [[5, 6], [43, 44], [7, 8], [58, 62]], loss = 0.2967684864997864\n",
            "Epoch = 8, pairs = [[54, 56], [89, 90], [18, 22], [85, 86]], loss = 0.4359897971153259\n",
            "Epoch = 8, pairs = [[82, 84], [88, 89], [15, 17], [24, 40]], loss = 0.5113257169723511\n",
            "Epoch = 8, pairs = [[48, 80], [31, 32], [47, 48], [14, 18]], loss = 0.7289378643035889\n",
            "Epoch = 8, pairs = [[8, 12], [24, 25], [27, 28], [45, 47]], loss = 0.35241109132766724\n",
            "Epoch = 8, pairs = [[76, 77], [86, 87], [36, 37], [90, 91]], loss = 0.3418843448162079\n",
            "Epoch = 8, pairs = [[10, 14], [88, 90], [18, 20], [58, 60]], loss = 0.5130820870399475\n",
            "Epoch = 8, pairs = [[62, 63], [64, 68], [80, 81], [19, 20]], loss = 0.4890344738960266\n",
            "Epoch = 8, pairs = [[38, 39], [22, 23], [56, 72], [66, 68]], loss = 1.191874384880066\n",
            "Epoch = 8, pairs = [[16, 18], [30, 32], [40, 48], [6, 10]], loss = 0.672692060470581\n",
            "Epoch = 8, pairs = [[56, 60], [80, 88], [79, 81], [14, 16]], loss = 0.7290940284729004\n",
            "Epoch = 8, pairs = [[60, 61], [66, 67], [72, 73], [52, 54]], loss = 0.3296258747577667\n",
            "Epoch = 8, pairs = [[38, 42], [23, 24], [74, 78], [32, 64]], loss = 0.9930334091186523\n",
            "Epoch = 8, pairs = [[57, 59], [55, 57], [64, 65], [24, 28]], loss = 0.5881044864654541\n",
            "Epoch = 8, pairs = [[72, 80], [44, 45], [77, 79], [56, 58]], loss = 0.7794057130813599\n",
            "Epoch = 8, pairs = [[38, 40], [25, 27], [4, 6], [54, 58]], loss = 0.5599358677864075\n",
            "Epoch = 8, pairs = [[28, 29], [72, 74], [82, 86], [87, 88]], loss = 0.42232269048690796\n",
            "Epoch = 8, pairs = [[30, 31], [8, 16], [42, 46], [52, 60]], loss = 0.9154843091964722\n",
            "Epoch = 8, pairs = [[68, 72], [32, 34], [81, 83], [15, 16]], loss = 0.49283429980278015\n",
            "Epoch = 8, pairs = [[44, 48], [50, 51], [48, 49], [47, 49]], loss = 0.37168821692466736\n",
            "Epoch = 8, pairs = [[74, 75], [87, 89], [54, 55], [67, 68]], loss = 0.355424702167511\n",
            "Epoch = 8, pairs = [[11, 13], [48, 64], [51, 53], [41, 43]], loss = 0.6573851108551025\n",
            "Epoch = 8, pairs = [[49, 50], [85, 87], [64, 66], [44, 46]], loss = 0.3676775097846985\n",
            "Epoch = 8, pairs = [[62, 66], [51, 52], [32, 33], [20, 21]], loss = 0.3991023898124695\n",
            "Epoch = 8, pairs = [[77, 78], [0, 2], [76, 80], [17, 18]], loss = 0.4369097948074341\n",
            "Epoch = 8, pairs = [[36, 44], [60, 68], [22, 26], [82, 83]], loss = 0.7530272603034973\n",
            "Epoch = 8, pairs = [[32, 48], [48, 50], [28, 30], [86, 88]], loss = 0.5456310510635376\n",
            "Epoch = 8, pairs = [[73, 75], [67, 69], [49, 51], [78, 79]], loss = 0.3716650605201721\n",
            "Epoch = 8, pairs = [[12, 16], [69, 70], [64, 72], [29, 31]], loss = 0.5389043092727661\n",
            "Epoch = 8, pairs = [[2, 6], [22, 24], [8, 9], [20, 24]], loss = 0.3762245178222656\n",
            "Epoch = 8, pairs = [[12, 13], [2, 4], [13, 15], [42, 43]], loss = 0.25895145535469055\n",
            "Epoch = 8, pairs = [[40, 56], [61, 62], [32, 36], [27, 29]], loss = 0.669456422328949\n",
            "Epoch = 8, pairs = [[14, 15], [4, 8], [12, 14], [50, 54]], loss = 0.3580634295940399\n",
            "Epoch = 8, pairs = [[79, 80], [37, 39], [64, 80], [30, 34]], loss = 0.5617372989654541\n",
            "Epoch = 8, pairs = [[20, 28], [48, 52], [43, 45], [81, 82]], loss = 0.43748968839645386\n",
            "Epoch = 8, pairs = [[70, 74], [2, 3], [63, 65], [6, 8]], loss = 0.32731568813323975\n",
            "Epoch = 8, pairs = [[18, 19], [46, 47], [72, 88], [35, 36]], loss = 0.49321064352989197\n",
            "Epoch = 8, pairs = [[84, 88], [8, 10], [37, 38], [71, 72]], loss = 0.4102175235748291\n",
            "Epoch = 8, pairs = [[0, 16], [9, 11], [26, 30], [42, 44]], loss = 0.5054295659065247\n",
            "Epoch = 8, pairs = [[73, 74], [12, 20], [28, 36], [70, 71]], loss = 0.5148256421089172\n",
            "Epoch = 8, pairs = [[13, 14], [23, 25], [50, 52], [0, 8]], loss = 0.3357947766780853\n",
            "Epoch = 8, pairs = [[10, 11], [40, 44], [16, 24], [36, 38]], loss = 0.592014729976654\n",
            "Epoch = 8, pairs = [[68, 70], [70, 72], [32, 40], [7, 9]], loss = 0.4307762086391449\n",
            "Epoch = 8, pairs = [[4, 12], [4, 5], [34, 35], [74, 76]], loss = 0.33244165778160095\n",
            "Epoch = 8, pairs = [[16, 17], [57, 58], [10, 12], [75, 76]], loss = 0.29447153210639954\n",
            "Epoch = 8, pairs = [[71, 73], [39, 41], [16, 20], [33, 34]], loss = 0.45815277099609375\n",
            "Epoch = 8, pairs = [[83, 84], [26, 27], [3, 5], [46, 50]], loss = 0.2885091006755829\n",
            "Epoch = 8, pairs = [[25, 26], [60, 64], [6, 7], [80, 84]], loss = 0.3697074055671692\n",
            "Epoch = 8, pairs = [[58, 59], [29, 30], [21, 23], [16, 48]], loss = 0.5883346199989319\n",
            "Epoch = 8, pairs = [[76, 78], [3, 4], [53, 55], [76, 84]], loss = 0.41875678300857544\n",
            "Epoch = 8, pairs = [[48, 56], [55, 56], [0, 4], [1, 2]], loss = 0.4814762473106384\n",
            "Epoch = 8, pairs = [[52, 53], [53, 54], [1, 3], [78, 82]], loss = 0.3029648959636688\n",
            "Epoch = 8, pairs = [[59, 60], [56, 57], [84, 86], [46, 48]], loss = 0.3353641629219055\n",
            "Epoch = 8, pairs = [[83, 85], [0, 1], [39, 40], [31, 33]], loss = 0.343605101108551\n",
            "Epoch = 8, pairs = [[68, 69], [34, 36], [28, 32], [9, 10]], loss = 0.3545745313167572\n",
            "Epoch = 8, pairs = [[61, 63], [19, 21], [69, 71], [16, 32]], loss = 0.6768364310264587\n",
            "Epoch = 8, pairs = [[80, 82], [75, 77], [65, 66], [11, 12]], loss = 0.3322666883468628\n",
            "Epoch = 8, pairs = [[66, 70], [56, 64], [60, 62], [89, 91]], loss = 0.625778079032898\n",
            "Epoch = 8, pairs = [[17, 19], [20, 22], [45, 46], [52, 56]], loss = 0.465894877910614\n",
            "Epoch = 8, pairs = [[36, 40], [0, 32], [8, 24], [40, 41]], loss = 1.2013189792633057\n",
            "Epoch 8 took 85.24s.\n",
            "( 0,  1): reprojection: 0.234285, disparity: 0.060073\n",
            "( 0,  2): reprojection: 0.350719, disparity: 0.061296\n",
            "( 0,  4): reprojection: 0.396595, disparity: 0.072399\n",
            "( 0,  8): reprojection: 0.510213, disparity: 0.076748\n",
            "( 0, 16): reprojection: 0.815594, disparity: 0.089816\n",
            "( 0, 32): reprojection: 1.189275, disparity: 0.101865\n",
            "( 1,  2): reprojection: 0.196948, disparity: 0.045375\n",
            "( 1,  3): reprojection: 0.252910, disparity: 0.050667\n",
            "( 2,  3): reprojection: 0.110418, disparity: 0.046232\n",
            "( 2,  4): reprojection: 0.181237, disparity: 0.051713\n",
            "( 2,  6): reprojection: 0.347691, disparity: 0.062628\n",
            "( 3,  4): reprojection: 0.110379, disparity: 0.047478\n",
            "( 3,  5): reprojection: 0.202449, disparity: 0.052732\n",
            "( 4,  5): reprojection: 0.154015, disparity: 0.047476\n",
            "( 4,  6): reprojection: 0.218920, disparity: 0.048015\n",
            "( 4,  8): reprojection: 0.288508, disparity: 0.061660\n",
            "( 4, 12): reprojection: 0.369038, disparity: 0.075430\n",
            "( 5,  6): reprojection: 0.117243, disparity: 0.046526\n",
            "( 5,  7): reprojection: 0.161184, disparity: 0.048082\n",
            "( 6,  7): reprojection: 0.099171, disparity: 0.044213\n",
            "( 6,  8): reprojection: 0.163375, disparity: 0.050895\n",
            "( 6, 10): reprojection: 0.378986, disparity: 0.060788\n",
            "( 7,  8): reprojection: 0.088731, disparity: 0.045272\n",
            "( 7,  9): reprojection: 0.186556, disparity: 0.050680\n",
            "( 8,  9): reprojection: 0.156191, disparity: 0.045766\n",
            "( 8, 10): reprojection: 0.350440, disparity: 0.051300\n",
            "( 8, 12): reprojection: 0.476318, disparity: 0.061944\n",
            "( 8, 16): reprojection: 0.612150, disparity: 0.077382\n",
            "( 8, 24): reprojection: 0.758497, disparity: 0.086296\n",
            "( 9, 10): reprojection: 0.242897, disparity: 0.046796\n",
            "( 9, 11): reprojection: 0.357619, disparity: 0.052454\n",
            "(10, 11): reprojection: 0.153427, disparity: 0.042454\n",
            "(10, 12): reprojection: 0.196812, disparity: 0.049405\n",
            "(10, 14): reprojection: 0.325309, disparity: 0.066439\n",
            "(11, 12): reprojection: 0.137917, disparity: 0.042781\n",
            "(11, 13): reprojection: 0.222951, disparity: 0.051913\n",
            "(12, 13): reprojection: 0.135020, disparity: 0.043086\n",
            "(12, 14): reprojection: 0.257910, disparity: 0.051420\n",
            "(12, 16): reprojection: 0.446294, disparity: 0.074769\n",
            "(12, 20): reprojection: 0.687431, disparity: 0.075240\n",
            "(13, 14): reprojection: 0.179384, disparity: 0.045743\n",
            "(13, 15): reprojection: 0.243922, disparity: 0.054016\n",
            "(14, 15): reprojection: 0.190743, disparity: 0.046653\n",
            "(14, 16): reprojection: 0.428049, disparity: 0.057362\n",
            "(14, 18): reprojection: 0.357039, disparity: 0.064416\n",
            "(15, 16): reprojection: 0.292657, disparity: 0.047067\n",
            "(15, 17): reprojection: 0.371661, disparity: 0.057368\n",
            "(16, 17): reprojection: 0.136103, disparity: 0.048241\n",
            "(16, 18): reprojection: 0.312664, disparity: 0.056020\n",
            "(16, 20): reprojection: 0.546692, disparity: 0.064206\n",
            "(16, 24): reprojection: 0.769992, disparity: 0.081235\n",
            "(16, 32): reprojection: 1.280724, disparity: 0.091766\n",
            "(16, 48): reprojection: 1.187502, disparity: 0.197212\n",
            "(17, 18): reprojection: 0.305821, disparity: 0.049785\n",
            "(17, 19): reprojection: 0.472966, disparity: 0.059755\n",
            "(18, 19): reprojection: 0.211016, disparity: 0.047358\n",
            "(18, 20): reprojection: 0.297822, disparity: 0.054122\n",
            "(18, 22): reprojection: 0.473633, disparity: 0.061287\n",
            "(19, 20): reprojection: 0.118787, disparity: 0.046183\n",
            "(19, 21): reprojection: 0.248071, disparity: 0.054556\n",
            "(20, 21): reprojection: 0.182747, disparity: 0.048118\n",
            "(20, 22): reprojection: 0.272047, disparity: 0.057199\n",
            "(20, 24): reprojection: 0.366794, disparity: 0.065345\n",
            "(20, 28): reprojection: 0.659560, disparity: 0.080502\n",
            "(21, 22): reprojection: 0.150467, disparity: 0.050244\n",
            "(21, 23): reprojection: 0.237788, disparity: 0.058103\n",
            "(22, 23): reprojection: 0.119967, disparity: 0.049073\n",
            "(22, 24): reprojection: 0.179864, disparity: 0.056836\n",
            "(22, 26): reprojection: 0.338189, disparity: 0.069427\n",
            "(23, 24): reprojection: 0.109123, disparity: 0.048960\n",
            "(23, 25): reprojection: 0.176467, disparity: 0.058941\n",
            "(24, 25): reprojection: 0.128314, disparity: 0.052082\n",
            "(24, 26): reprojection: 0.178046, disparity: 0.061021\n",
            "(24, 28): reprojection: 0.353488, disparity: 0.072780\n",
            "(24, 32): reprojection: 0.722069, disparity: 0.092271\n",
            "(24, 40): reprojection: 1.043585, disparity: 0.127825\n",
            "(25, 26): reprojection: 0.130163, disparity: 0.054778\n",
            "(25, 27): reprojection: 0.223540, disparity: 0.062209\n",
            "(26, 27): reprojection: 0.142257, disparity: 0.053419\n",
            "(26, 28): reprojection: 0.185821, disparity: 0.059572\n",
            "(26, 30): reprojection: 0.343166, disparity: 0.069232\n",
            "(27, 28): reprojection: 0.138557, disparity: 0.051904\n",
            "(27, 29): reprojection: 0.247808, disparity: 0.058120\n",
            "(28, 29): reprojection: 0.175902, disparity: 0.053657\n",
            "(28, 30): reprojection: 0.239769, disparity: 0.060581\n",
            "(28, 32): reprojection: 0.457385, disparity: 0.071108\n",
            "(28, 36): reprojection: 0.702048, disparity: 0.092487\n",
            "(29, 30): reprojection: 0.132261, disparity: 0.051586\n",
            "(29, 31): reprojection: 0.290887, disparity: 0.057136\n",
            "(30, 31): reprojection: 0.203341, disparity: 0.052342\n",
            "(30, 32): reprojection: 0.279864, disparity: 0.065134\n",
            "(30, 34): reprojection: 0.497360, disparity: 0.077335\n",
            "(31, 32): reprojection: 0.129939, disparity: 0.055132\n",
            "(31, 33): reprojection: 0.317241, disparity: 0.058678\n",
            "(32, 33): reprojection: 0.247739, disparity: 0.057228\n",
            "(32, 34): reprojection: 0.258887, disparity: 0.068121\n",
            "(32, 36): reprojection: 0.370848, disparity: 0.083497\n",
            "(32, 40): reprojection: 0.490410, disparity: 0.143572\n",
            "(32, 48): reprojection: 0.909538, disparity: 0.178805\n",
            "(32, 64): reprojection: 1.112587, disparity: 0.126112\n",
            "(33, 34): reprojection: 0.144731, disparity: 0.058386\n",
            "(33, 35): reprojection: 0.262441, disparity: 0.065929\n",
            "(34, 35): reprojection: 0.186816, disparity: 0.052149\n",
            "(34, 36): reprojection: 0.281195, disparity: 0.062624\n",
            "(34, 38): reprojection: 0.396910, disparity: 0.079771\n",
            "(35, 36): reprojection: 0.226541, disparity: 0.057533\n",
            "(35, 37): reprojection: 0.273383, disparity: 0.075643\n",
            "(36, 37): reprojection: 0.271631, disparity: 0.064677\n",
            "(36, 38): reprojection: 0.397116, disparity: 0.068425\n",
            "(36, 40): reprojection: 0.333656, disparity: 0.083075\n",
            "(36, 44): reprojection: 0.651391, disparity: 0.124657\n",
            "(37, 38): reprojection: 0.223532, disparity: 0.056339\n",
            "(37, 39): reprojection: 0.249365, disparity: 0.064097\n",
            "(38, 39): reprojection: 0.183855, disparity: 0.057894\n",
            "(38, 40): reprojection: 0.354685, disparity: 0.070304\n",
            "(38, 42): reprojection: 0.633641, disparity: 0.098834\n",
            "(39, 40): reprojection: 0.274118, disparity: 0.059982\n",
            "(39, 41): reprojection: 0.475954, disparity: 0.076207\n",
            "(40, 41): reprojection: 0.279309, disparity: 0.064209\n",
            "(40, 42): reprojection: 0.407605, disparity: 0.075228\n",
            "(40, 44): reprojection: 0.514009, disparity: 0.089881\n",
            "(40, 48): reprojection: 0.725940, disparity: 0.110729\n",
            "(40, 56): reprojection: 0.742782, disparity: 0.114129\n",
            "(41, 42): reprojection: 0.195492, disparity: 0.060030\n",
            "(41, 43): reprojection: 0.286581, disparity: 0.076732\n",
            "(42, 43): reprojection: 0.166108, disparity: 0.061634\n",
            "(42, 44): reprojection: 0.304174, disparity: 0.071603\n",
            "(42, 46): reprojection: 0.504159, disparity: 0.080228\n",
            "(43, 44): reprojection: 0.199462, disparity: 0.060168\n",
            "(43, 45): reprojection: 0.293328, disparity: 0.073835\n",
            "(44, 45): reprojection: 0.150499, disparity: 0.064072\n",
            "(44, 46): reprojection: 0.283437, disparity: 0.075067\n",
            "(44, 48): reprojection: 0.428146, disparity: 0.084582\n",
            "(44, 52): reprojection: 0.622266, disparity: 0.103471\n",
            "(45, 46): reprojection: 0.199898, disparity: 0.062699\n",
            "(45, 47): reprojection: 0.316034, disparity: 0.081862\n",
            "(46, 47): reprojection: 0.182464, disparity: 0.065033\n",
            "(46, 48): reprojection: 0.291240, disparity: 0.073891\n",
            "(46, 50): reprojection: 0.400936, disparity: 0.081450\n",
            "(47, 48): reprojection: 0.206422, disparity: 0.060792\n",
            "(47, 49): reprojection: 0.265475, disparity: 0.072402\n",
            "(48, 49): reprojection: 0.165135, disparity: 0.071211\n",
            "(48, 50): reprojection: 0.242870, disparity: 0.076241\n",
            "(48, 52): reprojection: 0.368484, disparity: 0.090538\n",
            "(48, 56): reprojection: 0.694295, disparity: 0.106982\n",
            "(48, 64): reprojection: 0.880081, disparity: 0.176221\n",
            "(48, 80): reprojection: 1.174040, disparity: 0.206409\n",
            "(49, 50): reprojection: 0.145171, disparity: 0.066130\n",
            "(49, 51): reprojection: 0.241431, disparity: 0.077308\n",
            "(50, 51): reprojection: 0.158706, disparity: 0.063419\n",
            "(50, 52): reprojection: 0.252898, disparity: 0.069449\n",
            "(50, 54): reprojection: 0.379095, disparity: 0.082190\n",
            "(51, 52): reprojection: 0.160466, disparity: 0.061817\n",
            "(51, 53): reprojection: 0.291052, disparity: 0.071611\n",
            "(52, 53): reprojection: 0.170764, disparity: 0.064360\n",
            "(52, 54): reprojection: 0.244893, disparity: 0.071545\n",
            "(52, 56): reprojection: 0.540124, disparity: 0.080068\n",
            "(52, 60): reprojection: 0.598095, disparity: 0.110839\n",
            "(53, 54): reprojection: 0.129133, disparity: 0.062230\n",
            "(53, 55): reprojection: 0.318323, disparity: 0.073326\n",
            "(54, 55): reprojection: 0.297234, disparity: 0.061314\n",
            "(54, 56): reprojection: 0.558614, disparity: 0.068879\n",
            "(54, 58): reprojection: 0.387119, disparity: 0.085046\n",
            "(55, 56): reprojection: 0.340210, disparity: 0.064589\n",
            "(55, 57): reprojection: 0.384845, disparity: 0.075951\n",
            "(56, 57): reprojection: 0.295560, disparity: 0.067649\n",
            "(56, 58): reprojection: 0.529842, disparity: 0.082503\n",
            "(56, 60): reprojection: 0.658983, disparity: 0.108869\n",
            "(56, 64): reprojection: 0.815353, disparity: 0.111066\n",
            "(56, 72): reprojection: 0.852745, disparity: 0.197527\n",
            "(57, 58): reprojection: 0.342793, disparity: 0.071516\n",
            "(57, 59): reprojection: 0.461471, disparity: 0.083605\n",
            "(58, 59): reprojection: 0.239959, disparity: 0.071117\n",
            "(58, 60): reprojection: 0.351838, disparity: 0.079730\n",
            "(58, 62): reprojection: 0.444433, disparity: 0.097372\n",
            "(59, 60): reprojection: 0.183573, disparity: 0.071307\n",
            "(59, 61): reprojection: 0.306412, disparity: 0.077617\n",
            "(60, 61): reprojection: 0.220711, disparity: 0.077014\n",
            "(60, 62): reprojection: 0.299076, disparity: 0.080149\n",
            "(60, 64): reprojection: 0.471292, disparity: 0.089708\n",
            "(60, 68): reprojection: 0.555385, disparity: 0.157397\n",
            "(61, 62): reprojection: 0.199543, disparity: 0.073137\n",
            "(61, 63): reprojection: 0.279154, disparity: 0.075147\n",
            "(62, 63): reprojection: 0.218952, disparity: 0.070513\n",
            "(62, 64): reprojection: 0.312273, disparity: 0.079142\n",
            "(62, 66): reprojection: 0.448386, disparity: 0.123652\n",
            "(63, 64): reprojection: 0.211310, disparity: 0.074697\n",
            "(63, 65): reprojection: 0.310148, disparity: 0.087269\n",
            "(64, 65): reprojection: 0.204316, disparity: 0.080654\n",
            "(64, 66): reprojection: 0.339905, disparity: 0.087776\n",
            "(64, 68): reprojection: 0.418707, disparity: 0.115895\n",
            "(64, 72): reprojection: 0.650177, disparity: 0.114514\n",
            "(64, 80): reprojection: 0.917718, disparity: 0.136196\n",
            "(65, 66): reprojection: 0.260873, disparity: 0.064682\n",
            "(65, 67): reprojection: 0.394885, disparity: 0.073816\n",
            "(66, 67): reprojection: 0.245214, disparity: 0.069469\n",
            "(66, 68): reprojection: 0.346486, disparity: 0.076248\n",
            "(66, 70): reprojection: 0.423381, disparity: 0.083385\n",
            "(67, 68): reprojection: 0.326964, disparity: 0.070590\n",
            "(67, 69): reprojection: 0.318747, disparity: 0.076651\n",
            "(68, 69): reprojection: 0.198357, disparity: 0.063653\n",
            "(68, 70): reprojection: 0.426486, disparity: 0.069307\n",
            "(68, 72): reprojection: 0.482374, disparity: 0.082093\n",
            "(68, 76): reprojection: 0.621392, disparity: 0.093422\n",
            "(69, 70): reprojection: 0.294194, disparity: 0.062591\n",
            "(69, 71): reprojection: 0.412737, disparity: 0.070326\n",
            "(70, 71): reprojection: 0.198953, disparity: 0.063710\n",
            "(70, 72): reprojection: 0.258188, disparity: 0.068926\n",
            "(70, 74): reprojection: 0.388236, disparity: 0.084288\n",
            "(71, 72): reprojection: 0.220696, disparity: 0.065267\n",
            "(71, 73): reprojection: 0.333228, disparity: 0.073087\n",
            "(72, 73): reprojection: 0.219770, disparity: 0.063904\n",
            "(72, 74): reprojection: 0.288339, disparity: 0.073128\n",
            "(72, 76): reprojection: 0.391533, disparity: 0.076938\n",
            "(72, 80): reprojection: 0.597550, disparity: 0.089115\n",
            "(72, 88): reprojection: 0.776763, disparity: 0.101511\n",
            "(73, 74): reprojection: 0.311162, disparity: 0.065257\n",
            "(73, 75): reprojection: 0.428261, disparity: 0.071570\n",
            "(74, 75): reprojection: 0.237756, disparity: 0.064205\n",
            "(74, 76): reprojection: 0.256951, disparity: 0.066584\n",
            "(74, 78): reprojection: 0.373523, disparity: 0.071259\n",
            "(75, 76): reprojection: 0.219241, disparity: 0.060691\n",
            "(75, 77): reprojection: 0.341692, disparity: 0.067119\n",
            "(76, 77): reprojection: 0.224477, disparity: 0.060196\n",
            "(76, 78): reprojection: 0.331730, disparity: 0.062206\n",
            "(76, 80): reprojection: 0.410531, disparity: 0.072207\n",
            "(76, 84): reprojection: 0.622331, disparity: 0.083869\n",
            "(77, 78): reprojection: 0.218256, disparity: 0.059778\n",
            "(77, 79): reprojection: 0.250037, disparity: 0.064865\n",
            "(78, 79): reprojection: 0.194383, disparity: 0.062454\n",
            "(78, 80): reprojection: 0.238590, disparity: 0.067431\n",
            "(78, 82): reprojection: 0.375915, disparity: 0.073997\n",
            "(79, 80): reprojection: 0.140495, disparity: 0.060143\n",
            "(79, 81): reprojection: 0.230869, disparity: 0.063890\n",
            "(80, 81): reprojection: 0.142364, disparity: 0.059448\n",
            "(80, 82): reprojection: 0.263437, disparity: 0.063378\n",
            "(80, 84): reprojection: 0.452957, disparity: 0.068550\n",
            "(80, 88): reprojection: 0.689409, disparity: 0.084993\n",
            "(81, 82): reprojection: 0.181567, disparity: 0.058348\n",
            "(81, 83): reprojection: 0.287540, disparity: 0.064168\n",
            "(82, 83): reprojection: 0.145429, disparity: 0.055781\n",
            "(82, 84): reprojection: 0.253484, disparity: 0.063220\n",
            "(82, 86): reprojection: 0.430143, disparity: 0.076839\n",
            "(83, 84): reprojection: 0.144901, disparity: 0.057606\n",
            "(83, 85): reprojection: 0.297000, disparity: 0.068737\n",
            "(84, 85): reprojection: 0.170597, disparity: 0.056533\n",
            "(84, 86): reprojection: 0.247193, disparity: 0.066545\n",
            "(84, 88): reprojection: 0.477481, disparity: 0.076138\n",
            "(85, 86): reprojection: 0.160102, disparity: 0.055284\n",
            "(85, 87): reprojection: 0.270172, disparity: 0.063857\n",
            "(86, 87): reprojection: 0.229810, disparity: 0.056220\n",
            "(86, 88): reprojection: 0.328994, disparity: 0.071601\n",
            "(86, 90): reprojection: 0.491188, disparity: 0.076189\n",
            "(87, 88): reprojection: 0.176694, disparity: 0.061362\n",
            "(87, 89): reprojection: 0.279722, disparity: 0.069067\n",
            "(88, 89): reprojection: 0.142722, disparity: 0.055016\n",
            "(88, 90): reprojection: 0.308666, disparity: 0.061342\n",
            "(89, 90): reprojection: 0.213133, disparity: 0.053139\n",
            "(89, 91): reprojection: 0.414311, disparity: 0.074000\n",
            "(90, 91): reprojection: 0.328489, disparity: 0.063572\n",
            "Mean:     reprojection: 0.328489, disparity: 0.063572\n",
            "Done Validation for epoch 9 (2340 iterations)\n",
            "Epoch = 9, pairs = [[30, 31], [56, 57], [48, 50], [36, 40]], loss = 0.3485148549079895\n",
            "Epoch = 9, pairs = [[39, 41], [50, 51], [0, 4], [77, 79]], loss = 0.3885939121246338\n",
            "Epoch = 9, pairs = [[12, 16], [44, 52], [64, 80], [64, 65]], loss = 0.8071383237838745\n",
            "Epoch = 9, pairs = [[47, 48], [21, 22], [11, 13], [4, 6]], loss = 0.2658068537712097\n",
            "Epoch = 9, pairs = [[74, 78], [86, 88], [60, 64], [12, 13]], loss = 0.45124438405036926\n",
            "Epoch = 9, pairs = [[26, 30], [14, 18], [15, 16], [5, 7]], loss = 0.3404422700405121\n",
            "Epoch = 9, pairs = [[69, 71], [57, 59], [80, 82], [22, 24]], loss = 0.4118703603744507\n",
            "Epoch = 9, pairs = [[8, 12], [70, 72], [58, 62], [84, 88]], loss = 0.5513726472854614\n",
            "Epoch = 9, pairs = [[78, 80], [17, 18], [19, 20], [84, 86]], loss = 0.30064117908477783\n",
            "Epoch = 9, pairs = [[85, 87], [78, 82], [46, 50], [0, 2]], loss = 0.45984935760498047\n",
            "Epoch = 9, pairs = [[64, 66], [20, 24], [44, 46], [48, 49]], loss = 0.45122000575065613\n",
            "Epoch = 9, pairs = [[42, 46], [31, 33], [22, 26], [24, 25]], loss = 0.48386818170547485\n",
            "Epoch = 9, pairs = [[29, 30], [23, 25], [61, 62], [58, 60]], loss = 0.30654528737068176\n",
            "Epoch = 9, pairs = [[32, 34], [27, 29], [32, 36], [86, 90]], loss = 0.42941951751708984\n",
            "Epoch = 9, pairs = [[45, 47], [81, 82], [6, 10], [55, 56]], loss = 0.38013890385627747\n",
            "Epoch = 9, pairs = [[64, 72], [2, 3], [28, 36], [42, 43]], loss = 0.5105066299438477\n",
            "Epoch = 9, pairs = [[26, 27], [67, 68], [10, 12], [87, 89]], loss = 0.31743913888931274\n",
            "Epoch = 9, pairs = [[1, 2], [56, 58], [65, 66], [10, 14]], loss = 0.4404135048389435\n",
            "Epoch = 9, pairs = [[29, 31], [52, 56], [3, 4], [88, 90]], loss = 0.3769734799861908\n",
            "Epoch = 9, pairs = [[4, 12], [37, 39], [27, 28], [11, 12]], loss = 0.374201238155365\n",
            "Epoch = 9, pairs = [[61, 63], [36, 38], [54, 58], [25, 27]], loss = 0.43569639325141907\n",
            "Epoch = 9, pairs = [[59, 61], [60, 68], [66, 67], [90, 91]], loss = 0.5265804529190063\n",
            "Epoch = 9, pairs = [[30, 34], [20, 21], [66, 68], [24, 32]], loss = 0.4481433629989624\n",
            "Epoch = 9, pairs = [[4, 8], [68, 69], [16, 24], [72, 76]], loss = 0.5056776404380798\n",
            "Epoch = 9, pairs = [[73, 74], [52, 60], [78, 79], [34, 38]], loss = 0.47763389348983765\n",
            "Epoch = 9, pairs = [[86, 87], [83, 84], [51, 52], [87, 88]], loss = 0.24124395847320557\n",
            "Epoch = 9, pairs = [[9, 10], [37, 38], [68, 70], [8, 9]], loss = 0.332813560962677\n",
            "Epoch = 9, pairs = [[20, 22], [18, 20], [32, 64], [34, 35]], loss = 0.7455556392669678\n",
            "Epoch = 9, pairs = [[38, 39], [1, 3], [56, 72], [16, 20]], loss = 0.6639230847358704\n",
            "Epoch = 9, pairs = [[40, 41], [45, 46], [24, 28], [88, 89]], loss = 0.3058641254901886\n",
            "Epoch = 9, pairs = [[22, 23], [48, 64], [89, 91], [40, 42]], loss = 0.6680175065994263\n",
            "Epoch = 9, pairs = [[25, 26], [72, 88], [56, 64], [71, 72]], loss = 0.7508863210678101\n",
            "Epoch = 9, pairs = [[48, 52], [49, 50], [72, 73], [16, 32]], loss = 0.5621449947357178\n",
            "Epoch = 9, pairs = [[63, 64], [8, 16], [73, 75], [62, 64]], loss = 0.47356706857681274\n",
            "Epoch = 9, pairs = [[77, 78], [41, 43], [0, 16], [42, 44]], loss = 0.5240881443023682\n",
            "Epoch = 9, pairs = [[43, 45], [47, 49], [16, 48], [7, 8]], loss = 0.6482934951782227\n",
            "Epoch = 9, pairs = [[18, 22], [57, 58], [35, 37], [21, 23]], loss = 0.39979177713394165\n",
            "Epoch = 9, pairs = [[33, 35], [82, 84], [68, 76], [81, 83]], loss = 0.4760470986366272\n",
            "Epoch = 9, pairs = [[75, 76], [60, 61], [7, 9], [63, 65]], loss = 0.3012664318084717\n",
            "Epoch = 9, pairs = [[28, 30], [66, 70], [18, 19], [76, 77]], loss = 0.36516761779785156\n",
            "Epoch = 9, pairs = [[74, 76], [50, 52], [62, 66], [84, 85]], loss = 0.35598474740982056\n",
            "Epoch = 9, pairs = [[4, 5], [15, 17], [48, 80], [82, 86]], loss = 0.7037304639816284\n",
            "Epoch = 9, pairs = [[6, 8], [43, 44], [38, 42], [76, 78]], loss = 0.42850977182388306\n",
            "Epoch = 9, pairs = [[28, 32], [19, 21], [53, 55], [79, 81]], loss = 0.4154170751571655\n",
            "Epoch = 9, pairs = [[16, 17], [71, 73], [39, 40], [24, 26]], loss = 0.34185436367988586\n",
            "Epoch = 9, pairs = [[75, 77], [26, 28], [82, 83], [62, 63]], loss = 0.3265526294708252\n",
            "Epoch = 9, pairs = [[56, 60], [8, 24], [35, 36], [48, 56]], loss = 1.027071475982666\n",
            "Epoch = 9, pairs = [[44, 48], [58, 59], [41, 42], [53, 54]], loss = 0.3661477267742157\n",
            "Epoch = 9, pairs = [[89, 90], [14, 16], [76, 84], [76, 80]], loss = 0.5341417789459229\n",
            "Epoch = 9, pairs = [[52, 54], [0, 32], [85, 86], [23, 24]], loss = 0.5771172046661377\n",
            "Epoch = 9, pairs = [[74, 75], [13, 14], [51, 53], [54, 56]], loss = 0.41506001353263855\n",
            "Epoch = 9, pairs = [[36, 44], [13, 15], [72, 80], [3, 5]], loss = 0.649673581123352\n",
            "Epoch = 9, pairs = [[32, 48], [33, 34], [0, 1], [40, 44]], loss = 0.6995684504508972\n",
            "Epoch = 9, pairs = [[5, 6], [24, 40], [30, 32], [49, 51]], loss = 0.5194356441497803\n",
            "Epoch = 9, pairs = [[28, 29], [20, 28], [31, 32], [79, 80]], loss = 0.3231695890426636\n",
            "Epoch = 9, pairs = [[34, 36], [72, 74], [50, 54], [54, 55]], loss = 0.4037688374519348\n",
            "Epoch = 9, pairs = [[67, 69], [46, 48], [10, 11], [59, 60]], loss = 0.314138799905777\n",
            "Epoch = 9, pairs = [[44, 45], [12, 14], [40, 56], [69, 70]], loss = 0.4890616834163666\n",
            "Epoch = 9, pairs = [[55, 57], [64, 68], [2, 4], [68, 72]], loss = 0.4729774594306946\n",
            "Epoch = 9, pairs = [[40, 48], [38, 40], [17, 19], [8, 10]], loss = 0.608094334602356\n",
            "Epoch = 9, pairs = [[65, 67], [70, 74], [32, 40], [12, 20]], loss = 0.6139897704124451\n",
            "Epoch = 9, pairs = [[80, 84], [9, 11], [14, 15], [52, 53]], loss = 0.3599596619606018\n",
            "Epoch = 9, pairs = [[80, 88], [83, 85], [46, 47], [60, 62]], loss = 0.4743475019931793\n",
            "Epoch = 9, pairs = [[6, 7], [70, 71], [0, 8], [36, 37]], loss = 0.3216997981071472\n",
            "Epoch = 9, pairs = [[16, 18], [80, 81], [2, 6], [32, 33]], loss = 0.3297055959701538\n",
            "Epoch 9 took 85.22s.\n",
            "( 0,  1): reprojection: 0.231450, disparity: 0.055650\n",
            "( 0,  2): reprojection: 0.341626, disparity: 0.056159\n",
            "( 0,  4): reprojection: 0.377599, disparity: 0.063036\n",
            "( 0,  8): reprojection: 0.463868, disparity: 0.073385\n",
            "( 0, 16): reprojection: 0.783904, disparity: 0.081044\n",
            "( 0, 32): reprojection: 1.283229, disparity: 0.156325\n",
            "( 1,  2): reprojection: 0.193226, disparity: 0.047433\n",
            "( 1,  3): reprojection: 0.247360, disparity: 0.051371\n",
            "( 2,  3): reprojection: 0.110611, disparity: 0.045727\n",
            "( 2,  4): reprojection: 0.178831, disparity: 0.052427\n",
            "( 2,  6): reprojection: 0.355302, disparity: 0.060725\n",
            "( 3,  4): reprojection: 0.108802, disparity: 0.047667\n",
            "( 3,  5): reprojection: 0.202321, disparity: 0.051410\n",
            "( 4,  5): reprojection: 0.155387, disparity: 0.047324\n",
            "( 4,  6): reprojection: 0.225657, disparity: 0.048283\n",
            "( 4,  8): reprojection: 0.285698, disparity: 0.057813\n",
            "( 4, 12): reprojection: 0.442976, disparity: 0.070311\n",
            "( 5,  6): reprojection: 0.121814, disparity: 0.046763\n",
            "( 5,  7): reprojection: 0.157173, disparity: 0.050548\n",
            "( 6,  7): reprojection: 0.092414, disparity: 0.046110\n",
            "( 6,  8): reprojection: 0.153996, disparity: 0.049717\n",
            "( 6, 10): reprojection: 0.391114, disparity: 0.059045\n",
            "( 7,  8): reprojection: 0.088594, disparity: 0.044734\n",
            "( 7,  9): reprojection: 0.194333, disparity: 0.049156\n",
            "( 8,  9): reprojection: 0.164064, disparity: 0.047257\n",
            "( 8, 10): reprojection: 0.373823, disparity: 0.052500\n",
            "( 8, 12): reprojection: 0.536722, disparity: 0.061345\n",
            "( 8, 16): reprojection: 0.704623, disparity: 0.072233\n",
            "( 8, 24): reprojection: 0.852942, disparity: 0.098664\n",
            "( 9, 10): reprojection: 0.262898, disparity: 0.047491\n",
            "( 9, 11): reprojection: 0.395897, disparity: 0.052283\n",
            "(10, 11): reprojection: 0.175002, disparity: 0.043082\n",
            "(10, 12): reprojection: 0.225613, disparity: 0.049652\n",
            "(10, 14): reprojection: 0.362449, disparity: 0.066022\n",
            "(11, 12): reprojection: 0.146515, disparity: 0.042811\n",
            "(11, 13): reprojection: 0.238099, disparity: 0.051792\n",
            "(12, 13): reprojection: 0.143644, disparity: 0.044136\n",
            "(12, 14): reprojection: 0.276582, disparity: 0.052262\n",
            "(12, 16): reprojection: 0.438678, disparity: 0.072967\n",
            "(12, 20): reprojection: 0.726864, disparity: 0.077397\n",
            "(13, 14): reprojection: 0.187695, disparity: 0.048183\n",
            "(13, 15): reprojection: 0.240078, disparity: 0.054519\n",
            "(14, 15): reprojection: 0.192800, disparity: 0.047441\n",
            "(14, 16): reprojection: 0.442317, disparity: 0.056001\n",
            "(14, 18): reprojection: 0.356155, disparity: 0.066852\n",
            "(15, 16): reprojection: 0.297773, disparity: 0.048476\n",
            "(15, 17): reprojection: 0.382303, disparity: 0.057862\n",
            "(16, 17): reprojection: 0.141912, disparity: 0.048382\n",
            "(16, 18): reprojection: 0.337494, disparity: 0.060120\n",
            "(16, 20): reprojection: 0.595164, disparity: 0.071913\n",
            "(16, 24): reprojection: 0.832169, disparity: 0.087025\n",
            "(16, 32): reprojection: 1.172471, disparity: 0.137835\n",
            "(16, 48): reprojection: 1.574690, disparity: 0.423392\n",
            "(17, 18): reprojection: 0.327796, disparity: 0.052198\n",
            "(17, 19): reprojection: 0.508301, disparity: 0.063740\n",
            "(18, 19): reprojection: 0.226935, disparity: 0.048575\n",
            "(18, 20): reprojection: 0.318547, disparity: 0.055160\n",
            "(18, 22): reprojection: 0.481315, disparity: 0.066032\n",
            "(19, 20): reprojection: 0.120710, disparity: 0.047119\n",
            "(19, 21): reprojection: 0.258951, disparity: 0.056787\n",
            "(20, 21): reprojection: 0.190540, disparity: 0.049577\n",
            "(20, 22): reprojection: 0.276255, disparity: 0.058357\n",
            "(20, 24): reprojection: 0.343456, disparity: 0.069578\n",
            "(20, 28): reprojection: 0.508999, disparity: 0.100203\n",
            "(21, 22): reprojection: 0.147470, disparity: 0.049887\n",
            "(21, 23): reprojection: 0.219993, disparity: 0.058360\n",
            "(22, 23): reprojection: 0.120767, disparity: 0.050789\n",
            "(22, 24): reprojection: 0.156832, disparity: 0.059980\n",
            "(22, 26): reprojection: 0.315597, disparity: 0.075843\n",
            "(23, 24): reprojection: 0.104072, disparity: 0.050651\n",
            "(23, 25): reprojection: 0.176941, disparity: 0.061344\n",
            "(24, 25): reprojection: 0.133180, disparity: 0.053753\n",
            "(24, 26): reprojection: 0.182017, disparity: 0.061790\n",
            "(24, 28): reprojection: 0.306132, disparity: 0.080279\n",
            "(24, 32): reprojection: 0.617786, disparity: 0.112488\n",
            "(24, 40): reprojection: 0.866050, disparity: 0.191240\n",
            "(25, 26): reprojection: 0.127532, disparity: 0.055910\n",
            "(25, 27): reprojection: 0.223454, disparity: 0.065041\n",
            "(26, 27): reprojection: 0.145264, disparity: 0.057664\n",
            "(26, 28): reprojection: 0.173738, disparity: 0.064977\n",
            "(26, 30): reprojection: 0.325922, disparity: 0.085648\n",
            "(27, 28): reprojection: 0.139274, disparity: 0.052185\n",
            "(27, 29): reprojection: 0.258654, disparity: 0.065137\n",
            "(28, 29): reprojection: 0.187597, disparity: 0.056566\n",
            "(28, 30): reprojection: 0.255451, disparity: 0.067365\n",
            "(28, 32): reprojection: 0.430071, disparity: 0.077628\n",
            "(28, 36): reprojection: 0.618966, disparity: 0.114282\n",
            "(29, 30): reprojection: 0.137012, disparity: 0.051853\n",
            "(29, 31): reprojection: 0.278937, disparity: 0.059611\n",
            "(30, 31): reprojection: 0.198395, disparity: 0.053040\n",
            "(30, 32): reprojection: 0.261416, disparity: 0.062215\n",
            "(30, 34): reprojection: 0.461927, disparity: 0.078389\n",
            "(31, 32): reprojection: 0.128580, disparity: 0.055178\n",
            "(31, 33): reprojection: 0.292603, disparity: 0.062702\n",
            "(32, 33): reprojection: 0.242138, disparity: 0.055797\n",
            "(32, 34): reprojection: 0.237056, disparity: 0.062319\n",
            "(32, 36): reprojection: 0.317262, disparity: 0.080597\n",
            "(32, 40): reprojection: 0.436272, disparity: 0.132841\n",
            "(32, 48): reprojection: 0.868560, disparity: 0.209914\n",
            "(32, 64): reprojection: 1.600473, disparity: 0.404565\n",
            "(33, 34): reprojection: 0.157454, disparity: 0.052666\n",
            "(33, 35): reprojection: 0.251707, disparity: 0.061831\n",
            "(34, 35): reprojection: 0.181691, disparity: 0.051948\n",
            "(34, 36): reprojection: 0.267224, disparity: 0.066677\n",
            "(34, 38): reprojection: 0.375937, disparity: 0.085890\n",
            "(35, 36): reprojection: 0.232706, disparity: 0.058083\n",
            "(35, 37): reprojection: 0.268553, disparity: 0.075142\n",
            "(36, 37): reprojection: 0.264719, disparity: 0.062068\n",
            "(36, 38): reprojection: 0.394895, disparity: 0.068357\n",
            "(36, 40): reprojection: 0.343106, disparity: 0.094005\n",
            "(36, 44): reprojection: 0.644854, disparity: 0.150500\n",
            "(37, 38): reprojection: 0.227757, disparity: 0.054582\n",
            "(37, 39): reprojection: 0.237957, disparity: 0.068461\n",
            "(38, 39): reprojection: 0.188050, disparity: 0.060561\n",
            "(38, 40): reprojection: 0.377450, disparity: 0.082115\n",
            "(38, 42): reprojection: 0.702641, disparity: 0.114579\n",
            "(39, 40): reprojection: 0.287926, disparity: 0.067099\n",
            "(39, 41): reprojection: 0.509250, disparity: 0.092202\n",
            "(40, 41): reprojection: 0.297776, disparity: 0.071648\n",
            "(40, 42): reprojection: 0.442769, disparity: 0.087775\n",
            "(40, 44): reprojection: 0.554308, disparity: 0.117128\n",
            "(40, 48): reprojection: 0.740232, disparity: 0.162931\n",
            "(40, 56): reprojection: 0.780455, disparity: 0.235223\n",
            "(41, 42): reprojection: 0.205767, disparity: 0.062867\n",
            "(41, 43): reprojection: 0.299832, disparity: 0.079203\n",
            "(42, 43): reprojection: 0.174555, disparity: 0.064847\n",
            "(42, 44): reprojection: 0.313120, disparity: 0.083458\n",
            "(42, 46): reprojection: 0.500383, disparity: 0.122734\n",
            "(43, 44): reprojection: 0.193533, disparity: 0.062629\n",
            "(43, 45): reprojection: 0.281967, disparity: 0.082919\n",
            "(44, 45): reprojection: 0.146511, disparity: 0.067396\n",
            "(44, 46): reprojection: 0.274498, disparity: 0.093804\n",
            "(44, 48): reprojection: 0.404148, disparity: 0.117175\n",
            "(44, 52): reprojection: 0.592319, disparity: 0.147382\n",
            "(45, 46): reprojection: 0.197203, disparity: 0.069387\n",
            "(45, 47): reprojection: 0.310978, disparity: 0.088801\n",
            "(46, 47): reprojection: 0.184061, disparity: 0.065195\n",
            "(46, 48): reprojection: 0.293882, disparity: 0.084852\n",
            "(46, 50): reprojection: 0.388986, disparity: 0.106810\n",
            "(47, 48): reprojection: 0.204001, disparity: 0.062771\n",
            "(47, 49): reprojection: 0.257397, disparity: 0.084175\n",
            "(48, 49): reprojection: 0.163802, disparity: 0.073319\n",
            "(48, 50): reprojection: 0.231001, disparity: 0.088066\n",
            "(48, 52): reprojection: 0.367359, disparity: 0.104174\n",
            "(48, 56): reprojection: 0.672294, disparity: 0.128378\n",
            "(48, 64): reprojection: 1.013425, disparity: 0.224945\n",
            "(48, 80): reprojection: 1.354848, disparity: 0.133960\n",
            "(49, 50): reprojection: 0.143370, disparity: 0.067939\n",
            "(49, 51): reprojection: 0.238650, disparity: 0.080437\n",
            "(50, 51): reprojection: 0.153176, disparity: 0.062774\n",
            "(50, 52): reprojection: 0.255838, disparity: 0.078315\n",
            "(50, 54): reprojection: 0.419769, disparity: 0.107726\n",
            "(51, 52): reprojection: 0.173673, disparity: 0.063946\n",
            "(51, 53): reprojection: 0.303502, disparity: 0.080936\n",
            "(52, 53): reprojection: 0.176927, disparity: 0.067955\n",
            "(52, 54): reprojection: 0.255811, disparity: 0.084040\n",
            "(52, 56): reprojection: 0.533181, disparity: 0.086982\n",
            "(52, 60): reprojection: 0.588172, disparity: 0.136264\n",
            "(53, 54): reprojection: 0.137948, disparity: 0.065171\n",
            "(53, 55): reprojection: 0.330206, disparity: 0.077900\n",
            "(54, 55): reprojection: 0.304665, disparity: 0.059477\n",
            "(54, 56): reprojection: 0.564008, disparity: 0.067437\n",
            "(54, 58): reprojection: 0.355304, disparity: 0.082943\n",
            "(55, 56): reprojection: 0.342504, disparity: 0.062902\n",
            "(55, 57): reprojection: 0.366489, disparity: 0.075224\n",
            "(56, 57): reprojection: 0.297980, disparity: 0.068296\n",
            "(56, 58): reprojection: 0.533594, disparity: 0.086526\n",
            "(56, 60): reprojection: 0.664068, disparity: 0.123701\n",
            "(56, 64): reprojection: 0.809806, disparity: 0.138053\n",
            "(56, 72): reprojection: 0.995231, disparity: 0.129970\n",
            "(57, 58): reprojection: 0.348279, disparity: 0.071468\n",
            "(57, 59): reprojection: 0.476384, disparity: 0.086676\n",
            "(58, 59): reprojection: 0.245968, disparity: 0.072420\n",
            "(58, 60): reprojection: 0.355142, disparity: 0.082544\n",
            "(58, 62): reprojection: 0.458749, disparity: 0.111549\n",
            "(59, 60): reprojection: 0.179529, disparity: 0.074432\n",
            "(59, 61): reprojection: 0.297140, disparity: 0.081519\n",
            "(60, 61): reprojection: 0.219210, disparity: 0.073048\n",
            "(60, 62): reprojection: 0.289725, disparity: 0.081549\n",
            "(60, 64): reprojection: 0.492330, disparity: 0.090655\n",
            "(60, 68): reprojection: 0.644994, disparity: 0.121483\n",
            "(61, 62): reprojection: 0.204027, disparity: 0.072695\n",
            "(61, 63): reprojection: 0.297793, disparity: 0.077304\n",
            "(62, 63): reprojection: 0.228566, disparity: 0.068522\n",
            "(62, 64): reprojection: 0.328276, disparity: 0.074747\n",
            "(62, 66): reprojection: 0.480630, disparity: 0.103078\n",
            "(63, 64): reprojection: 0.217554, disparity: 0.073861\n",
            "(63, 65): reprojection: 0.330527, disparity: 0.077220\n",
            "(64, 65): reprojection: 0.207103, disparity: 0.071699\n",
            "(64, 66): reprojection: 0.353693, disparity: 0.075962\n",
            "(64, 68): reprojection: 0.420266, disparity: 0.087142\n",
            "(64, 72): reprojection: 0.686330, disparity: 0.108415\n",
            "(64, 80): reprojection: 0.875605, disparity: 0.120494\n",
            "(65, 66): reprojection: 0.275508, disparity: 0.065031\n",
            "(65, 67): reprojection: 0.417073, disparity: 0.072699\n",
            "(66, 67): reprojection: 0.252633, disparity: 0.067343\n",
            "(66, 68): reprojection: 0.356853, disparity: 0.066045\n",
            "(66, 70): reprojection: 0.463255, disparity: 0.082612\n",
            "(67, 68): reprojection: 0.333562, disparity: 0.064811\n",
            "(67, 69): reprojection: 0.340450, disparity: 0.068837\n",
            "(68, 69): reprojection: 0.205480, disparity: 0.062214\n",
            "(68, 70): reprojection: 0.453224, disparity: 0.069710\n",
            "(68, 72): reprojection: 0.502040, disparity: 0.081529\n",
            "(68, 76): reprojection: 0.638397, disparity: 0.089683\n",
            "(69, 70): reprojection: 0.309496, disparity: 0.063742\n",
            "(69, 71): reprojection: 0.439737, disparity: 0.069565\n",
            "(70, 71): reprojection: 0.207879, disparity: 0.062709\n",
            "(70, 72): reprojection: 0.258295, disparity: 0.064594\n",
            "(70, 74): reprojection: 0.367096, disparity: 0.073186\n",
            "(71, 72): reprojection: 0.226158, disparity: 0.063056\n",
            "(71, 73): reprojection: 0.341170, disparity: 0.068814\n",
            "(72, 73): reprojection: 0.218663, disparity: 0.063109\n",
            "(72, 74): reprojection: 0.285575, disparity: 0.067585\n",
            "(72, 76): reprojection: 0.398794, disparity: 0.077066\n",
            "(72, 80): reprojection: 0.524220, disparity: 0.099154\n",
            "(72, 88): reprojection: 0.854102, disparity: 0.107119\n",
            "(73, 74): reprojection: 0.313681, disparity: 0.065351\n",
            "(73, 75): reprojection: 0.435592, disparity: 0.070819\n",
            "(74, 75): reprojection: 0.239049, disparity: 0.062701\n",
            "(74, 76): reprojection: 0.246241, disparity: 0.065205\n",
            "(74, 78): reprojection: 0.367255, disparity: 0.073755\n",
            "(75, 76): reprojection: 0.221547, disparity: 0.060295\n",
            "(75, 77): reprojection: 0.344881, disparity: 0.068472\n",
            "(76, 77): reprojection: 0.222369, disparity: 0.059122\n",
            "(76, 78): reprojection: 0.324841, disparity: 0.061767\n",
            "(76, 80): reprojection: 0.382495, disparity: 0.074357\n",
            "(76, 84): reprojection: 0.596276, disparity: 0.085612\n",
            "(77, 78): reprojection: 0.215135, disparity: 0.059874\n",
            "(77, 79): reprojection: 0.254059, disparity: 0.064137\n",
            "(78, 79): reprojection: 0.199362, disparity: 0.061852\n",
            "(78, 80): reprojection: 0.245558, disparity: 0.066849\n",
            "(78, 82): reprojection: 0.392097, disparity: 0.074288\n",
            "(79, 80): reprojection: 0.141590, disparity: 0.058917\n",
            "(79, 81): reprojection: 0.226604, disparity: 0.063992\n",
            "(80, 81): reprojection: 0.138567, disparity: 0.058182\n",
            "(80, 82): reprojection: 0.258654, disparity: 0.063162\n",
            "(80, 84): reprojection: 0.465669, disparity: 0.067678\n",
            "(80, 88): reprojection: 0.734122, disparity: 0.085319\n",
            "(81, 82): reprojection: 0.185843, disparity: 0.057167\n",
            "(81, 83): reprojection: 0.299076, disparity: 0.061576\n",
            "(82, 83): reprojection: 0.151896, disparity: 0.054903\n",
            "(82, 84): reprojection: 0.256365, disparity: 0.061578\n",
            "(82, 86): reprojection: 0.464133, disparity: 0.071250\n",
            "(83, 84): reprojection: 0.145490, disparity: 0.055356\n",
            "(83, 85): reprojection: 0.313984, disparity: 0.066536\n",
            "(84, 85): reprojection: 0.177651, disparity: 0.054874\n",
            "(84, 86): reprojection: 0.263295, disparity: 0.062602\n",
            "(84, 88): reprojection: 0.508132, disparity: 0.071065\n",
            "(85, 86): reprojection: 0.165943, disparity: 0.053658\n",
            "(85, 87): reprojection: 0.282837, disparity: 0.061168\n",
            "(86, 87): reprojection: 0.238028, disparity: 0.055332\n",
            "(86, 88): reprojection: 0.343064, disparity: 0.066635\n",
            "(86, 90): reprojection: 0.491756, disparity: 0.069116\n",
            "(87, 88): reprojection: 0.183453, disparity: 0.057861\n",
            "(87, 89): reprojection: 0.280268, disparity: 0.062489\n",
            "(88, 89): reprojection: 0.140970, disparity: 0.053811\n",
            "(88, 90): reprojection: 0.306874, disparity: 0.059118\n",
            "(89, 90): reprojection: 0.213210, disparity: 0.051577\n",
            "(89, 91): reprojection: 0.421273, disparity: 0.070035\n",
            "(90, 91): reprojection: 0.339284, disparity: 0.062627\n",
            "Mean:     reprojection: 0.339284, disparity: 0.062627\n",
            "Done Validation for epoch 10 (2600 iterations)\n",
            "Epoch = 10, pairs = [[48, 52], [37, 39], [75, 76], [56, 57]], loss = 0.35559386014938354\n",
            "Epoch = 10, pairs = [[12, 16], [27, 28], [73, 75], [48, 56]], loss = 0.5019199848175049\n",
            "Epoch = 10, pairs = [[26, 28], [10, 11], [8, 10], [54, 56]], loss = 0.3787710666656494\n",
            "Epoch = 10, pairs = [[54, 55], [24, 32], [76, 78], [13, 15]], loss = 0.4116050601005554\n",
            "Epoch = 10, pairs = [[20, 21], [4, 8], [77, 79], [58, 59]], loss = 0.307509183883667\n",
            "Epoch = 10, pairs = [[10, 12], [20, 24], [24, 26], [79, 80]], loss = 0.2842692732810974\n",
            "Epoch = 10, pairs = [[42, 46], [82, 86], [64, 65], [68, 72]], loss = 0.5045574903488159\n",
            "Epoch = 10, pairs = [[34, 36], [23, 25], [4, 12], [62, 64]], loss = 0.38041234016418457\n",
            "Epoch = 10, pairs = [[53, 55], [85, 87], [40, 42], [38, 42]], loss = 0.4794710576534271\n",
            "Epoch = 10, pairs = [[1, 2], [72, 73], [38, 40], [65, 66]], loss = 0.3221782445907593\n",
            "Epoch = 10, pairs = [[72, 80], [78, 80], [46, 48], [79, 81]], loss = 0.4509909152984619\n",
            "Epoch = 10, pairs = [[44, 45], [21, 23], [15, 17], [36, 40]], loss = 0.3426356315612793\n",
            "Epoch = 10, pairs = [[21, 22], [22, 23], [25, 27], [70, 74]], loss = 0.2748158574104309\n",
            "Epoch = 10, pairs = [[75, 77], [54, 58], [85, 86], [67, 69]], loss = 0.37935906648635864\n",
            "Epoch = 10, pairs = [[14, 15], [69, 71], [81, 83], [86, 90]], loss = 0.4088253080844879\n",
            "Epoch = 10, pairs = [[32, 34], [42, 44], [34, 35], [45, 47]], loss = 0.3374224901199341\n",
            "Epoch = 10, pairs = [[15, 16], [45, 46], [7, 9], [33, 34]], loss = 0.25862205028533936\n",
            "Epoch = 10, pairs = [[24, 28], [80, 82], [2, 4], [43, 45]], loss = 0.308682918548584\n",
            "Epoch = 10, pairs = [[26, 30], [36, 37], [72, 88], [12, 20]], loss = 0.5879634618759155\n",
            "Epoch = 10, pairs = [[89, 91], [11, 13], [61, 62], [40, 41]], loss = 0.3486059308052063\n",
            "Epoch = 10, pairs = [[32, 33], [16, 17], [14, 18], [77, 78]], loss = 0.27858009934425354\n",
            "Epoch = 10, pairs = [[28, 36], [18, 19], [23, 24], [78, 82]], loss = 0.3695985674858093\n",
            "Epoch = 10, pairs = [[51, 53], [86, 88], [52, 53], [61, 63]], loss = 0.35393720865249634\n",
            "Epoch = 10, pairs = [[3, 4], [40, 48], [53, 54], [28, 29]], loss = 0.37154796719551086\n",
            "Epoch = 10, pairs = [[14, 16], [24, 40], [84, 85], [70, 71]], loss = 0.4604761004447937\n",
            "Epoch = 10, pairs = [[63, 65], [32, 64], [44, 48], [68, 76]], loss = 0.8381982445716858\n",
            "Epoch = 10, pairs = [[49, 50], [60, 68], [36, 38], [41, 42]], loss = 0.4105301797389984\n",
            "Epoch = 10, pairs = [[35, 36], [32, 48], [22, 26], [66, 70]], loss = 0.6060400605201721\n",
            "Epoch = 10, pairs = [[71, 73], [19, 21], [0, 4], [35, 37]], loss = 0.3845561146736145\n",
            "Epoch = 10, pairs = [[59, 60], [82, 83], [16, 24], [22, 24]], loss = 0.43535134196281433\n",
            "Epoch = 10, pairs = [[80, 88], [78, 79], [2, 6], [64, 68]], loss = 0.526423454284668\n",
            "Epoch = 10, pairs = [[7, 8], [43, 44], [17, 18], [50, 54]], loss = 0.3129531443119049\n",
            "Epoch = 10, pairs = [[5, 6], [37, 38], [56, 64], [67, 68]], loss = 0.47990018129348755\n",
            "Epoch = 10, pairs = [[50, 52], [74, 78], [52, 60], [64, 66]], loss = 0.5813859105110168\n",
            "Epoch = 10, pairs = [[68, 69], [19, 20], [44, 52], [48, 80]], loss = 0.7581495046615601\n",
            "Epoch = 10, pairs = [[16, 20], [47, 49], [29, 31], [10, 14]], loss = 0.4780726134777069\n",
            "Epoch = 10, pairs = [[49, 51], [0, 2], [4, 6], [55, 57]], loss = 0.3662395775318146\n",
            "Epoch = 10, pairs = [[11, 12], [90, 91], [81, 82], [5, 7]], loss = 0.2576269805431366\n",
            "Epoch = 10, pairs = [[60, 62], [0, 32], [57, 59], [0, 16]], loss = 0.7718120813369751\n",
            "Epoch = 10, pairs = [[66, 68], [74, 76], [6, 10], [70, 72]], loss = 0.4195125102996826\n",
            "Epoch = 10, pairs = [[68, 70], [56, 60], [12, 13], [76, 77]], loss = 0.4714858829975128\n",
            "Epoch = 10, pairs = [[20, 28], [24, 25], [8, 9], [66, 67]], loss = 0.36215007305145264\n",
            "Epoch = 10, pairs = [[48, 50], [3, 5], [39, 40], [71, 72]], loss = 0.31103166937828064\n",
            "Epoch = 10, pairs = [[32, 40], [20, 22], [18, 22], [16, 18]], loss = 0.5080752968788147\n",
            "Epoch = 10, pairs = [[33, 35], [80, 81], [42, 43], [36, 44]], loss = 0.45027056336402893\n",
            "Epoch = 10, pairs = [[72, 76], [39, 41], [58, 60], [83, 85]], loss = 0.5097767114639282\n",
            "Epoch = 10, pairs = [[4, 5], [31, 32], [44, 46], [0, 1]], loss = 0.2664945125579834\n",
            "Epoch = 10, pairs = [[26, 27], [40, 44], [84, 88], [38, 39]], loss = 0.43707937002182007\n",
            "Epoch = 10, pairs = [[88, 90], [51, 52], [64, 72], [6, 7]], loss = 0.4599120020866394\n",
            "Epoch = 10, pairs = [[88, 89], [83, 84], [74, 75], [48, 64]], loss = 0.5827441215515137\n",
            "Epoch = 10, pairs = [[64, 80], [63, 64], [46, 47], [87, 89]], loss = 0.5950146317481995\n",
            "Epoch = 10, pairs = [[30, 32], [16, 32], [31, 33], [60, 64]], loss = 0.6352640986442566\n",
            "Epoch = 10, pairs = [[56, 58], [57, 58], [13, 14], [27, 29]], loss = 0.38884538412094116\n",
            "Epoch = 10, pairs = [[8, 24], [0, 8], [8, 16], [62, 63]], loss = 0.6862782835960388\n",
            "Epoch = 10, pairs = [[48, 49], [86, 87], [47, 48], [46, 50]], loss = 0.3240223228931427\n",
            "Epoch = 10, pairs = [[30, 34], [41, 43], [2, 3], [9, 10]], loss = 0.35454148054122925\n",
            "Epoch = 10, pairs = [[89, 90], [6, 8], [9, 11], [60, 61]], loss = 0.30898386240005493\n",
            "Epoch = 10, pairs = [[58, 62], [12, 14], [76, 84], [59, 61]], loss = 0.5180774331092834\n",
            "Epoch = 10, pairs = [[65, 67], [80, 84], [87, 88], [32, 36]], loss = 0.44000035524368286\n",
            "Epoch = 10, pairs = [[25, 26], [52, 56], [18, 20], [16, 48]], loss = 0.780241847038269\n",
            "Epoch = 10, pairs = [[28, 32], [84, 86], [17, 19], [29, 30]], loss = 0.4073253273963928\n",
            "Epoch = 10, pairs = [[34, 38], [62, 66], [55, 56], [30, 31]], loss = 0.45121413469314575\n",
            "Epoch = 10, pairs = [[40, 56], [50, 51], [73, 74], [52, 54]], loss = 0.5919442772865295\n",
            "Epoch = 10, pairs = [[72, 74], [76, 80], [69, 70], [82, 84]], loss = 0.3796154260635376\n",
            "Epoch = 10, pairs = [[1, 3], [28, 30], [8, 12], [56, 72]], loss = 0.5578534603118896\n",
            "Epoch 10 took 84.45s.\n",
            "( 0,  1): reprojection: 0.231181, disparity: 0.060930\n",
            "( 0,  2): reprojection: 0.343998, disparity: 0.059968\n",
            "( 0,  4): reprojection: 0.384347, disparity: 0.068566\n",
            "( 0,  8): reprojection: 0.498081, disparity: 0.074006\n",
            "( 0, 16): reprojection: 0.765111, disparity: 0.095551\n",
            "( 0, 32): reprojection: 1.056706, disparity: 0.119561\n",
            "( 1,  2): reprojection: 0.196006, disparity: 0.046480\n",
            "( 1,  3): reprojection: 0.250719, disparity: 0.049231\n",
            "( 2,  3): reprojection: 0.108364, disparity: 0.044119\n",
            "( 2,  4): reprojection: 0.179359, disparity: 0.050071\n",
            "( 2,  6): reprojection: 0.352278, disparity: 0.057883\n",
            "( 3,  4): reprojection: 0.108132, disparity: 0.045369\n",
            "( 3,  5): reprojection: 0.204887, disparity: 0.050573\n",
            "( 4,  5): reprojection: 0.155860, disparity: 0.046605\n",
            "( 4,  6): reprojection: 0.225188, disparity: 0.048915\n",
            "( 4,  8): reprojection: 0.294698, disparity: 0.059210\n",
            "( 4, 12): reprojection: 0.424956, disparity: 0.073585\n",
            "( 5,  6): reprojection: 0.117530, disparity: 0.044111\n",
            "( 5,  7): reprojection: 0.156718, disparity: 0.048523\n",
            "( 6,  7): reprojection: 0.095536, disparity: 0.044250\n",
            "( 6,  8): reprojection: 0.161710, disparity: 0.049925\n",
            "( 6, 10): reprojection: 0.392952, disparity: 0.059225\n",
            "( 7,  8): reprojection: 0.087016, disparity: 0.043739\n",
            "( 7,  9): reprojection: 0.193876, disparity: 0.049483\n",
            "( 8,  9): reprojection: 0.158441, disparity: 0.046117\n",
            "( 8, 10): reprojection: 0.358561, disparity: 0.052332\n",
            "( 8, 12): reprojection: 0.515341, disparity: 0.067529\n",
            "( 8, 16): reprojection: 0.617841, disparity: 0.077176\n",
            "( 8, 24): reprojection: 0.709867, disparity: 0.103036\n",
            "( 9, 10): reprojection: 0.246901, disparity: 0.045906\n",
            "( 9, 11): reprojection: 0.367512, disparity: 0.053630\n",
            "(10, 11): reprojection: 0.161164, disparity: 0.042957\n",
            "(10, 12): reprojection: 0.205418, disparity: 0.051590\n",
            "(10, 14): reprojection: 0.346899, disparity: 0.066588\n",
            "(11, 12): reprojection: 0.141679, disparity: 0.042419\n",
            "(11, 13): reprojection: 0.226939, disparity: 0.052401\n",
            "(12, 13): reprojection: 0.128024, disparity: 0.042976\n",
            "(12, 14): reprojection: 0.256014, disparity: 0.051690\n",
            "(12, 16): reprojection: 0.401014, disparity: 0.071085\n",
            "(12, 20): reprojection: 0.633131, disparity: 0.084529\n",
            "(13, 14): reprojection: 0.185848, disparity: 0.045398\n",
            "(13, 15): reprojection: 0.221910, disparity: 0.053767\n",
            "(14, 15): reprojection: 0.179591, disparity: 0.047338\n",
            "(14, 16): reprojection: 0.410496, disparity: 0.055281\n",
            "(14, 18): reprojection: 0.320146, disparity: 0.067816\n",
            "(15, 16): reprojection: 0.286130, disparity: 0.046303\n",
            "(15, 17): reprojection: 0.360723, disparity: 0.057383\n",
            "(16, 17): reprojection: 0.132015, disparity: 0.047748\n",
            "(16, 18): reprojection: 0.319387, disparity: 0.057632\n",
            "(16, 20): reprojection: 0.587403, disparity: 0.071528\n",
            "(16, 24): reprojection: 0.796849, disparity: 0.094552\n",
            "(16, 32): reprojection: 1.100374, disparity: 0.118868\n",
            "(16, 48): reprojection: 1.351553, disparity: 0.163119\n",
            "(17, 18): reprojection: 0.309540, disparity: 0.050114\n",
            "(17, 19): reprojection: 0.487205, disparity: 0.059428\n",
            "(18, 19): reprojection: 0.220423, disparity: 0.047482\n",
            "(18, 20): reprojection: 0.319331, disparity: 0.057006\n",
            "(18, 22): reprojection: 0.491886, disparity: 0.065593\n",
            "(19, 20): reprojection: 0.121378, disparity: 0.046881\n",
            "(19, 21): reprojection: 0.267805, disparity: 0.055283\n",
            "(20, 21): reprojection: 0.180640, disparity: 0.047359\n",
            "(20, 22): reprojection: 0.264496, disparity: 0.057391\n",
            "(20, 24): reprojection: 0.335520, disparity: 0.069491\n",
            "(20, 28): reprojection: 0.544528, disparity: 0.103457\n",
            "(21, 22): reprojection: 0.137441, disparity: 0.049080\n",
            "(21, 23): reprojection: 0.204749, disparity: 0.058373\n",
            "(22, 23): reprojection: 0.110580, disparity: 0.049275\n",
            "(22, 24): reprojection: 0.167048, disparity: 0.059288\n",
            "(22, 26): reprojection: 0.322146, disparity: 0.073306\n",
            "(23, 24): reprojection: 0.107656, disparity: 0.049863\n",
            "(23, 25): reprojection: 0.180257, disparity: 0.060674\n",
            "(24, 25): reprojection: 0.119213, disparity: 0.051273\n",
            "(24, 26): reprojection: 0.172823, disparity: 0.060420\n",
            "(24, 28): reprojection: 0.305844, disparity: 0.077836\n",
            "(24, 32): reprojection: 0.580316, disparity: 0.091729\n",
            "(24, 40): reprojection: 0.919920, disparity: 0.119143\n",
            "(25, 26): reprojection: 0.129734, disparity: 0.053552\n",
            "(25, 27): reprojection: 0.212225, disparity: 0.061621\n",
            "(26, 27): reprojection: 0.134834, disparity: 0.056210\n",
            "(26, 28): reprojection: 0.177689, disparity: 0.063756\n",
            "(26, 30): reprojection: 0.320059, disparity: 0.073468\n",
            "(27, 28): reprojection: 0.131859, disparity: 0.051455\n",
            "(27, 29): reprojection: 0.247399, disparity: 0.058341\n",
            "(28, 29): reprojection: 0.172068, disparity: 0.054155\n",
            "(28, 30): reprojection: 0.243791, disparity: 0.063532\n",
            "(28, 32): reprojection: 0.398075, disparity: 0.070100\n",
            "(28, 36): reprojection: 0.601766, disparity: 0.092163\n",
            "(29, 30): reprojection: 0.131023, disparity: 0.052997\n",
            "(29, 31): reprojection: 0.265078, disparity: 0.058468\n",
            "(30, 31): reprojection: 0.184548, disparity: 0.051975\n",
            "(30, 32): reprojection: 0.251659, disparity: 0.058544\n",
            "(30, 34): reprojection: 0.439933, disparity: 0.067801\n",
            "(31, 32): reprojection: 0.126921, disparity: 0.054126\n",
            "(31, 33): reprojection: 0.287726, disparity: 0.057649\n",
            "(32, 33): reprojection: 0.231787, disparity: 0.051352\n",
            "(32, 34): reprojection: 0.229914, disparity: 0.056082\n",
            "(32, 36): reprojection: 0.334048, disparity: 0.068791\n",
            "(32, 40): reprojection: 0.424387, disparity: 0.105850\n",
            "(32, 48): reprojection: 0.851554, disparity: 0.135542\n",
            "(32, 64): reprojection: 1.195717, disparity: 0.152200\n",
            "(33, 34): reprojection: 0.149399, disparity: 0.050541\n",
            "(33, 35): reprojection: 0.259111, disparity: 0.056129\n",
            "(34, 35): reprojection: 0.182275, disparity: 0.050321\n",
            "(34, 36): reprojection: 0.267557, disparity: 0.059607\n",
            "(34, 38): reprojection: 0.367097, disparity: 0.071800\n",
            "(35, 36): reprojection: 0.221944, disparity: 0.054501\n",
            "(35, 37): reprojection: 0.262193, disparity: 0.071531\n",
            "(36, 37): reprojection: 0.258983, disparity: 0.059009\n",
            "(36, 38): reprojection: 0.386031, disparity: 0.068539\n",
            "(36, 40): reprojection: 0.327671, disparity: 0.077069\n",
            "(36, 44): reprojection: 0.645640, disparity: 0.098980\n",
            "(37, 38): reprojection: 0.222890, disparity: 0.054404\n",
            "(37, 39): reprojection: 0.243694, disparity: 0.062244\n",
            "(38, 39): reprojection: 0.188068, disparity: 0.056130\n",
            "(38, 40): reprojection: 0.361179, disparity: 0.067858\n",
            "(38, 42): reprojection: 0.657850, disparity: 0.084969\n",
            "(39, 40): reprojection: 0.279062, disparity: 0.059358\n",
            "(39, 41): reprojection: 0.492414, disparity: 0.071392\n",
            "(40, 41): reprojection: 0.293660, disparity: 0.063455\n",
            "(40, 42): reprojection: 0.422390, disparity: 0.073910\n",
            "(40, 44): reprojection: 0.553994, disparity: 0.090152\n",
            "(40, 48): reprojection: 0.768864, disparity: 0.116251\n",
            "(40, 56): reprojection: 0.738960, disparity: 0.118294\n",
            "(41, 42): reprojection: 0.192088, disparity: 0.059145\n",
            "(41, 43): reprojection: 0.297710, disparity: 0.069853\n",
            "(42, 43): reprojection: 0.175171, disparity: 0.060409\n",
            "(42, 44): reprojection: 0.322798, disparity: 0.075423\n",
            "(42, 46): reprojection: 0.538681, disparity: 0.096817\n",
            "(43, 44): reprojection: 0.198443, disparity: 0.060944\n",
            "(43, 45): reprojection: 0.296487, disparity: 0.077044\n",
            "(44, 45): reprojection: 0.147065, disparity: 0.066843\n",
            "(44, 46): reprojection: 0.284759, disparity: 0.092358\n",
            "(44, 48): reprojection: 0.403700, disparity: 0.108106\n",
            "(44, 52): reprojection: 0.599463, disparity: 0.111558\n",
            "(45, 46): reprojection: 0.199920, disparity: 0.066859\n",
            "(45, 47): reprojection: 0.317184, disparity: 0.084625\n",
            "(46, 47): reprojection: 0.183991, disparity: 0.063645\n",
            "(46, 48): reprojection: 0.287924, disparity: 0.077035\n",
            "(46, 50): reprojection: 0.393549, disparity: 0.087043\n",
            "(47, 48): reprojection: 0.202083, disparity: 0.061006\n",
            "(47, 49): reprojection: 0.255058, disparity: 0.071147\n",
            "(48, 49): reprojection: 0.163053, disparity: 0.066530\n",
            "(48, 50): reprojection: 0.233239, disparity: 0.077087\n",
            "(48, 52): reprojection: 0.365246, disparity: 0.095918\n",
            "(48, 56): reprojection: 0.663730, disparity: 0.121505\n",
            "(48, 64): reprojection: 0.918249, disparity: 0.160352\n",
            "(48, 80): reprojection: 1.206141, disparity: 0.164578\n",
            "(49, 50): reprojection: 0.140109, disparity: 0.065632\n",
            "(49, 51): reprojection: 0.232926, disparity: 0.077096\n",
            "(50, 51): reprojection: 0.146583, disparity: 0.063853\n",
            "(50, 52): reprojection: 0.252825, disparity: 0.073189\n",
            "(50, 54): reprojection: 0.411115, disparity: 0.094814\n",
            "(51, 52): reprojection: 0.171070, disparity: 0.061661\n",
            "(51, 53): reprojection: 0.317639, disparity: 0.073206\n",
            "(52, 53): reprojection: 0.177424, disparity: 0.062465\n",
            "(52, 54): reprojection: 0.264041, disparity: 0.078889\n",
            "(52, 56): reprojection: 0.540482, disparity: 0.078799\n",
            "(52, 60): reprojection: 0.612763, disparity: 0.110164\n",
            "(53, 54): reprojection: 0.133435, disparity: 0.064650\n",
            "(53, 55): reprojection: 0.333611, disparity: 0.079228\n",
            "(54, 55): reprojection: 0.309760, disparity: 0.060972\n",
            "(54, 56): reprojection: 0.554653, disparity: 0.065077\n",
            "(54, 58): reprojection: 0.371367, disparity: 0.086350\n",
            "(55, 56): reprojection: 0.341978, disparity: 0.063824\n",
            "(55, 57): reprojection: 0.371939, disparity: 0.072120\n",
            "(56, 57): reprojection: 0.302050, disparity: 0.068391\n",
            "(56, 58): reprojection: 0.549129, disparity: 0.084727\n",
            "(56, 60): reprojection: 0.666919, disparity: 0.097241\n",
            "(56, 64): reprojection: 0.836728, disparity: 0.112958\n",
            "(56, 72): reprojection: 0.836816, disparity: 0.109953\n",
            "(57, 58): reprojection: 0.351458, disparity: 0.069061\n",
            "(57, 59): reprojection: 0.478406, disparity: 0.079247\n",
            "(58, 59): reprojection: 0.254659, disparity: 0.071020\n",
            "(58, 60): reprojection: 0.365152, disparity: 0.084161\n",
            "(58, 62): reprojection: 0.460598, disparity: 0.097197\n",
            "(59, 60): reprojection: 0.183998, disparity: 0.072497\n",
            "(59, 61): reprojection: 0.308944, disparity: 0.080886\n",
            "(60, 61): reprojection: 0.221504, disparity: 0.072495\n",
            "(60, 62): reprojection: 0.307683, disparity: 0.081363\n",
            "(60, 64): reprojection: 0.507920, disparity: 0.101448\n",
            "(60, 68): reprojection: 0.565732, disparity: 0.115293\n",
            "(61, 62): reprojection: 0.222116, disparity: 0.075210\n",
            "(61, 63): reprojection: 0.303073, disparity: 0.078098\n",
            "(62, 63): reprojection: 0.229690, disparity: 0.070370\n",
            "(62, 64): reprojection: 0.346150, disparity: 0.080494\n",
            "(62, 66): reprojection: 0.477290, disparity: 0.103196\n",
            "(63, 64): reprojection: 0.219632, disparity: 0.074019\n",
            "(63, 65): reprojection: 0.309426, disparity: 0.072329\n",
            "(64, 65): reprojection: 0.201312, disparity: 0.073795\n",
            "(64, 66): reprojection: 0.329663, disparity: 0.076313\n",
            "(64, 68): reprojection: 0.431447, disparity: 0.097978\n",
            "(64, 72): reprojection: 0.721009, disparity: 0.132760\n",
            "(64, 80): reprojection: 0.869448, disparity: 0.122618\n",
            "(65, 66): reprojection: 0.283457, disparity: 0.066037\n",
            "(65, 67): reprojection: 0.425171, disparity: 0.074325\n",
            "(66, 67): reprojection: 0.256852, disparity: 0.069680\n",
            "(66, 68): reprojection: 0.362691, disparity: 0.072814\n",
            "(66, 70): reprojection: 0.449915, disparity: 0.086905\n",
            "(67, 68): reprojection: 0.339500, disparity: 0.066449\n",
            "(67, 69): reprojection: 0.344100, disparity: 0.071197\n",
            "(68, 69): reprojection: 0.200017, disparity: 0.063750\n",
            "(68, 70): reprojection: 0.446124, disparity: 0.072706\n",
            "(68, 72): reprojection: 0.480537, disparity: 0.089632\n",
            "(68, 76): reprojection: 0.620164, disparity: 0.109409\n",
            "(69, 70): reprojection: 0.308678, disparity: 0.062002\n",
            "(69, 71): reprojection: 0.449764, disparity: 0.071235\n",
            "(70, 71): reprojection: 0.205447, disparity: 0.062671\n",
            "(70, 72): reprojection: 0.258761, disparity: 0.069571\n",
            "(70, 74): reprojection: 0.368046, disparity: 0.084149\n",
            "(71, 72): reprojection: 0.229852, disparity: 0.063373\n",
            "(71, 73): reprojection: 0.352980, disparity: 0.072324\n",
            "(72, 73): reprojection: 0.231284, disparity: 0.065297\n",
            "(72, 74): reprojection: 0.283323, disparity: 0.072168\n",
            "(72, 76): reprojection: 0.396231, disparity: 0.086348\n",
            "(72, 80): reprojection: 0.530595, disparity: 0.102827\n",
            "(72, 88): reprojection: 0.793255, disparity: 0.107691\n",
            "(73, 74): reprojection: 0.309056, disparity: 0.067398\n",
            "(73, 75): reprojection: 0.410122, disparity: 0.072612\n",
            "(74, 75): reprojection: 0.227321, disparity: 0.064637\n",
            "(74, 76): reprojection: 0.251183, disparity: 0.067600\n",
            "(74, 78): reprojection: 0.351374, disparity: 0.080761\n",
            "(75, 76): reprojection: 0.228206, disparity: 0.061416\n",
            "(75, 77): reprojection: 0.352833, disparity: 0.069308\n",
            "(76, 77): reprojection: 0.224860, disparity: 0.061505\n",
            "(76, 78): reprojection: 0.328124, disparity: 0.066957\n",
            "(76, 80): reprojection: 0.384413, disparity: 0.078968\n",
            "(76, 84): reprojection: 0.532669, disparity: 0.092470\n",
            "(77, 78): reprojection: 0.220965, disparity: 0.060714\n",
            "(77, 79): reprojection: 0.262078, disparity: 0.066135\n",
            "(78, 79): reprojection: 0.195602, disparity: 0.062328\n",
            "(78, 80): reprojection: 0.249834, disparity: 0.067765\n",
            "(78, 82): reprojection: 0.393423, disparity: 0.074844\n",
            "(79, 80): reprojection: 0.144189, disparity: 0.060203\n",
            "(79, 81): reprojection: 0.234937, disparity: 0.063383\n",
            "(80, 81): reprojection: 0.142436, disparity: 0.057825\n",
            "(80, 82): reprojection: 0.248155, disparity: 0.067141\n",
            "(80, 84): reprojection: 0.429404, disparity: 0.070186\n",
            "(80, 88): reprojection: 0.689943, disparity: 0.086851\n",
            "(81, 82): reprojection: 0.183118, disparity: 0.059041\n",
            "(81, 83): reprojection: 0.291958, disparity: 0.065897\n",
            "(82, 83): reprojection: 0.155622, disparity: 0.058157\n",
            "(82, 84): reprojection: 0.246837, disparity: 0.062978\n",
            "(82, 86): reprojection: 0.467680, disparity: 0.073664\n",
            "(83, 84): reprojection: 0.143117, disparity: 0.056737\n",
            "(83, 85): reprojection: 0.302235, disparity: 0.067391\n",
            "(84, 85): reprojection: 0.175744, disparity: 0.055292\n",
            "(84, 86): reprojection: 0.269094, disparity: 0.063659\n",
            "(84, 88): reprojection: 0.489261, disparity: 0.069780\n",
            "(85, 86): reprojection: 0.168234, disparity: 0.054520\n",
            "(85, 87): reprojection: 0.288302, disparity: 0.062158\n",
            "(86, 87): reprojection: 0.239351, disparity: 0.057176\n",
            "(86, 88): reprojection: 0.325030, disparity: 0.066671\n",
            "(86, 90): reprojection: 0.466990, disparity: 0.070264\n",
            "(87, 88): reprojection: 0.171674, disparity: 0.056644\n",
            "(87, 89): reprojection: 0.266325, disparity: 0.062346\n",
            "(88, 89): reprojection: 0.137117, disparity: 0.052729\n",
            "(88, 90): reprojection: 0.304453, disparity: 0.057807\n",
            "(89, 90): reprojection: 0.214043, disparity: 0.053475\n",
            "(89, 91): reprojection: 0.426443, disparity: 0.066628\n",
            "(90, 91): reprojection: 0.336346, disparity: 0.058356\n",
            "Mean:     reprojection: 0.336346, disparity: 0.058356\n",
            "Done Validation for epoch 11 (2860 iterations)\n",
            "Epoch = 11, pairs = [[48, 64], [21, 22], [43, 44], [46, 47]], loss = 0.4492669701576233\n",
            "Epoch = 11, pairs = [[38, 42], [90, 91], [62, 66], [10, 14]], loss = 0.5530382394790649\n",
            "Epoch = 11, pairs = [[87, 89], [73, 75], [50, 54], [24, 26]], loss = 0.4529901146888733\n",
            "Epoch = 11, pairs = [[20, 22], [44, 52], [18, 20], [66, 67]], loss = 0.44707977771759033\n",
            "Epoch = 11, pairs = [[42, 43], [76, 77], [67, 68], [17, 19]], loss = 0.37020444869995117\n",
            "Epoch = 11, pairs = [[14, 18], [12, 16], [64, 68], [57, 58]], loss = 0.46295636892318726\n",
            "Epoch = 11, pairs = [[68, 70], [64, 80], [47, 49], [17, 18]], loss = 0.5447673797607422\n",
            "Epoch = 11, pairs = [[16, 17], [69, 71], [52, 60], [1, 3]], loss = 0.40622037649154663\n",
            "Epoch = 11, pairs = [[81, 83], [32, 33], [58, 62], [29, 31]], loss = 0.370799720287323\n",
            "Epoch = 11, pairs = [[45, 46], [27, 28], [86, 90], [87, 88]], loss = 0.3098958134651184\n",
            "Epoch = 11, pairs = [[59, 61], [55, 56], [41, 43], [25, 26]], loss = 0.33464157581329346\n",
            "Epoch = 11, pairs = [[64, 66], [13, 15], [69, 70], [70, 74]], loss = 0.38653528690338135\n",
            "Epoch = 11, pairs = [[57, 59], [16, 48], [76, 80], [23, 25]], loss = 0.783761203289032\n",
            "Epoch = 11, pairs = [[40, 56], [64, 72], [5, 7], [0, 32]], loss = 0.7998871803283691\n",
            "Epoch = 11, pairs = [[52, 54], [51, 52], [6, 7], [8, 12]], loss = 0.3184658885002136\n",
            "Epoch = 11, pairs = [[82, 86], [80, 81], [60, 64], [84, 86]], loss = 0.5099632740020752\n",
            "Epoch = 11, pairs = [[75, 77], [34, 36], [19, 20], [63, 64]], loss = 0.3142606317996979\n",
            "Epoch = 11, pairs = [[24, 32], [83, 84], [36, 44], [25, 27]], loss = 0.594802975654602\n",
            "Epoch = 11, pairs = [[28, 32], [41, 42], [80, 84], [65, 66]], loss = 0.43266069889068604\n",
            "Epoch = 11, pairs = [[22, 24], [75, 76], [82, 84], [2, 4]], loss = 0.27215903997421265\n",
            "Epoch = 11, pairs = [[49, 50], [5, 6], [26, 30], [48, 52]], loss = 0.32278549671173096\n",
            "Epoch = 11, pairs = [[0, 4], [11, 12], [83, 85], [31, 32]], loss = 0.2948240041732788\n",
            "Epoch = 11, pairs = [[78, 80], [85, 86], [50, 51], [88, 89]], loss = 0.23587970435619354\n",
            "Epoch = 11, pairs = [[29, 30], [3, 5], [77, 79], [12, 20]], loss = 0.3950672745704651\n",
            "Epoch = 11, pairs = [[72, 76], [30, 34], [26, 28], [4, 5]], loss = 0.40426841378211975\n",
            "Epoch = 11, pairs = [[86, 88], [59, 60], [68, 72], [24, 40]], loss = 0.625709056854248\n",
            "Epoch = 11, pairs = [[82, 83], [70, 71], [26, 27], [45, 47]], loss = 0.28544896841049194\n",
            "Epoch = 11, pairs = [[40, 42], [30, 32], [84, 85], [8, 10]], loss = 0.37293216586112976\n",
            "Epoch = 11, pairs = [[77, 78], [39, 40], [24, 25], [85, 87]], loss = 0.30367425084114075\n",
            "Epoch = 11, pairs = [[79, 80], [33, 34], [50, 52], [20, 21]], loss = 0.24897854030132294\n",
            "Epoch = 11, pairs = [[61, 63], [4, 12], [53, 55], [54, 56]], loss = 0.5099184513092041\n",
            "Epoch = 11, pairs = [[74, 75], [8, 16], [72, 73], [56, 58]], loss = 0.5288161039352417\n",
            "Epoch = 11, pairs = [[71, 72], [28, 30], [89, 91], [0, 1]], loss = 0.34452980756759644\n",
            "Epoch = 11, pairs = [[1, 2], [15, 17], [70, 72], [4, 6]], loss = 0.3167939782142639\n",
            "Epoch = 11, pairs = [[46, 50], [18, 22], [38, 40], [20, 28]], loss = 0.5835164785385132\n",
            "Epoch = 11, pairs = [[32, 36], [13, 14], [89, 90], [16, 32]], loss = 0.5579066872596741\n",
            "Epoch = 11, pairs = [[8, 24], [62, 64], [80, 88], [44, 45]], loss = 0.5782462358474731\n",
            "Epoch = 11, pairs = [[19, 21], [32, 34], [58, 59], [3, 4]], loss = 0.25919944047927856\n",
            "Epoch = 11, pairs = [[60, 62], [58, 60], [10, 11], [37, 38]], loss = 0.31828874349594116\n",
            "Epoch = 11, pairs = [[16, 24], [72, 74], [2, 3], [8, 9]], loss = 0.4159279465675354\n",
            "Epoch = 11, pairs = [[54, 55], [63, 65], [40, 48], [7, 9]], loss = 0.5158560872077942\n",
            "Epoch = 11, pairs = [[78, 79], [31, 33], [35, 37], [16, 20]], loss = 0.3976646363735199\n",
            "Epoch = 11, pairs = [[60, 68], [33, 35], [4, 8], [44, 48]], loss = 0.4609297513961792\n",
            "Epoch = 11, pairs = [[81, 82], [24, 28], [38, 39], [9, 10]], loss = 0.28706175088882446\n",
            "Epoch = 11, pairs = [[22, 26], [6, 8], [56, 57], [61, 62]], loss = 0.3042014241218567\n",
            "Epoch = 11, pairs = [[36, 37], [35, 36], [37, 39], [42, 44]], loss = 0.33729761838912964\n",
            "Epoch = 11, pairs = [[78, 82], [0, 2], [65, 67], [14, 15]], loss = 0.387417197227478\n",
            "Epoch = 11, pairs = [[15, 16], [80, 82], [10, 12], [32, 48]], loss = 0.4831553101539612\n",
            "Epoch = 11, pairs = [[39, 41], [12, 14], [32, 64], [66, 68]], loss = 0.6190482974052429\n",
            "Epoch = 11, pairs = [[56, 60], [18, 19], [74, 78], [14, 16]], loss = 0.4883166551589966\n",
            "Epoch = 11, pairs = [[40, 41], [66, 70], [32, 40], [48, 80]], loss = 0.6671857237815857\n",
            "Epoch = 11, pairs = [[79, 81], [60, 61], [21, 23], [71, 73]], loss = 0.3101491630077362\n",
            "Epoch = 11, pairs = [[2, 6], [23, 24], [72, 80], [30, 31]], loss = 0.3659553825855255\n",
            "Epoch = 11, pairs = [[76, 84], [54, 58], [51, 53], [20, 24]], loss = 0.4763227701187134\n",
            "Epoch = 11, pairs = [[62, 63], [68, 69], [46, 48], [53, 54]], loss = 0.3046942949295044\n",
            "Epoch = 11, pairs = [[56, 72], [7, 8], [86, 87], [52, 53]], loss = 0.49314191937446594\n",
            "Epoch = 11, pairs = [[55, 57], [64, 65], [48, 49], [9, 11]], loss = 0.35247373580932617\n",
            "Epoch = 11, pairs = [[42, 46], [56, 64], [16, 18], [0, 16]], loss = 0.68720543384552\n",
            "Epoch = 11, pairs = [[74, 76], [28, 29], [27, 29], [49, 51]], loss = 0.2998674809932709\n",
            "Epoch = 11, pairs = [[11, 13], [72, 88], [73, 74], [48, 56]], loss = 0.5783243775367737\n",
            "Epoch = 11, pairs = [[28, 36], [43, 45], [68, 76], [44, 46]], loss = 0.5752723217010498\n",
            "Epoch = 11, pairs = [[12, 13], [67, 69], [0, 8], [84, 88]], loss = 0.42241257429122925\n",
            "Epoch = 11, pairs = [[48, 50], [47, 48], [40, 44], [88, 90]], loss = 0.39582452178001404\n",
            "Epoch = 11, pairs = [[36, 38], [6, 10], [34, 38], [22, 23]], loss = 0.3750971555709839\n",
            "Epoch = 11, pairs = [[76, 78], [52, 56], [36, 40], [34, 35]], loss = 0.4390842914581299\n",
            "Epoch 11 took 85.28s.\n",
            "( 0,  1): reprojection: 0.226785, disparity: 0.057447\n",
            "( 0,  2): reprojection: 0.330734, disparity: 0.057903\n",
            "( 0,  4): reprojection: 0.360414, disparity: 0.067675\n",
            "( 0,  8): reprojection: 0.484136, disparity: 0.082963\n",
            "( 0, 16): reprojection: 0.738024, disparity: 0.083908\n",
            "( 0, 32): reprojection: 1.269988, disparity: 0.135074\n",
            "( 1,  2): reprojection: 0.191128, disparity: 0.047599\n",
            "( 1,  3): reprojection: 0.243077, disparity: 0.052711\n",
            "( 2,  3): reprojection: 0.108305, disparity: 0.049055\n",
            "( 2,  4): reprojection: 0.177932, disparity: 0.055848\n",
            "( 2,  6): reprojection: 0.365434, disparity: 0.068087\n",
            "( 3,  4): reprojection: 0.101973, disparity: 0.049367\n",
            "( 3,  5): reprojection: 0.210084, disparity: 0.056139\n",
            "( 4,  5): reprojection: 0.160284, disparity: 0.049170\n",
            "( 4,  6): reprojection: 0.238673, disparity: 0.054575\n",
            "( 4,  8): reprojection: 0.309440, disparity: 0.067156\n",
            "( 4, 12): reprojection: 0.575641, disparity: 0.080937\n",
            "( 5,  6): reprojection: 0.119735, disparity: 0.048685\n",
            "( 5,  7): reprojection: 0.158535, disparity: 0.055992\n",
            "( 6,  7): reprojection: 0.089208, disparity: 0.048237\n",
            "( 6,  8): reprojection: 0.161679, disparity: 0.054404\n",
            "( 6, 10): reprojection: 0.438437, disparity: 0.065830\n",
            "( 7,  8): reprojection: 0.092125, disparity: 0.049082\n",
            "( 7,  9): reprojection: 0.219801, disparity: 0.056640\n",
            "( 8,  9): reprojection: 0.169504, disparity: 0.050083\n",
            "( 8, 10): reprojection: 0.387913, disparity: 0.057389\n",
            "( 8, 12): reprojection: 0.603720, disparity: 0.069780\n",
            "( 8, 16): reprojection: 0.733167, disparity: 0.080520\n",
            "( 8, 24): reprojection: 1.089962, disparity: 0.115807\n",
            "( 9, 10): reprojection: 0.267889, disparity: 0.049710\n",
            "( 9, 11): reprojection: 0.410182, disparity: 0.057423\n",
            "(10, 11): reprojection: 0.185219, disparity: 0.045482\n",
            "(10, 12): reprojection: 0.242027, disparity: 0.055525\n",
            "(10, 14): reprojection: 0.393584, disparity: 0.070483\n",
            "(11, 12): reprojection: 0.143762, disparity: 0.045696\n",
            "(11, 13): reprojection: 0.242612, disparity: 0.056851\n",
            "(12, 13): reprojection: 0.127370, disparity: 0.046705\n",
            "(12, 14): reprojection: 0.273518, disparity: 0.056689\n",
            "(12, 16): reprojection: 0.363078, disparity: 0.077912\n",
            "(12, 20): reprojection: 0.675790, disparity: 0.086169\n",
            "(13, 14): reprojection: 0.202952, disparity: 0.050540\n",
            "(13, 15): reprojection: 0.188750, disparity: 0.059740\n",
            "(14, 15): reprojection: 0.173412, disparity: 0.051968\n",
            "(14, 16): reprojection: 0.418314, disparity: 0.059805\n",
            "(14, 18): reprojection: 0.372563, disparity: 0.076758\n",
            "(15, 16): reprojection: 0.294415, disparity: 0.051925\n",
            "(15, 17): reprojection: 0.384422, disparity: 0.060878\n",
            "(16, 17): reprojection: 0.144286, disparity: 0.050373\n",
            "(16, 18): reprojection: 0.359694, disparity: 0.065720\n",
            "(16, 20): reprojection: 0.694524, disparity: 0.083160\n",
            "(16, 24): reprojection: 0.936144, disparity: 0.094105\n",
            "(16, 32): reprojection: 1.177286, disparity: 0.135090\n",
            "(16, 48): reprojection: 1.698090, disparity: 0.271541\n",
            "(17, 18): reprojection: 0.329459, disparity: 0.054594\n",
            "(17, 19): reprojection: 0.537087, disparity: 0.067648\n",
            "(18, 19): reprojection: 0.246210, disparity: 0.053783\n",
            "(18, 20): reprojection: 0.379596, disparity: 0.062679\n",
            "(18, 22): reprojection: 0.557730, disparity: 0.073577\n",
            "(19, 20): reprojection: 0.143611, disparity: 0.053000\n",
            "(19, 21): reprojection: 0.314959, disparity: 0.064821\n",
            "(20, 21): reprojection: 0.190085, disparity: 0.054700\n",
            "(20, 22): reprojection: 0.291879, disparity: 0.064093\n",
            "(20, 24): reprojection: 0.358325, disparity: 0.070258\n",
            "(20, 28): reprojection: 0.534727, disparity: 0.101688\n",
            "(21, 22): reprojection: 0.143803, disparity: 0.053240\n",
            "(21, 23): reprojection: 0.195226, disparity: 0.060639\n",
            "(22, 23): reprojection: 0.126213, disparity: 0.055479\n",
            "(22, 24): reprojection: 0.188664, disparity: 0.063351\n",
            "(22, 26): reprojection: 0.417775, disparity: 0.078894\n",
            "(23, 24): reprojection: 0.120574, disparity: 0.054476\n",
            "(23, 25): reprojection: 0.236651, disparity: 0.067648\n",
            "(24, 25): reprojection: 0.134326, disparity: 0.057028\n",
            "(24, 26): reprojection: 0.217401, disparity: 0.064656\n",
            "(24, 28): reprojection: 0.417177, disparity: 0.086340\n",
            "(24, 32): reprojection: 0.642873, disparity: 0.114679\n",
            "(24, 40): reprojection: 1.062666, disparity: 0.167200\n",
            "(25, 26): reprojection: 0.156616, disparity: 0.060830\n",
            "(25, 27): reprojection: 0.247092, disparity: 0.067986\n",
            "(26, 27): reprojection: 0.148981, disparity: 0.064513\n",
            "(26, 28): reprojection: 0.248348, disparity: 0.073912\n",
            "(26, 30): reprojection: 0.439043, disparity: 0.087430\n",
            "(27, 28): reprojection: 0.138173, disparity: 0.054578\n",
            "(27, 29): reprojection: 0.273871, disparity: 0.066538\n",
            "(28, 29): reprojection: 0.177022, disparity: 0.058920\n",
            "(28, 30): reprojection: 0.295158, disparity: 0.074026\n",
            "(28, 32): reprojection: 0.370927, disparity: 0.082769\n",
            "(28, 36): reprojection: 0.593145, disparity: 0.106770\n",
            "(29, 30): reprojection: 0.151803, disparity: 0.057919\n",
            "(29, 31): reprojection: 0.258701, disparity: 0.063642\n",
            "(30, 31): reprojection: 0.182657, disparity: 0.057117\n",
            "(30, 32): reprojection: 0.238470, disparity: 0.068889\n",
            "(30, 34): reprojection: 0.396827, disparity: 0.073805\n",
            "(31, 32): reprojection: 0.133305, disparity: 0.060996\n",
            "(31, 33): reprojection: 0.282140, disparity: 0.065426\n",
            "(32, 33): reprojection: 0.228116, disparity: 0.057420\n",
            "(32, 34): reprojection: 0.230925, disparity: 0.063037\n",
            "(32, 36): reprojection: 0.354995, disparity: 0.076092\n",
            "(32, 40): reprojection: 0.560018, disparity: 0.121319\n",
            "(32, 48): reprojection: 0.992023, disparity: 0.144258\n",
            "(32, 64): reprojection: 1.444223, disparity: 0.198449\n",
            "(33, 34): reprojection: 0.157798, disparity: 0.052423\n",
            "(33, 35): reprojection: 0.257540, disparity: 0.058701\n",
            "(34, 35): reprojection: 0.176286, disparity: 0.052400\n",
            "(34, 36): reprojection: 0.265601, disparity: 0.062033\n",
            "(34, 38): reprojection: 0.339754, disparity: 0.073389\n",
            "(35, 36): reprojection: 0.225219, disparity: 0.057235\n",
            "(35, 37): reprojection: 0.282313, disparity: 0.075224\n",
            "(36, 37): reprojection: 0.247614, disparity: 0.058520\n",
            "(36, 38): reprojection: 0.396017, disparity: 0.070347\n",
            "(36, 40): reprojection: 0.346142, disparity: 0.077019\n",
            "(36, 44): reprojection: 0.689511, disparity: 0.100718\n",
            "(37, 38): reprojection: 0.224974, disparity: 0.055129\n",
            "(37, 39): reprojection: 0.249305, disparity: 0.060816\n",
            "(38, 39): reprojection: 0.198368, disparity: 0.056563\n",
            "(38, 40): reprojection: 0.381160, disparity: 0.065201\n",
            "(38, 42): reprojection: 0.718863, disparity: 0.081450\n",
            "(39, 40): reprojection: 0.284918, disparity: 0.058278\n",
            "(39, 41): reprojection: 0.511966, disparity: 0.067541\n",
            "(40, 41): reprojection: 0.313757, disparity: 0.060554\n",
            "(40, 42): reprojection: 0.467342, disparity: 0.067210\n",
            "(40, 44): reprojection: 0.631102, disparity: 0.078566\n",
            "(40, 48): reprojection: 0.905787, disparity: 0.116372\n",
            "(40, 56): reprojection: 0.869291, disparity: 0.146631\n",
            "(41, 42): reprojection: 0.210196, disparity: 0.059894\n",
            "(41, 43): reprojection: 0.338731, disparity: 0.069816\n",
            "(42, 43): reprojection: 0.190186, disparity: 0.059401\n",
            "(42, 44): reprojection: 0.348840, disparity: 0.068845\n",
            "(42, 46): reprojection: 0.615066, disparity: 0.084632\n",
            "(43, 44): reprojection: 0.196537, disparity: 0.060808\n",
            "(43, 45): reprojection: 0.310296, disparity: 0.075007\n",
            "(44, 45): reprojection: 0.150530, disparity: 0.060501\n",
            "(44, 46): reprojection: 0.315359, disparity: 0.075359\n",
            "(44, 48): reprojection: 0.462113, disparity: 0.101202\n",
            "(44, 52): reprojection: 0.720467, disparity: 0.125449\n",
            "(45, 46): reprojection: 0.209621, disparity: 0.064935\n",
            "(45, 47): reprojection: 0.357886, disparity: 0.092632\n",
            "(46, 47): reprojection: 0.201935, disparity: 0.065474\n",
            "(46, 48): reprojection: 0.319356, disparity: 0.083974\n",
            "(46, 50): reprojection: 0.447507, disparity: 0.087138\n",
            "(47, 48): reprojection: 0.204766, disparity: 0.061706\n",
            "(47, 49): reprojection: 0.264144, disparity: 0.069937\n",
            "(48, 49): reprojection: 0.168904, disparity: 0.066822\n",
            "(48, 50): reprojection: 0.232017, disparity: 0.072881\n",
            "(48, 52): reprojection: 0.420419, disparity: 0.086928\n",
            "(48, 56): reprojection: 0.734625, disparity: 0.111985\n",
            "(48, 64): reprojection: 1.124554, disparity: 0.138667\n",
            "(48, 80): reprojection: 1.190590, disparity: 0.119669\n",
            "(49, 50): reprojection: 0.135439, disparity: 0.062744\n",
            "(49, 51): reprojection: 0.240316, disparity: 0.071066\n",
            "(50, 51): reprojection: 0.154847, disparity: 0.064364\n",
            "(50, 52): reprojection: 0.280453, disparity: 0.071349\n",
            "(50, 54): reprojection: 0.479107, disparity: 0.082631\n",
            "(51, 52): reprojection: 0.188012, disparity: 0.061804\n",
            "(51, 53): reprojection: 0.355025, disparity: 0.070159\n",
            "(52, 53): reprojection: 0.189647, disparity: 0.062214\n",
            "(52, 54): reprojection: 0.298252, disparity: 0.075294\n",
            "(52, 56): reprojection: 0.578658, disparity: 0.081875\n",
            "(52, 60): reprojection: 0.729551, disparity: 0.105092\n",
            "(53, 54): reprojection: 0.140815, disparity: 0.062483\n",
            "(53, 55): reprojection: 0.355984, disparity: 0.078309\n",
            "(54, 55): reprojection: 0.322477, disparity: 0.064239\n",
            "(54, 56): reprojection: 0.562273, disparity: 0.068890\n",
            "(54, 58): reprojection: 0.434890, disparity: 0.088225\n",
            "(55, 56): reprojection: 0.346446, disparity: 0.068888\n",
            "(55, 57): reprojection: 0.371156, disparity: 0.075208\n",
            "(56, 57): reprojection: 0.318711, disparity: 0.072837\n",
            "(56, 58): reprojection: 0.596434, disparity: 0.092828\n",
            "(56, 60): reprojection: 0.736921, disparity: 0.096158\n",
            "(56, 64): reprojection: 0.943993, disparity: 0.124886\n",
            "(56, 72): reprojection: 1.043813, disparity: 0.130364\n",
            "(57, 58): reprojection: 0.373234, disparity: 0.072324\n",
            "(57, 59): reprojection: 0.523778, disparity: 0.081537\n",
            "(58, 59): reprojection: 0.283710, disparity: 0.074078\n",
            "(58, 60): reprojection: 0.418777, disparity: 0.089119\n",
            "(58, 62): reprojection: 0.552040, disparity: 0.100920\n",
            "(59, 60): reprojection: 0.199059, disparity: 0.074028\n",
            "(59, 61): reprojection: 0.348073, disparity: 0.088235\n",
            "(60, 61): reprojection: 0.232610, disparity: 0.076487\n",
            "(60, 62): reprojection: 0.342158, disparity: 0.082802\n",
            "(60, 64): reprojection: 0.603020, disparity: 0.092944\n",
            "(60, 68): reprojection: 0.784022, disparity: 0.127048\n",
            "(61, 62): reprojection: 0.255983, disparity: 0.075899\n",
            "(61, 63): reprojection: 0.347042, disparity: 0.077896\n",
            "(62, 63): reprojection: 0.251208, disparity: 0.071972\n",
            "(62, 64): reprojection: 0.388619, disparity: 0.079338\n",
            "(62, 66): reprojection: 0.588296, disparity: 0.110757\n",
            "(63, 64): reprojection: 0.244755, disparity: 0.080509\n",
            "(63, 65): reprojection: 0.337132, disparity: 0.077394\n",
            "(64, 65): reprojection: 0.218847, disparity: 0.077400\n",
            "(64, 66): reprojection: 0.370472, disparity: 0.075645\n",
            "(64, 68): reprojection: 0.579679, disparity: 0.100270\n",
            "(64, 72): reprojection: 1.040916, disparity: 0.137498\n",
            "(64, 80): reprojection: 1.167061, disparity: 0.132251\n",
            "(65, 66): reprojection: 0.317980, disparity: 0.069620\n",
            "(65, 67): reprojection: 0.481762, disparity: 0.076688\n",
            "(66, 67): reprojection: 0.274365, disparity: 0.075528\n",
            "(66, 68): reprojection: 0.411365, disparity: 0.074855\n",
            "(66, 70): reprojection: 0.554320, disparity: 0.085932\n",
            "(67, 68): reprojection: 0.363592, disparity: 0.067997\n",
            "(67, 69): reprojection: 0.408303, disparity: 0.070184\n",
            "(68, 69): reprojection: 0.220783, disparity: 0.066660\n",
            "(68, 70): reprojection: 0.502327, disparity: 0.071701\n",
            "(68, 72): reprojection: 0.577691, disparity: 0.083472\n",
            "(68, 76): reprojection: 0.833411, disparity: 0.104820\n",
            "(69, 70): reprojection: 0.339901, disparity: 0.065496\n",
            "(69, 71): reprojection: 0.525603, disparity: 0.068644\n",
            "(70, 71): reprojection: 0.227717, disparity: 0.065478\n",
            "(70, 72): reprojection: 0.313130, disparity: 0.072278\n",
            "(70, 74): reprojection: 0.425008, disparity: 0.081535\n",
            "(71, 72): reprojection: 0.252086, disparity: 0.067002\n",
            "(71, 73): reprojection: 0.398906, disparity: 0.074152\n",
            "(72, 73): reprojection: 0.253052, disparity: 0.070457\n",
            "(72, 74): reprojection: 0.318812, disparity: 0.076334\n",
            "(72, 76): reprojection: 0.536255, disparity: 0.093409\n",
            "(72, 80): reprojection: 0.559757, disparity: 0.121641\n",
            "(72, 88): reprojection: 1.167462, disparity: 0.128999\n",
            "(73, 74): reprojection: 0.328457, disparity: 0.074153\n",
            "(73, 75): reprojection: 0.433654, disparity: 0.077974\n",
            "(74, 75): reprojection: 0.232074, disparity: 0.067260\n",
            "(74, 76): reprojection: 0.255736, disparity: 0.068843\n",
            "(74, 78): reprojection: 0.391120, disparity: 0.085944\n",
            "(75, 76): reprojection: 0.240785, disparity: 0.065417\n",
            "(75, 77): reprojection: 0.376525, disparity: 0.073225\n",
            "(76, 77): reprojection: 0.225826, disparity: 0.063395\n",
            "(76, 78): reprojection: 0.339636, disparity: 0.069165\n",
            "(76, 80): reprojection: 0.417244, disparity: 0.083133\n",
            "(76, 84): reprojection: 0.657214, disparity: 0.101591\n",
            "(77, 78): reprojection: 0.229872, disparity: 0.063173\n",
            "(77, 79): reprojection: 0.303549, disparity: 0.071179\n",
            "(78, 79): reprojection: 0.204507, disparity: 0.067413\n",
            "(78, 80): reprojection: 0.284932, disparity: 0.073970\n",
            "(78, 82): reprojection: 0.490740, disparity: 0.087320\n",
            "(79, 80): reprojection: 0.154161, disparity: 0.063603\n",
            "(79, 81): reprojection: 0.259480, disparity: 0.071290\n",
            "(80, 81): reprojection: 0.155696, disparity: 0.062600\n",
            "(80, 82): reprojection: 0.273768, disparity: 0.072158\n",
            "(80, 84): reprojection: 0.498847, disparity: 0.075907\n",
            "(80, 88): reprojection: 0.794932, disparity: 0.089421\n",
            "(81, 82): reprojection: 0.195255, disparity: 0.063964\n",
            "(81, 83): reprojection: 0.335918, disparity: 0.068568\n",
            "(82, 83): reprojection: 0.175814, disparity: 0.062371\n",
            "(82, 84): reprojection: 0.262014, disparity: 0.067822\n",
            "(82, 86): reprojection: 0.560801, disparity: 0.082529\n",
            "(83, 84): reprojection: 0.150497, disparity: 0.060596\n",
            "(83, 85): reprojection: 0.322087, disparity: 0.074012\n",
            "(84, 85): reprojection: 0.178521, disparity: 0.059917\n",
            "(84, 86): reprojection: 0.302277, disparity: 0.068568\n",
            "(84, 88): reprojection: 0.535352, disparity: 0.072482\n",
            "(85, 86): reprojection: 0.178191, disparity: 0.057682\n",
            "(85, 87): reprojection: 0.313921, disparity: 0.066947\n",
            "(86, 87): reprojection: 0.252298, disparity: 0.061304\n",
            "(86, 88): reprojection: 0.347840, disparity: 0.068499\n",
            "(86, 90): reprojection: 0.488658, disparity: 0.078162\n",
            "(87, 88): reprojection: 0.181329, disparity: 0.059156\n",
            "(87, 89): reprojection: 0.275237, disparity: 0.066865\n",
            "(88, 89): reprojection: 0.134801, disparity: 0.056377\n",
            "(88, 90): reprojection: 0.305895, disparity: 0.062580\n",
            "(89, 90): reprojection: 0.215493, disparity: 0.058213\n",
            "(89, 91): reprojection: 0.441709, disparity: 0.068888\n",
            "(90, 91): reprojection: 0.348254, disparity: 0.059848\n",
            "Mean:     reprojection: 0.348254, disparity: 0.059848\n",
            "Done Validation for epoch 12 (3120 iterations)\n",
            "Epoch = 12, pairs = [[48, 52], [0, 16], [30, 31], [80, 84]], loss = 0.5324078798294067\n",
            "Epoch = 12, pairs = [[32, 36], [32, 64], [64, 65], [63, 65]], loss = 0.6833176612854004\n",
            "Epoch = 12, pairs = [[75, 77], [54, 58], [39, 41], [82, 86]], loss = 0.480812132358551\n",
            "Epoch = 12, pairs = [[62, 63], [8, 24], [23, 24], [65, 67]], loss = 0.4827515482902527\n",
            "Epoch = 12, pairs = [[18, 22], [22, 26], [71, 73], [28, 30]], loss = 0.4825066924095154\n",
            "Epoch = 12, pairs = [[64, 66], [79, 80], [36, 37], [37, 38]], loss = 0.33323168754577637\n",
            "Epoch = 12, pairs = [[49, 51], [54, 55], [14, 16], [47, 49]], loss = 0.4116150736808777\n",
            "Epoch = 12, pairs = [[10, 14], [1, 2], [73, 74], [86, 87]], loss = 0.32911819219589233\n",
            "Epoch = 12, pairs = [[78, 82], [40, 42], [68, 69], [20, 21]], loss = 0.36356720328330994\n",
            "Epoch = 12, pairs = [[57, 59], [72, 88], [48, 64], [67, 69]], loss = 0.7516320943832397\n",
            "Epoch = 12, pairs = [[29, 31], [66, 68], [2, 4], [12, 14]], loss = 0.3248986303806305\n",
            "Epoch = 12, pairs = [[16, 17], [84, 85], [6, 8], [87, 88]], loss = 0.2119135558605194\n",
            "Epoch = 12, pairs = [[38, 39], [36, 38], [54, 56], [74, 76]], loss = 0.41846373677253723\n",
            "Epoch = 12, pairs = [[10, 12], [20, 22], [68, 76], [48, 80]], loss = 1.0928024053573608\n",
            "Epoch = 12, pairs = [[40, 56], [28, 32], [25, 27], [59, 61]], loss = 0.47502607107162476\n",
            "Epoch = 12, pairs = [[49, 50], [8, 9], [13, 14], [16, 18]], loss = 0.25494176149368286\n",
            "Epoch = 12, pairs = [[76, 80], [58, 60], [64, 72], [27, 28]], loss = 0.5895256400108337\n",
            "Epoch = 12, pairs = [[30, 34], [15, 16], [17, 19], [9, 10]], loss = 0.48615318536758423\n",
            "Epoch = 12, pairs = [[21, 23], [74, 75], [6, 7], [78, 79]], loss = 0.2755342721939087\n",
            "Epoch = 12, pairs = [[44, 48], [88, 89], [29, 30], [50, 54]], loss = 0.41598063707351685\n",
            "Epoch = 12, pairs = [[80, 82], [76, 78], [4, 12], [42, 44]], loss = 0.46750110387802124\n",
            "Epoch = 12, pairs = [[85, 86], [53, 54], [63, 64], [35, 36]], loss = 0.2643459439277649\n",
            "Epoch = 12, pairs = [[16, 48], [50, 51], [31, 32], [20, 24]], loss = 1.3694078922271729\n",
            "Epoch = 12, pairs = [[4, 6], [68, 72], [80, 88], [36, 44]], loss = 0.7608927488327026\n",
            "Epoch = 12, pairs = [[43, 44], [51, 52], [81, 82], [79, 81]], loss = 0.2838435173034668\n",
            "Epoch = 12, pairs = [[22, 23], [65, 66], [80, 81], [52, 53]], loss = 0.26589298248291016\n",
            "Epoch = 12, pairs = [[43, 45], [31, 33], [9, 11], [70, 74]], loss = 0.4587833881378174\n",
            "Epoch = 12, pairs = [[88, 90], [45, 47], [0, 2], [66, 70]], loss = 0.5038754940032959\n",
            "Epoch = 12, pairs = [[19, 20], [58, 59], [60, 61], [72, 76]], loss = 0.3662033677101135\n",
            "Epoch = 12, pairs = [[60, 64], [84, 88], [56, 58], [32, 33]], loss = 0.5747882723808289\n",
            "Epoch = 12, pairs = [[18, 20], [40, 44], [44, 45], [25, 26]], loss = 0.3663020431995392\n",
            "Epoch = 12, pairs = [[16, 24], [38, 42], [62, 64], [69, 70]], loss = 0.6677684783935547\n",
            "Epoch = 12, pairs = [[26, 27], [83, 85], [77, 78], [81, 83]], loss = 0.31165605783462524\n",
            "Epoch = 12, pairs = [[2, 3], [45, 46], [27, 29], [76, 84]], loss = 0.47810032963752747\n",
            "Epoch = 12, pairs = [[89, 90], [0, 32], [30, 32], [66, 67]], loss = 0.919879138469696\n",
            "Epoch = 12, pairs = [[28, 29], [44, 46], [53, 55], [55, 57]], loss = 0.38604891300201416\n",
            "Epoch = 12, pairs = [[61, 62], [46, 50], [2, 6], [11, 12]], loss = 0.3569517433643341\n",
            "Epoch = 12, pairs = [[57, 58], [32, 34], [14, 15], [12, 16]], loss = 0.3778906464576721\n",
            "Epoch = 12, pairs = [[41, 42], [40, 41], [5, 7], [86, 88]], loss = 0.31897616386413574\n",
            "Epoch = 12, pairs = [[23, 25], [12, 13], [1, 3], [4, 5]], loss = 0.2544865310192108\n",
            "Epoch = 12, pairs = [[42, 46], [78, 80], [48, 50], [48, 49]], loss = 0.4475298821926117\n",
            "Epoch = 12, pairs = [[67, 68], [56, 57], [60, 62], [42, 43]], loss = 0.419914573431015\n",
            "Epoch = 12, pairs = [[38, 40], [60, 68], [24, 26], [32, 48]], loss = 0.9643784761428833\n",
            "Epoch = 12, pairs = [[8, 16], [87, 89], [21, 22], [3, 5]], loss = 0.4152902364730835\n",
            "Epoch = 12, pairs = [[82, 83], [47, 48], [4, 8], [41, 43]], loss = 0.29338428378105164\n",
            "Epoch = 12, pairs = [[64, 68], [10, 11], [19, 21], [11, 13]], loss = 0.3727816641330719\n",
            "Epoch = 12, pairs = [[18, 19], [37, 39], [34, 38], [34, 35]], loss = 0.3992690145969391\n",
            "Epoch = 12, pairs = [[36, 40], [8, 12], [72, 74], [0, 4]], loss = 0.5365290641784668\n",
            "Epoch = 12, pairs = [[58, 62], [46, 48], [15, 17], [56, 60]], loss = 0.6834162473678589\n",
            "Epoch = 12, pairs = [[13, 15], [52, 60], [0, 8], [61, 63]], loss = 0.6836744546890259\n",
            "Epoch = 12, pairs = [[56, 72], [77, 79], [24, 40], [32, 40]], loss = 1.3298184871673584\n",
            "Epoch = 12, pairs = [[17, 18], [44, 52], [24, 25], [68, 70]], loss = 0.4365553855895996\n",
            "Epoch = 12, pairs = [[70, 72], [26, 28], [39, 40], [24, 32]], loss = 0.4269988238811493\n",
            "Epoch = 12, pairs = [[84, 86], [6, 10], [64, 80], [89, 91]], loss = 1.045502781867981\n",
            "Epoch = 12, pairs = [[35, 37], [52, 56], [76, 77], [24, 28]], loss = 0.5485725402832031\n",
            "Epoch = 12, pairs = [[33, 35], [72, 73], [86, 90], [82, 84]], loss = 0.4468572735786438\n",
            "Epoch = 12, pairs = [[7, 9], [55, 56], [73, 75], [74, 78]], loss = 0.46843844652175903\n",
            "Epoch = 12, pairs = [[56, 64], [7, 8], [48, 56], [72, 80]], loss = 0.8489222526550293\n",
            "Epoch = 12, pairs = [[16, 32], [5, 6], [26, 30], [52, 54]], loss = 0.5977113246917725\n",
            "Epoch = 12, pairs = [[85, 87], [75, 76], [90, 91], [70, 71]], loss = 0.3467026352882385\n",
            "Epoch = 12, pairs = [[46, 47], [16, 20], [12, 20], [20, 28]], loss = 0.726695716381073\n",
            "Epoch = 12, pairs = [[33, 34], [62, 66], [22, 24], [28, 36]], loss = 0.5828658938407898\n",
            "Epoch = 12, pairs = [[50, 52], [71, 72], [3, 4], [83, 84]], loss = 0.254946231842041\n",
            "Epoch = 12, pairs = [[14, 18], [51, 53], [69, 71], [8, 10]], loss = 0.4338817000389099\n",
            "Epoch = 12, pairs = [[0, 1], [40, 48], [34, 36], [59, 60]], loss = 0.47955048084259033\n",
            "Epoch 12 took 84.41s.\n",
            "( 0,  1): reprojection: 0.234836, disparity: 0.065642\n",
            "( 0,  2): reprojection: 0.351420, disparity: 0.067638\n",
            "( 0,  4): reprojection: 0.410084, disparity: 0.080738\n",
            "( 0,  8): reprojection: 0.574408, disparity: 0.081757\n",
            "( 0, 16): reprojection: 1.023782, disparity: 0.086154\n",
            "( 0, 32): reprojection: 1.727799, disparity: 0.132768\n",
            "( 1,  2): reprojection: 0.198644, disparity: 0.044259\n",
            "( 1,  3): reprojection: 0.258474, disparity: 0.048348\n",
            "( 2,  3): reprojection: 0.112068, disparity: 0.044924\n",
            "( 2,  4): reprojection: 0.190167, disparity: 0.052811\n",
            "( 2,  6): reprojection: 0.366314, disparity: 0.062947\n",
            "( 3,  4): reprojection: 0.110408, disparity: 0.045957\n",
            "( 3,  5): reprojection: 0.210205, disparity: 0.052426\n",
            "( 4,  5): reprojection: 0.156346, disparity: 0.046127\n",
            "( 4,  6): reprojection: 0.226256, disparity: 0.050021\n",
            "( 4,  8): reprojection: 0.319637, disparity: 0.061841\n",
            "( 4, 12): reprojection: 0.408539, disparity: 0.079989\n",
            "( 5,  6): reprojection: 0.116021, disparity: 0.044821\n",
            "( 5,  7): reprojection: 0.170868, disparity: 0.049671\n",
            "( 6,  7): reprojection: 0.105235, disparity: 0.044666\n",
            "( 6,  8): reprojection: 0.184208, disparity: 0.050480\n",
            "( 6, 10): reprojection: 0.398308, disparity: 0.064113\n",
            "( 7,  8): reprojection: 0.094536, disparity: 0.045118\n",
            "( 7,  9): reprojection: 0.199275, disparity: 0.052082\n",
            "( 8,  9): reprojection: 0.156582, disparity: 0.046537\n",
            "( 8, 10): reprojection: 0.349369, disparity: 0.053191\n",
            "( 8, 12): reprojection: 0.493456, disparity: 0.064759\n",
            "( 8, 16): reprojection: 0.650848, disparity: 0.079955\n",
            "( 8, 24): reprojection: 1.013835, disparity: 0.104977\n",
            "( 9, 10): reprojection: 0.240889, disparity: 0.046815\n",
            "( 9, 11): reprojection: 0.351291, disparity: 0.055113\n",
            "(10, 11): reprojection: 0.148429, disparity: 0.044608\n",
            "(10, 12): reprojection: 0.198955, disparity: 0.052311\n",
            "(10, 14): reprojection: 0.364157, disparity: 0.069863\n",
            "(11, 12): reprojection: 0.142093, disparity: 0.043271\n",
            "(11, 13): reprojection: 0.238097, disparity: 0.052742\n",
            "(12, 13): reprojection: 0.135302, disparity: 0.043854\n",
            "(12, 14): reprojection: 0.262642, disparity: 0.053681\n",
            "(12, 16): reprojection: 0.498513, disparity: 0.073560\n",
            "(12, 20): reprojection: 0.794046, disparity: 0.078726\n",
            "(13, 14): reprojection: 0.180328, disparity: 0.046475\n",
            "(13, 15): reprojection: 0.258614, disparity: 0.054934\n",
            "(14, 15): reprojection: 0.200751, disparity: 0.046988\n",
            "(14, 16): reprojection: 0.446108, disparity: 0.055671\n",
            "(14, 18): reprojection: 0.426800, disparity: 0.067922\n",
            "(15, 16): reprojection: 0.300543, disparity: 0.047154\n",
            "(15, 17): reprojection: 0.395594, disparity: 0.056834\n",
            "(16, 17): reprojection: 0.145215, disparity: 0.047377\n",
            "(16, 18): reprojection: 0.319619, disparity: 0.058553\n",
            "(16, 20): reprojection: 0.601226, disparity: 0.072248\n",
            "(16, 24): reprojection: 0.898682, disparity: 0.091564\n",
            "(16, 32): reprojection: 1.512790, disparity: 0.132141\n",
            "(16, 48): reprojection: 1.510344, disparity: 0.269924\n",
            "(17, 18): reprojection: 0.293134, disparity: 0.051182\n",
            "(17, 19): reprojection: 0.474113, disparity: 0.061918\n",
            "(18, 19): reprojection: 0.221235, disparity: 0.048162\n",
            "(18, 20): reprojection: 0.328776, disparity: 0.057180\n",
            "(18, 22): reprojection: 0.544456, disparity: 0.065165\n",
            "(19, 20): reprojection: 0.133808, disparity: 0.048236\n",
            "(19, 21): reprojection: 0.281391, disparity: 0.056429\n",
            "(20, 21): reprojection: 0.182574, disparity: 0.049377\n",
            "(20, 22): reprojection: 0.285109, disparity: 0.057027\n",
            "(20, 24): reprojection: 0.449741, disparity: 0.069373\n",
            "(20, 28): reprojection: 0.824398, disparity: 0.096113\n",
            "(21, 22): reprojection: 0.157646, disparity: 0.048663\n",
            "(21, 23): reprojection: 0.256869, disparity: 0.058304\n",
            "(22, 23): reprojection: 0.130181, disparity: 0.049496\n",
            "(22, 24): reprojection: 0.230592, disparity: 0.061316\n",
            "(22, 26): reprojection: 0.460144, disparity: 0.078647\n",
            "(23, 24): reprojection: 0.131737, disparity: 0.050556\n",
            "(23, 25): reprojection: 0.220259, disparity: 0.060422\n",
            "(24, 25): reprojection: 0.132110, disparity: 0.051859\n",
            "(24, 26): reprojection: 0.225259, disparity: 0.060292\n",
            "(24, 28): reprojection: 0.436726, disparity: 0.081994\n",
            "(24, 32): reprojection: 0.868594, disparity: 0.109794\n",
            "(24, 40): reprojection: 1.468311, disparity: 0.154609\n",
            "(25, 26): reprojection: 0.149602, disparity: 0.054897\n",
            "(25, 27): reprojection: 0.279310, disparity: 0.062291\n",
            "(26, 27): reprojection: 0.155391, disparity: 0.053481\n",
            "(26, 28): reprojection: 0.223826, disparity: 0.068077\n",
            "(26, 30): reprojection: 0.412660, disparity: 0.083517\n",
            "(27, 28): reprojection: 0.143595, disparity: 0.052227\n",
            "(27, 29): reprojection: 0.275272, disparity: 0.064826\n",
            "(28, 29): reprojection: 0.171542, disparity: 0.055756\n",
            "(28, 30): reprojection: 0.264737, disparity: 0.068277\n",
            "(28, 32): reprojection: 0.518002, disparity: 0.078351\n",
            "(28, 36): reprojection: 0.860867, disparity: 0.099858\n",
            "(29, 30): reprojection: 0.138321, disparity: 0.053959\n",
            "(29, 31): reprojection: 0.326292, disparity: 0.060072\n",
            "(30, 31): reprojection: 0.209323, disparity: 0.052310\n",
            "(30, 32): reprojection: 0.306084, disparity: 0.062448\n",
            "(30, 34): reprojection: 0.557228, disparity: 0.067549\n",
            "(31, 32): reprojection: 0.133779, disparity: 0.053189\n",
            "(31, 33): reprojection: 0.338281, disparity: 0.057359\n",
            "(32, 33): reprojection: 0.251621, disparity: 0.051653\n",
            "(32, 34): reprojection: 0.276147, disparity: 0.055566\n",
            "(32, 36): reprojection: 0.444955, disparity: 0.070016\n",
            "(32, 40): reprojection: 0.629112, disparity: 0.096310\n",
            "(32, 48): reprojection: 1.189011, disparity: 0.133754\n",
            "(32, 64): reprojection: 1.431970, disparity: 0.248732\n",
            "(33, 34): reprojection: 0.139096, disparity: 0.049223\n",
            "(33, 35): reprojection: 0.286032, disparity: 0.054812\n",
            "(34, 35): reprojection: 0.193707, disparity: 0.049263\n",
            "(34, 36): reprojection: 0.293022, disparity: 0.060341\n",
            "(34, 38): reprojection: 0.484256, disparity: 0.068932\n",
            "(35, 36): reprojection: 0.219829, disparity: 0.054520\n",
            "(35, 37): reprojection: 0.298588, disparity: 0.067908\n",
            "(36, 37): reprojection: 0.281879, disparity: 0.055337\n",
            "(36, 38): reprojection: 0.415316, disparity: 0.065040\n",
            "(36, 40): reprojection: 0.381474, disparity: 0.070393\n",
            "(36, 44): reprojection: 0.761859, disparity: 0.096632\n",
            "(37, 38): reprojection: 0.229380, disparity: 0.052857\n",
            "(37, 39): reprojection: 0.281715, disparity: 0.058172\n",
            "(38, 39): reprojection: 0.183635, disparity: 0.052555\n",
            "(38, 40): reprojection: 0.337722, disparity: 0.062641\n",
            "(38, 42): reprojection: 0.590366, disparity: 0.079473\n",
            "(39, 40): reprojection: 0.260623, disparity: 0.055902\n",
            "(39, 41): reprojection: 0.449428, disparity: 0.065520\n",
            "(40, 41): reprojection: 0.264863, disparity: 0.056393\n",
            "(40, 42): reprojection: 0.383320, disparity: 0.064889\n",
            "(40, 44): reprojection: 0.520261, disparity: 0.084507\n",
            "(40, 48): reprojection: 0.835759, disparity: 0.106799\n",
            "(40, 56): reprojection: 1.176710, disparity: 0.144649\n",
            "(41, 42): reprojection: 0.186816, disparity: 0.056573\n",
            "(41, 43): reprojection: 0.295927, disparity: 0.070926\n",
            "(42, 43): reprojection: 0.169674, disparity: 0.058005\n",
            "(42, 44): reprojection: 0.326440, disparity: 0.071067\n",
            "(42, 46): reprojection: 0.563545, disparity: 0.087504\n",
            "(43, 44): reprojection: 0.202115, disparity: 0.057751\n",
            "(43, 45): reprojection: 0.311951, disparity: 0.074324\n",
            "(44, 45): reprojection: 0.158461, disparity: 0.058325\n",
            "(44, 46): reprojection: 0.306764, disparity: 0.072180\n",
            "(44, 48): reprojection: 0.496274, disparity: 0.092534\n",
            "(44, 52): reprojection: 0.797426, disparity: 0.113831\n",
            "(45, 46): reprojection: 0.211590, disparity: 0.059799\n",
            "(45, 47): reprojection: 0.335980, disparity: 0.078431\n",
            "(46, 47): reprojection: 0.184612, disparity: 0.062940\n",
            "(46, 48): reprojection: 0.306756, disparity: 0.078927\n",
            "(46, 50): reprojection: 0.465080, disparity: 0.090587\n",
            "(47, 48): reprojection: 0.213364, disparity: 0.060728\n",
            "(47, 49): reprojection: 0.293500, disparity: 0.068228\n",
            "(48, 49): reprojection: 0.166487, disparity: 0.063823\n",
            "(48, 50): reprojection: 0.278064, disparity: 0.074205\n",
            "(48, 52): reprojection: 0.444201, disparity: 0.085516\n",
            "(48, 56): reprojection: 0.872174, disparity: 0.109927\n",
            "(48, 64): reprojection: 1.281222, disparity: 0.142642\n",
            "(48, 80): reprojection: 1.845865, disparity: 0.121390\n",
            "(49, 50): reprojection: 0.154906, disparity: 0.061452\n",
            "(49, 51): reprojection: 0.275135, disparity: 0.073775\n",
            "(50, 51): reprojection: 0.175703, disparity: 0.061707\n",
            "(50, 52): reprojection: 0.290088, disparity: 0.071500\n",
            "(50, 54): reprojection: 0.415492, disparity: 0.076743\n",
            "(51, 52): reprojection: 0.162958, disparity: 0.059227\n",
            "(51, 53): reprojection: 0.309539, disparity: 0.064792\n",
            "(52, 53): reprojection: 0.175503, disparity: 0.058928\n",
            "(52, 54): reprojection: 0.256565, disparity: 0.067943\n",
            "(52, 56): reprojection: 0.595719, disparity: 0.077542\n",
            "(52, 60): reprojection: 0.872264, disparity: 0.090096\n",
            "(53, 54): reprojection: 0.121144, disparity: 0.058676\n",
            "(53, 55): reprojection: 0.328099, disparity: 0.072732\n",
            "(54, 55): reprojection: 0.294125, disparity: 0.060593\n",
            "(54, 56): reprojection: 0.563348, disparity: 0.069428\n",
            "(54, 58): reprojection: 0.498704, disparity: 0.076425\n",
            "(55, 56): reprojection: 0.344954, disparity: 0.065339\n",
            "(55, 57): reprojection: 0.425564, disparity: 0.067819\n",
            "(56, 57): reprojection: 0.294050, disparity: 0.062032\n",
            "(56, 58): reprojection: 0.537844, disparity: 0.072731\n",
            "(56, 60): reprojection: 0.728436, disparity: 0.102806\n",
            "(56, 64): reprojection: 1.108630, disparity: 0.101302\n",
            "(56, 72): reprojection: 1.308012, disparity: 0.122694\n",
            "(57, 58): reprojection: 0.341813, disparity: 0.065396\n",
            "(57, 59): reprojection: 0.466490, disparity: 0.076500\n",
            "(58, 59): reprojection: 0.242113, disparity: 0.070944\n",
            "(58, 60): reprojection: 0.385165, disparity: 0.080128\n",
            "(58, 62): reprojection: 0.560927, disparity: 0.092372\n",
            "(59, 60): reprojection: 0.205407, disparity: 0.069905\n",
            "(59, 61): reprojection: 0.357775, disparity: 0.082178\n",
            "(60, 61): reprojection: 0.234183, disparity: 0.072713\n",
            "(60, 62): reprojection: 0.369108, disparity: 0.079182\n",
            "(60, 64): reprojection: 0.577589, disparity: 0.091701\n",
            "(60, 68): reprojection: 0.813536, disparity: 0.110054\n",
            "(61, 62): reprojection: 0.207303, disparity: 0.071360\n",
            "(61, 63): reprojection: 0.310183, disparity: 0.074042\n",
            "(62, 63): reprojection: 0.216514, disparity: 0.066979\n",
            "(62, 64): reprojection: 0.324758, disparity: 0.076099\n",
            "(62, 66): reprojection: 0.500765, disparity: 0.102964\n",
            "(63, 64): reprojection: 0.202684, disparity: 0.071196\n",
            "(63, 65): reprojection: 0.329151, disparity: 0.072172\n",
            "(64, 65): reprojection: 0.198355, disparity: 0.070629\n",
            "(64, 66): reprojection: 0.346443, disparity: 0.072046\n",
            "(64, 68): reprojection: 0.530473, disparity: 0.097725\n",
            "(64, 72): reprojection: 0.964760, disparity: 0.110563\n",
            "(64, 80): reprojection: 1.373108, disparity: 0.099333\n",
            "(65, 66): reprojection: 0.253681, disparity: 0.065872\n",
            "(65, 67): reprojection: 0.386843, disparity: 0.072097\n",
            "(66, 67): reprojection: 0.244744, disparity: 0.070322\n",
            "(66, 68): reprojection: 0.370475, disparity: 0.072123\n",
            "(66, 70): reprojection: 0.509388, disparity: 0.080376\n",
            "(67, 68): reprojection: 0.323723, disparity: 0.065114\n",
            "(67, 69): reprojection: 0.346220, disparity: 0.069329\n",
            "(68, 69): reprojection: 0.214209, disparity: 0.062987\n",
            "(68, 70): reprojection: 0.455334, disparity: 0.070519\n",
            "(68, 72): reprojection: 0.619804, disparity: 0.076924\n",
            "(68, 76): reprojection: 0.943859, disparity: 0.090139\n",
            "(69, 70): reprojection: 0.286122, disparity: 0.061182\n",
            "(69, 71): reprojection: 0.429298, disparity: 0.066890\n",
            "(70, 71): reprojection: 0.194914, disparity: 0.061478\n",
            "(70, 72): reprojection: 0.324732, disparity: 0.067292\n",
            "(70, 74): reprojection: 0.544057, disparity: 0.076665\n",
            "(71, 72): reprojection: 0.234386, disparity: 0.062778\n",
            "(71, 73): reprojection: 0.379089, disparity: 0.070679\n",
            "(72, 73): reprojection: 0.229722, disparity: 0.064056\n",
            "(72, 74): reprojection: 0.333480, disparity: 0.068640\n",
            "(72, 76): reprojection: 0.539300, disparity: 0.082831\n",
            "(72, 80): reprojection: 0.857164, disparity: 0.101067\n",
            "(72, 88): reprojection: 1.146915, disparity: 0.098577\n",
            "(73, 74): reprojection: 0.315714, disparity: 0.065386\n",
            "(73, 75): reprojection: 0.447725, disparity: 0.069118\n",
            "(74, 75): reprojection: 0.244111, disparity: 0.062036\n",
            "(74, 76): reprojection: 0.302587, disparity: 0.067529\n",
            "(74, 78): reprojection: 0.455937, disparity: 0.078931\n",
            "(75, 76): reprojection: 0.214509, disparity: 0.061815\n",
            "(75, 77): reprojection: 0.340908, disparity: 0.071330\n",
            "(76, 77): reprojection: 0.218942, disparity: 0.059351\n",
            "(76, 78): reprojection: 0.345779, disparity: 0.064096\n",
            "(76, 80): reprojection: 0.499070, disparity: 0.076636\n",
            "(76, 84): reprojection: 0.840357, disparity: 0.104971\n",
            "(77, 78): reprojection: 0.224895, disparity: 0.058582\n",
            "(77, 79): reprojection: 0.280127, disparity: 0.065301\n",
            "(78, 79): reprojection: 0.213182, disparity: 0.060713\n",
            "(78, 80): reprojection: 0.283307, disparity: 0.066003\n",
            "(78, 82): reprojection: 0.468132, disparity: 0.081372\n",
            "(79, 80): reprojection: 0.152743, disparity: 0.057050\n",
            "(79, 81): reprojection: 0.262922, disparity: 0.064981\n",
            "(80, 81): reprojection: 0.150537, disparity: 0.058348\n",
            "(80, 82): reprojection: 0.291963, disparity: 0.070907\n",
            "(80, 84): reprojection: 0.535653, disparity: 0.084672\n",
            "(80, 88): reprojection: 0.830007, disparity: 0.094348\n",
            "(81, 82): reprojection: 0.199838, disparity: 0.059185\n",
            "(81, 83): reprojection: 0.317974, disparity: 0.066407\n",
            "(82, 83): reprojection: 0.152630, disparity: 0.056686\n",
            "(82, 84): reprojection: 0.292363, disparity: 0.063626\n",
            "(82, 86): reprojection: 0.490958, disparity: 0.075664\n",
            "(83, 84): reprojection: 0.160030, disparity: 0.055712\n",
            "(83, 85): reprojection: 0.322766, disparity: 0.065873\n",
            "(84, 85): reprojection: 0.178434, disparity: 0.054455\n",
            "(84, 86): reprojection: 0.255715, disparity: 0.062234\n",
            "(84, 88): reprojection: 0.488053, disparity: 0.067839\n",
            "(85, 86): reprojection: 0.158379, disparity: 0.053940\n",
            "(85, 87): reprojection: 0.276957, disparity: 0.060363\n",
            "(86, 87): reprojection: 0.230459, disparity: 0.055987\n",
            "(86, 88): reprojection: 0.320622, disparity: 0.065956\n",
            "(86, 90): reprojection: 0.516588, disparity: 0.071410\n",
            "(87, 88): reprojection: 0.168284, disparity: 0.055109\n",
            "(87, 89): reprojection: 0.285165, disparity: 0.061192\n",
            "(88, 89): reprojection: 0.149050, disparity: 0.051571\n",
            "(88, 90): reprojection: 0.319698, disparity: 0.057567\n",
            "(89, 90): reprojection: 0.214366, disparity: 0.051610\n",
            "(89, 91): reprojection: 0.400530, disparity: 0.062872\n",
            "(90, 91): reprojection: 0.312811, disparity: 0.058041\n",
            "Mean:     reprojection: 0.312811, disparity: 0.058041\n",
            "Done Validation for epoch 13 (3380 iterations)\n",
            "Epoch = 13, pairs = [[39, 40], [72, 74], [38, 40], [24, 28]], loss = 0.40813392400741577\n",
            "Epoch = 13, pairs = [[6, 7], [78, 79], [63, 65], [41, 42]], loss = 0.2657071650028229\n",
            "Epoch = 13, pairs = [[16, 24], [2, 4], [16, 48], [16, 17]], loss = 0.8045185804367065\n",
            "Epoch = 13, pairs = [[76, 77], [27, 29], [35, 37], [68, 69]], loss = 0.3168829381465912\n",
            "Epoch = 13, pairs = [[25, 27], [40, 56], [52, 54], [15, 16]], loss = 0.47208207845687866\n",
            "Epoch = 13, pairs = [[51, 53], [32, 48], [57, 58], [46, 47]], loss = 0.522391140460968\n",
            "Epoch = 13, pairs = [[76, 84], [26, 28], [85, 87], [22, 24]], loss = 0.3433547616004944\n",
            "Epoch = 13, pairs = [[76, 80], [70, 72], [61, 62], [11, 12]], loss = 0.30671828985214233\n",
            "Epoch = 13, pairs = [[5, 7], [76, 78], [77, 78], [30, 31]], loss = 0.2805435359477997\n",
            "Epoch = 13, pairs = [[40, 48], [21, 23], [9, 11], [32, 40]], loss = 0.6567055583000183\n",
            "Epoch = 13, pairs = [[10, 11], [74, 78], [48, 49], [52, 53]], loss = 0.2982322871685028\n",
            "Epoch = 13, pairs = [[24, 25], [72, 88], [32, 34], [18, 20]], loss = 0.4995259642601013\n",
            "Epoch = 13, pairs = [[39, 41], [30, 34], [21, 22], [12, 14]], loss = 0.4324735999107361\n",
            "Epoch = 13, pairs = [[48, 50], [59, 61], [20, 28], [89, 91]], loss = 0.42902955412864685\n",
            "Epoch = 13, pairs = [[28, 32], [75, 77], [4, 5], [52, 60]], loss = 0.45530036091804504\n",
            "Epoch = 13, pairs = [[73, 75], [22, 26], [23, 25], [17, 18]], loss = 0.3660447597503662\n",
            "Epoch = 13, pairs = [[1, 3], [20, 24], [48, 52], [79, 81]], loss = 0.36416545510292053\n",
            "Epoch = 13, pairs = [[71, 73], [0, 2], [32, 33], [15, 17]], loss = 0.37948185205459595\n",
            "Epoch = 13, pairs = [[24, 26], [37, 39], [18, 22], [88, 89]], loss = 0.3239564895629883\n",
            "Epoch = 13, pairs = [[0, 4], [13, 15], [66, 70], [3, 4]], loss = 0.35229402780532837\n",
            "Epoch = 13, pairs = [[82, 83], [89, 90], [54, 58], [29, 31]], loss = 0.3187003433704376\n",
            "Epoch = 13, pairs = [[12, 20], [19, 20], [31, 32], [43, 45]], loss = 0.3601694107055664\n",
            "Epoch = 13, pairs = [[28, 36], [28, 30], [4, 8], [60, 61]], loss = 0.4295578896999359\n",
            "Epoch = 13, pairs = [[46, 50], [75, 76], [44, 48], [63, 64]], loss = 0.3847936689853668\n",
            "Epoch = 13, pairs = [[80, 82], [60, 68], [48, 80], [64, 68]], loss = 0.8677660226821899\n",
            "Epoch = 13, pairs = [[72, 80], [49, 50], [65, 66], [79, 80]], loss = 0.3593469262123108\n",
            "Epoch = 13, pairs = [[16, 32], [56, 58], [64, 65], [36, 44]], loss = 0.8007022142410278\n",
            "Epoch = 13, pairs = [[0, 1], [44, 45], [73, 74], [40, 42]], loss = 0.3612297773361206\n",
            "Epoch = 13, pairs = [[80, 84], [2, 6], [78, 80], [36, 38]], loss = 0.4751798212528229\n",
            "Epoch = 13, pairs = [[24, 40], [24, 32], [0, 32], [87, 88]], loss = 1.0662503242492676\n",
            "Epoch = 13, pairs = [[81, 83], [60, 62], [10, 12], [83, 84]], loss = 0.32717663049697876\n",
            "Epoch = 13, pairs = [[69, 70], [26, 27], [56, 64], [37, 38]], loss = 0.48282694816589355\n",
            "Epoch = 13, pairs = [[53, 55], [2, 3], [47, 49], [14, 16]], loss = 0.3726845979690552\n",
            "Epoch = 13, pairs = [[29, 30], [14, 15], [33, 35], [86, 88]], loss = 0.3072737157344818\n",
            "Epoch = 13, pairs = [[55, 57], [84, 85], [58, 59], [16, 18]], loss = 0.37363508343696594\n",
            "Epoch = 13, pairs = [[50, 51], [46, 48], [78, 82], [19, 21]], loss = 0.43136852979660034\n",
            "Epoch = 13, pairs = [[65, 67], [4, 12], [72, 76], [54, 55]], loss = 0.5455479025840759\n",
            "Epoch = 13, pairs = [[80, 88], [7, 9], [61, 63], [70, 74]], loss = 0.5729519724845886\n",
            "Epoch = 13, pairs = [[10, 14], [74, 76], [43, 44], [20, 22]], loss = 0.3775453567504883\n",
            "Epoch = 13, pairs = [[86, 90], [4, 6], [42, 43], [45, 47]], loss = 0.38540148735046387\n",
            "Epoch = 13, pairs = [[34, 38], [82, 84], [67, 69], [71, 72]], loss = 0.4103853106498718\n",
            "Epoch = 13, pairs = [[81, 82], [36, 40], [49, 51], [22, 23]], loss = 0.30814993381500244\n",
            "Epoch = 13, pairs = [[8, 10], [6, 8], [90, 91], [7, 8]], loss = 0.30058062076568604\n",
            "Epoch = 13, pairs = [[68, 72], [27, 28], [0, 8], [64, 72]], loss = 0.6081453561782837\n",
            "Epoch = 13, pairs = [[14, 18], [38, 39], [40, 41], [31, 33]], loss = 0.3528708219528198\n",
            "Epoch = 13, pairs = [[36, 37], [86, 87], [82, 86], [72, 73]], loss = 0.36160868406295776\n",
            "Epoch = 13, pairs = [[23, 24], [44, 46], [56, 60], [83, 85]], loss = 0.41454291343688965\n",
            "Epoch = 13, pairs = [[62, 63], [88, 90], [59, 60], [16, 20]], loss = 0.38637763261795044\n",
            "Epoch = 13, pairs = [[68, 76], [34, 35], [57, 59], [45, 46]], loss = 0.4565187096595764\n",
            "Epoch = 13, pairs = [[9, 10], [20, 21], [13, 14], [56, 57]], loss = 0.29070690274238586\n",
            "Epoch = 13, pairs = [[48, 64], [8, 24], [69, 71], [8, 9]], loss = 0.673861026763916\n",
            "Epoch = 13, pairs = [[32, 36], [8, 16], [80, 81], [12, 16]], loss = 0.46568289399147034\n",
            "Epoch = 13, pairs = [[77, 79], [25, 26], [12, 13], [48, 56]], loss = 0.36301425099372864\n",
            "Epoch = 13, pairs = [[51, 52], [58, 60], [33, 34], [38, 42]], loss = 0.405728280544281\n",
            "Epoch = 13, pairs = [[0, 16], [34, 36], [56, 72], [74, 75]], loss = 0.657050371170044\n",
            "Epoch = 13, pairs = [[42, 46], [26, 30], [6, 10], [47, 48]], loss = 0.42430198192596436\n",
            "Epoch = 13, pairs = [[30, 32], [52, 56], [11, 13], [67, 68]], loss = 0.38801687955856323\n",
            "Epoch = 13, pairs = [[55, 56], [32, 64], [44, 52], [5, 6]], loss = 0.6137481927871704\n",
            "Epoch = 13, pairs = [[84, 86], [68, 70], [64, 66], [3, 5]], loss = 0.38625916838645935\n",
            "Epoch = 13, pairs = [[64, 80], [35, 36], [87, 89], [62, 64]], loss = 0.5834776163101196\n",
            "Epoch = 13, pairs = [[28, 29], [62, 66], [85, 86], [60, 64]], loss = 0.4157339334487915\n",
            "Epoch = 13, pairs = [[50, 52], [84, 88], [8, 12], [1, 2]], loss = 0.4242464303970337\n",
            "Epoch = 13, pairs = [[40, 44], [54, 56], [17, 19], [42, 44]], loss = 0.5765153169631958\n",
            "Epoch = 13, pairs = [[50, 54], [53, 54], [66, 68], [66, 67]], loss = 0.3588429391384125\n",
            "Epoch = 13, pairs = [[70, 71], [18, 19], [58, 62], [41, 43]], loss = 0.3703722655773163\n",
            "Epoch 13 took 85.14s.\n",
            "( 0,  1): reprojection: 0.237844, disparity: 0.054081\n",
            "( 0,  2): reprojection: 0.357651, disparity: 0.052763\n",
            "( 0,  4): reprojection: 0.408993, disparity: 0.062490\n",
            "( 0,  8): reprojection: 0.502007, disparity: 0.082990\n",
            "( 0, 16): reprojection: 0.898055, disparity: 0.080467\n",
            "( 0, 32): reprojection: 1.617436, disparity: 0.105105\n",
            "( 1,  2): reprojection: 0.198710, disparity: 0.046593\n",
            "( 1,  3): reprojection: 0.254351, disparity: 0.050715\n",
            "( 2,  3): reprojection: 0.110548, disparity: 0.044860\n",
            "( 2,  4): reprojection: 0.176565, disparity: 0.052062\n",
            "( 2,  6): reprojection: 0.339890, disparity: 0.063020\n",
            "( 3,  4): reprojection: 0.112641, disparity: 0.046742\n",
            "( 3,  5): reprojection: 0.195607, disparity: 0.052583\n",
            "( 4,  5): reprojection: 0.151494, disparity: 0.047382\n",
            "( 4,  6): reprojection: 0.212969, disparity: 0.052009\n",
            "( 4,  8): reprojection: 0.296102, disparity: 0.066010\n",
            "( 4, 12): reprojection: 0.356804, disparity: 0.075167\n",
            "( 5,  6): reprojection: 0.121645, disparity: 0.045723\n",
            "( 5,  7): reprojection: 0.165381, disparity: 0.049593\n",
            "( 6,  7): reprojection: 0.104395, disparity: 0.044761\n",
            "( 6,  8): reprojection: 0.175304, disparity: 0.051686\n",
            "( 6, 10): reprojection: 0.388275, disparity: 0.060961\n",
            "( 7,  8): reprojection: 0.095385, disparity: 0.045619\n",
            "( 7,  9): reprojection: 0.193418, disparity: 0.050738\n",
            "( 8,  9): reprojection: 0.162480, disparity: 0.047471\n",
            "( 8, 10): reprojection: 0.349555, disparity: 0.053417\n",
            "( 8, 12): reprojection: 0.470083, disparity: 0.061361\n",
            "( 8, 16): reprojection: 0.634452, disparity: 0.076180\n",
            "( 8, 24): reprojection: 0.947680, disparity: 0.091985\n",
            "( 9, 10): reprojection: 0.239851, disparity: 0.047113\n",
            "( 9, 11): reprojection: 0.353275, disparity: 0.052909\n",
            "(10, 11): reprojection: 0.151627, disparity: 0.043156\n",
            "(10, 12): reprojection: 0.191919, disparity: 0.050485\n",
            "(10, 14): reprojection: 0.333956, disparity: 0.065641\n",
            "(11, 12): reprojection: 0.132319, disparity: 0.043159\n",
            "(11, 13): reprojection: 0.211610, disparity: 0.052747\n",
            "(12, 13): reprojection: 0.144098, disparity: 0.044110\n",
            "(12, 14): reprojection: 0.277606, disparity: 0.054457\n",
            "(12, 16): reprojection: 0.501686, disparity: 0.074540\n",
            "(12, 20): reprojection: 0.795265, disparity: 0.076258\n",
            "(13, 14): reprojection: 0.182045, disparity: 0.048537\n",
            "(13, 15): reprojection: 0.278345, disparity: 0.054401\n",
            "(14, 15): reprojection: 0.204538, disparity: 0.047074\n",
            "(14, 16): reprojection: 0.441468, disparity: 0.056507\n",
            "(14, 18): reprojection: 0.408624, disparity: 0.066173\n",
            "(15, 16): reprojection: 0.301718, disparity: 0.048260\n",
            "(15, 17): reprojection: 0.386918, disparity: 0.058901\n",
            "(16, 17): reprojection: 0.146399, disparity: 0.049026\n",
            "(16, 18): reprojection: 0.331054, disparity: 0.057514\n",
            "(16, 20): reprojection: 0.555608, disparity: 0.066568\n",
            "(16, 24): reprojection: 0.878997, disparity: 0.081383\n",
            "(16, 32): reprojection: 1.538125, disparity: 0.104249\n",
            "(16, 48): reprojection: 1.465283, disparity: 0.156051\n",
            "(17, 18): reprojection: 0.315508, disparity: 0.051182\n",
            "(17, 19): reprojection: 0.485934, disparity: 0.059772\n",
            "(18, 19): reprojection: 0.212985, disparity: 0.049119\n",
            "(18, 20): reprojection: 0.286896, disparity: 0.056088\n",
            "(18, 22): reprojection: 0.518309, disparity: 0.063118\n",
            "(19, 20): reprojection: 0.116184, disparity: 0.047413\n",
            "(19, 21): reprojection: 0.250336, disparity: 0.056235\n",
            "(20, 21): reprojection: 0.202708, disparity: 0.049704\n",
            "(20, 22): reprojection: 0.311076, disparity: 0.058360\n",
            "(20, 24): reprojection: 0.447652, disparity: 0.065792\n",
            "(20, 28): reprojection: 0.857415, disparity: 0.092786\n",
            "(21, 22): reprojection: 0.173621, disparity: 0.049963\n",
            "(21, 23): reprojection: 0.286915, disparity: 0.057443\n",
            "(22, 23): reprojection: 0.137045, disparity: 0.049466\n",
            "(22, 24): reprojection: 0.204836, disparity: 0.058730\n",
            "(22, 26): reprojection: 0.363376, disparity: 0.074716\n",
            "(23, 24): reprojection: 0.114475, disparity: 0.051410\n",
            "(23, 25): reprojection: 0.197769, disparity: 0.062987\n",
            "(24, 25): reprojection: 0.155411, disparity: 0.054427\n",
            "(24, 26): reprojection: 0.211464, disparity: 0.062433\n",
            "(24, 28): reprojection: 0.344250, disparity: 0.075572\n",
            "(24, 32): reprojection: 0.731343, disparity: 0.090587\n",
            "(24, 40): reprojection: 1.251585, disparity: 0.124377\n",
            "(25, 26): reprojection: 0.135163, disparity: 0.055230\n",
            "(25, 27): reprojection: 0.242915, disparity: 0.062701\n",
            "(26, 27): reprojection: 0.157904, disparity: 0.054305\n",
            "(26, 28): reprojection: 0.192411, disparity: 0.061892\n",
            "(26, 30): reprojection: 0.370081, disparity: 0.070353\n",
            "(27, 28): reprojection: 0.155870, disparity: 0.053828\n",
            "(27, 29): reprojection: 0.272860, disparity: 0.062251\n",
            "(28, 29): reprojection: 0.192778, disparity: 0.056312\n",
            "(28, 30): reprojection: 0.252298, disparity: 0.063946\n",
            "(28, 32): reprojection: 0.525181, disparity: 0.072198\n",
            "(28, 36): reprojection: 0.872697, disparity: 0.086400\n",
            "(29, 30): reprojection: 0.137431, disparity: 0.052872\n",
            "(29, 31): reprojection: 0.308676, disparity: 0.059264\n",
            "(30, 31): reprojection: 0.206351, disparity: 0.053559\n",
            "(30, 32): reprojection: 0.305062, disparity: 0.061179\n",
            "(30, 34): reprojection: 0.565832, disparity: 0.070223\n",
            "(31, 32): reprojection: 0.144329, disparity: 0.053218\n",
            "(31, 33): reprojection: 0.343946, disparity: 0.059477\n",
            "(32, 33): reprojection: 0.264630, disparity: 0.054853\n",
            "(32, 34): reprojection: 0.285374, disparity: 0.059034\n",
            "(32, 36): reprojection: 0.434234, disparity: 0.071505\n",
            "(32, 40): reprojection: 0.708434, disparity: 0.100660\n",
            "(32, 48): reprojection: 1.259761, disparity: 0.142628\n",
            "(32, 64): reprojection: 1.294076, disparity: 0.153930\n",
            "(33, 34): reprojection: 0.145620, disparity: 0.052170\n",
            "(33, 35): reprojection: 0.279251, disparity: 0.057411\n",
            "(34, 35): reprojection: 0.197967, disparity: 0.051245\n",
            "(34, 36): reprojection: 0.317691, disparity: 0.061431\n",
            "(34, 38): reprojection: 0.454332, disparity: 0.073474\n",
            "(35, 36): reprojection: 0.245721, disparity: 0.056051\n",
            "(35, 37): reprojection: 0.321462, disparity: 0.070235\n",
            "(36, 37): reprojection: 0.280664, disparity: 0.060732\n",
            "(36, 38): reprojection: 0.384420, disparity: 0.067171\n",
            "(36, 40): reprojection: 0.382978, disparity: 0.077571\n",
            "(36, 44): reprojection: 0.783427, disparity: 0.109021\n",
            "(37, 38): reprojection: 0.221358, disparity: 0.053669\n",
            "(37, 39): reprojection: 0.263413, disparity: 0.061622\n",
            "(38, 39): reprojection: 0.198326, disparity: 0.055158\n",
            "(38, 40): reprojection: 0.375661, disparity: 0.065598\n",
            "(38, 42): reprojection: 0.654714, disparity: 0.080802\n",
            "(39, 40): reprojection: 0.284318, disparity: 0.057211\n",
            "(39, 41): reprojection: 0.486938, disparity: 0.067610\n",
            "(40, 41): reprojection: 0.279419, disparity: 0.059567\n",
            "(40, 42): reprojection: 0.401954, disparity: 0.069196\n",
            "(40, 44): reprojection: 0.538005, disparity: 0.084247\n",
            "(40, 48): reprojection: 0.815995, disparity: 0.110198\n",
            "(40, 56): reprojection: 1.048486, disparity: 0.118036\n",
            "(41, 42): reprojection: 0.199845, disparity: 0.061589\n",
            "(41, 43): reprojection: 0.305748, disparity: 0.071166\n",
            "(42, 43): reprojection: 0.169960, disparity: 0.058376\n",
            "(42, 44): reprojection: 0.322317, disparity: 0.070377\n",
            "(42, 46): reprojection: 0.526838, disparity: 0.080315\n",
            "(43, 44): reprojection: 0.219020, disparity: 0.057847\n",
            "(43, 45): reprojection: 0.319043, disparity: 0.070581\n",
            "(44, 45): reprojection: 0.162289, disparity: 0.058910\n",
            "(44, 46): reprojection: 0.291879, disparity: 0.067327\n",
            "(44, 48): reprojection: 0.458202, disparity: 0.086403\n",
            "(44, 52): reprojection: 0.688193, disparity: 0.089148\n",
            "(45, 46): reprojection: 0.207218, disparity: 0.060255\n",
            "(45, 47): reprojection: 0.340881, disparity: 0.071146\n",
            "(46, 47): reprojection: 0.197734, disparity: 0.061588\n",
            "(46, 48): reprojection: 0.305023, disparity: 0.076869\n",
            "(46, 50): reprojection: 0.435689, disparity: 0.075750\n",
            "(47, 48): reprojection: 0.211694, disparity: 0.058907\n",
            "(47, 49): reprojection: 0.289311, disparity: 0.064420\n",
            "(48, 49): reprojection: 0.184440, disparity: 0.066163\n",
            "(48, 50): reprojection: 0.285759, disparity: 0.072271\n",
            "(48, 52): reprojection: 0.391182, disparity: 0.085027\n",
            "(48, 56): reprojection: 0.832271, disparity: 0.092898\n",
            "(48, 64): reprojection: 1.161842, disparity: 0.135696\n",
            "(48, 80): reprojection: 1.487716, disparity: 0.159945\n",
            "(49, 50): reprojection: 0.164598, disparity: 0.063075\n",
            "(49, 51): reprojection: 0.264077, disparity: 0.068530\n",
            "(50, 51): reprojection: 0.165762, disparity: 0.060392\n",
            "(50, 52): reprojection: 0.262048, disparity: 0.065745\n",
            "(50, 54): reprojection: 0.381515, disparity: 0.075658\n",
            "(51, 52): reprojection: 0.157362, disparity: 0.059832\n",
            "(51, 53): reprojection: 0.283722, disparity: 0.065004\n",
            "(52, 53): reprojection: 0.166391, disparity: 0.061707\n",
            "(52, 54): reprojection: 0.246290, disparity: 0.067504\n",
            "(52, 56): reprojection: 0.549258, disparity: 0.069387\n",
            "(52, 60): reprojection: 0.729466, disparity: 0.095856\n",
            "(53, 54): reprojection: 0.138865, disparity: 0.059775\n",
            "(53, 55): reprojection: 0.331461, disparity: 0.066370\n",
            "(54, 55): reprojection: 0.289436, disparity: 0.059137\n",
            "(54, 56): reprojection: 0.564335, disparity: 0.065620\n",
            "(54, 58): reprojection: 0.444421, disparity: 0.075903\n",
            "(55, 56): reprojection: 0.347529, disparity: 0.063183\n",
            "(55, 57): reprojection: 0.416579, disparity: 0.067346\n",
            "(56, 57): reprojection: 0.302804, disparity: 0.062517\n",
            "(56, 58): reprojection: 0.527931, disparity: 0.075473\n",
            "(56, 60): reprojection: 0.672488, disparity: 0.088101\n",
            "(56, 64): reprojection: 0.973808, disparity: 0.095984\n",
            "(56, 72): reprojection: 1.215392, disparity: 0.156692\n",
            "(57, 58): reprojection: 0.342935, disparity: 0.067829\n",
            "(57, 59): reprojection: 0.457667, disparity: 0.074190\n",
            "(58, 59): reprojection: 0.228873, disparity: 0.068175\n",
            "(58, 60): reprojection: 0.363349, disparity: 0.072990\n",
            "(58, 62): reprojection: 0.453400, disparity: 0.085292\n",
            "(59, 60): reprojection: 0.194049, disparity: 0.068223\n",
            "(59, 61): reprojection: 0.316709, disparity: 0.077127\n",
            "(60, 61): reprojection: 0.218810, disparity: 0.070528\n",
            "(60, 62): reprojection: 0.305617, disparity: 0.076155\n",
            "(60, 64): reprojection: 0.475316, disparity: 0.081742\n",
            "(60, 68): reprojection: 0.675496, disparity: 0.109913\n",
            "(61, 62): reprojection: 0.181793, disparity: 0.070588\n",
            "(61, 63): reprojection: 0.283553, disparity: 0.074466\n",
            "(62, 63): reprojection: 0.218845, disparity: 0.066922\n",
            "(62, 64): reprojection: 0.327018, disparity: 0.073121\n",
            "(62, 66): reprojection: 0.571559, disparity: 0.096699\n",
            "(63, 64): reprojection: 0.212177, disparity: 0.067584\n",
            "(63, 65): reprojection: 0.349268, disparity: 0.073488\n",
            "(64, 65): reprojection: 0.226923, disparity: 0.071119\n",
            "(64, 66): reprojection: 0.374912, disparity: 0.074505\n",
            "(64, 68): reprojection: 0.524772, disparity: 0.088626\n",
            "(64, 72): reprojection: 0.941471, disparity: 0.114745\n",
            "(64, 80): reprojection: 1.295271, disparity: 0.117373\n",
            "(65, 66): reprojection: 0.251213, disparity: 0.063919\n",
            "(65, 67): reprojection: 0.392640, disparity: 0.070480\n",
            "(66, 67): reprojection: 0.251748, disparity: 0.067051\n",
            "(66, 68): reprojection: 0.364935, disparity: 0.067604\n",
            "(66, 70): reprojection: 0.485756, disparity: 0.083990\n",
            "(67, 68): reprojection: 0.324993, disparity: 0.063117\n",
            "(67, 69): reprojection: 0.306004, disparity: 0.070333\n",
            "(68, 69): reprojection: 0.204711, disparity: 0.062473\n",
            "(68, 70): reprojection: 0.431820, disparity: 0.066560\n",
            "(68, 72): reprojection: 0.594151, disparity: 0.092107\n",
            "(68, 76): reprojection: 0.806893, disparity: 0.096798\n",
            "(69, 70): reprojection: 0.287193, disparity: 0.060389\n",
            "(69, 71): reprojection: 0.402856, disparity: 0.064472\n",
            "(70, 71): reprojection: 0.210762, disparity: 0.064593\n",
            "(70, 72): reprojection: 0.305094, disparity: 0.072072\n",
            "(70, 74): reprojection: 0.502265, disparity: 0.082361\n",
            "(71, 72): reprojection: 0.224914, disparity: 0.064867\n",
            "(71, 73): reprojection: 0.338583, disparity: 0.071765\n",
            "(72, 73): reprojection: 0.219969, disparity: 0.063005\n",
            "(72, 74): reprojection: 0.326139, disparity: 0.068464\n",
            "(72, 76): reprojection: 0.465001, disparity: 0.074668\n",
            "(72, 80): reprojection: 0.855200, disparity: 0.089344\n",
            "(72, 88): reprojection: 0.958690, disparity: 0.109017\n",
            "(73, 74): reprojection: 0.319837, disparity: 0.062349\n",
            "(73, 75): reprojection: 0.451528, disparity: 0.068118\n",
            "(74, 75): reprojection: 0.249476, disparity: 0.061499\n",
            "(74, 76): reprojection: 0.293004, disparity: 0.063490\n",
            "(74, 78): reprojection: 0.462403, disparity: 0.074463\n",
            "(75, 76): reprojection: 0.223222, disparity: 0.059205\n",
            "(75, 77): reprojection: 0.357070, disparity: 0.065565\n",
            "(76, 77): reprojection: 0.237497, disparity: 0.059073\n",
            "(76, 78): reprojection: 0.378271, disparity: 0.062351\n",
            "(76, 80): reprojection: 0.494074, disparity: 0.071564\n",
            "(76, 84): reprojection: 0.733220, disparity: 0.090549\n",
            "(77, 78): reprojection: 0.243497, disparity: 0.059260\n",
            "(77, 79): reprojection: 0.272836, disparity: 0.063195\n",
            "(78, 79): reprojection: 0.199706, disparity: 0.061902\n",
            "(78, 80): reprojection: 0.250117, disparity: 0.066143\n",
            "(78, 82): reprojection: 0.396929, disparity: 0.074905\n",
            "(79, 80): reprojection: 0.148071, disparity: 0.059008\n",
            "(79, 81): reprojection: 0.251360, disparity: 0.063424\n",
            "(80, 81): reprojection: 0.156299, disparity: 0.058083\n",
            "(80, 82): reprojection: 0.280388, disparity: 0.065197\n",
            "(80, 84): reprojection: 0.464509, disparity: 0.070265\n",
            "(80, 88): reprojection: 0.723941, disparity: 0.088225\n",
            "(81, 82): reprojection: 0.184378, disparity: 0.058170\n",
            "(81, 83): reprojection: 0.279986, disparity: 0.064309\n",
            "(82, 83): reprojection: 0.143217, disparity: 0.057201\n",
            "(82, 84): reprojection: 0.262769, disparity: 0.063564\n",
            "(82, 86): reprojection: 0.420883, disparity: 0.072481\n",
            "(83, 84): reprojection: 0.147171, disparity: 0.057499\n",
            "(83, 85): reprojection: 0.300158, disparity: 0.067303\n",
            "(84, 85): reprojection: 0.172911, disparity: 0.055334\n",
            "(84, 86): reprojection: 0.240484, disparity: 0.061897\n",
            "(84, 88): reprojection: 0.467512, disparity: 0.070010\n",
            "(85, 86): reprojection: 0.156506, disparity: 0.054430\n",
            "(85, 87): reprojection: 0.264966, disparity: 0.061816\n",
            "(86, 87): reprojection: 0.225940, disparity: 0.056764\n",
            "(86, 88): reprojection: 0.321983, disparity: 0.066419\n",
            "(86, 90): reprojection: 0.504289, disparity: 0.070860\n",
            "(87, 88): reprojection: 0.174031, disparity: 0.055936\n",
            "(87, 89): reprojection: 0.285514, disparity: 0.064006\n",
            "(88, 89): reprojection: 0.147913, disparity: 0.054275\n",
            "(88, 90): reprojection: 0.312533, disparity: 0.060523\n",
            "(89, 90): reprojection: 0.213130, disparity: 0.051538\n",
            "(89, 91): reprojection: 0.409943, disparity: 0.064510\n",
            "(90, 91): reprojection: 0.321964, disparity: 0.057670\n",
            "Mean:     reprojection: 0.321964, disparity: 0.057670\n",
            "Done Validation for epoch 14 (3640 iterations)\n",
            "Epoch = 14, pairs = [[30, 31], [38, 40], [12, 13], [87, 88]], loss = 0.2832399010658264\n",
            "Epoch = 14, pairs = [[65, 66], [70, 71], [31, 33], [15, 16]], loss = 0.328031986951828\n",
            "Epoch = 14, pairs = [[82, 86], [47, 49], [56, 58], [69, 71]], loss = 0.48546281456947327\n",
            "Epoch = 14, pairs = [[48, 80], [17, 18], [32, 33], [0, 1]], loss = 0.6351320743560791\n",
            "Epoch = 14, pairs = [[13, 14], [86, 88], [67, 69], [6, 7]], loss = 0.28075817227363586\n",
            "Epoch = 14, pairs = [[55, 57], [42, 46], [50, 51], [48, 64]], loss = 0.704909086227417\n",
            "Epoch = 14, pairs = [[72, 76], [85, 87], [18, 19], [84, 86]], loss = 0.4017147421836853\n",
            "Epoch = 14, pairs = [[14, 16], [74, 76], [69, 70], [36, 38]], loss = 0.4255196154117584\n",
            "Epoch = 14, pairs = [[32, 34], [86, 87], [40, 41], [36, 44]], loss = 0.43791553378105164\n",
            "Epoch = 14, pairs = [[74, 78], [24, 28], [89, 91], [80, 82]], loss = 0.4305323362350464\n",
            "Epoch = 14, pairs = [[22, 24], [81, 83], [22, 26], [8, 9]], loss = 0.2896345257759094\n",
            "Epoch = 14, pairs = [[48, 50], [37, 38], [11, 12], [20, 24]], loss = 0.3254276514053345\n",
            "Epoch = 14, pairs = [[26, 28], [26, 30], [13, 15], [39, 41]], loss = 0.4016014039516449\n",
            "Epoch = 14, pairs = [[26, 27], [20, 21], [65, 67], [66, 67]], loss = 0.3399091958999634\n",
            "Epoch = 14, pairs = [[73, 75], [12, 16], [43, 45], [46, 48]], loss = 0.4465879499912262\n",
            "Epoch = 14, pairs = [[35, 36], [42, 44], [19, 20], [72, 88]], loss = 0.48252248764038086\n",
            "Epoch = 14, pairs = [[81, 82], [64, 80], [48, 49], [5, 6]], loss = 0.4612942636013031\n",
            "Epoch = 14, pairs = [[8, 10], [50, 54], [3, 4], [5, 7]], loss = 0.32048583030700684\n",
            "Epoch = 14, pairs = [[22, 23], [40, 44], [7, 9], [75, 76]], loss = 0.3292415738105774\n",
            "Epoch = 14, pairs = [[60, 62], [14, 15], [30, 34], [57, 58]], loss = 0.3944410979747772\n",
            "Epoch = 14, pairs = [[49, 50], [16, 24], [62, 66], [56, 60]], loss = 0.6444503664970398\n",
            "Epoch = 14, pairs = [[20, 22], [1, 3], [45, 47], [25, 26]], loss = 0.30511167645454407\n",
            "Epoch = 14, pairs = [[31, 32], [27, 28], [52, 60], [8, 12]], loss = 0.41808512806892395\n",
            "Epoch = 14, pairs = [[71, 73], [42, 43], [33, 34], [58, 62]], loss = 0.3619333803653717\n",
            "Epoch = 14, pairs = [[52, 56], [12, 20], [44, 46], [60, 68]], loss = 0.6172597408294678\n",
            "Epoch = 14, pairs = [[8, 24], [70, 72], [15, 17], [80, 84]], loss = 0.5207281112670898\n",
            "Epoch = 14, pairs = [[87, 89], [61, 63], [56, 64], [45, 46]], loss = 0.4669361114501953\n",
            "Epoch = 14, pairs = [[0, 32], [32, 36], [24, 32], [55, 56]], loss = 0.8094673752784729\n",
            "Epoch = 14, pairs = [[21, 23], [34, 38], [2, 6], [32, 64]], loss = 0.6247245073318481\n",
            "Epoch = 14, pairs = [[24, 26], [56, 72], [24, 40], [46, 47]], loss = 0.6484775543212891\n",
            "Epoch = 14, pairs = [[78, 80], [50, 52], [16, 32], [28, 36]], loss = 0.5966868996620178\n",
            "Epoch = 14, pairs = [[68, 70], [84, 88], [7, 8], [75, 77]], loss = 0.4362228810787201\n",
            "Epoch = 14, pairs = [[68, 76], [67, 68], [34, 36], [78, 82]], loss = 0.5150796175003052\n",
            "Epoch = 14, pairs = [[62, 64], [54, 55], [62, 63], [4, 5]], loss = 0.3159492611885071\n",
            "Epoch = 14, pairs = [[32, 40], [2, 4], [0, 4], [40, 56]], loss = 0.629802942276001\n",
            "Epoch = 14, pairs = [[16, 17], [43, 44], [41, 43], [74, 75]], loss = 0.28907155990600586\n",
            "Epoch = 14, pairs = [[16, 18], [77, 78], [52, 53], [4, 6]], loss = 0.2941296696662903\n",
            "Epoch = 14, pairs = [[63, 65], [79, 81], [57, 59], [36, 40]], loss = 0.41786670684814453\n",
            "Epoch = 14, pairs = [[30, 32], [44, 52], [60, 64], [18, 20]], loss = 0.4890892803668976\n",
            "Epoch = 14, pairs = [[40, 42], [16, 48], [38, 42], [51, 52]], loss = 0.8508421182632446\n",
            "Epoch = 14, pairs = [[66, 70], [18, 22], [48, 52], [58, 59]], loss = 0.4834699034690857\n",
            "Epoch = 14, pairs = [[64, 68], [6, 8], [41, 42], [10, 12]], loss = 0.33267393708229065\n",
            "Epoch = 14, pairs = [[46, 50], [68, 69], [0, 16], [59, 60]], loss = 0.5739567279815674\n",
            "Epoch = 14, pairs = [[73, 74], [40, 48], [47, 48], [76, 84]], loss = 0.658612847328186\n",
            "Epoch = 14, pairs = [[76, 78], [70, 74], [83, 85], [58, 60]], loss = 0.4632728099822998\n",
            "Epoch = 14, pairs = [[29, 31], [27, 29], [88, 90], [88, 89]], loss = 0.32224535942077637\n",
            "Epoch = 14, pairs = [[80, 81], [90, 91], [28, 29], [21, 22]], loss = 0.28042072057724\n",
            "Epoch = 14, pairs = [[68, 72], [44, 45], [4, 8], [23, 25]], loss = 0.40605515241622925\n",
            "Epoch = 14, pairs = [[8, 16], [23, 24], [4, 12], [1, 2]], loss = 0.5453499555587769\n",
            "Epoch = 14, pairs = [[72, 80], [29, 30], [38, 39], [10, 14]], loss = 0.4989461302757263\n",
            "Epoch = 14, pairs = [[84, 85], [10, 11], [64, 65], [72, 74]], loss = 0.29538843035697937\n",
            "Epoch = 14, pairs = [[35, 37], [85, 86], [19, 21], [53, 55]], loss = 0.36523187160491943\n",
            "Epoch = 14, pairs = [[32, 48], [52, 54], [78, 79], [82, 83]], loss = 0.551385223865509\n",
            "Epoch = 14, pairs = [[82, 84], [54, 58], [61, 62], [49, 51]], loss = 0.34205925464630127\n",
            "Epoch = 14, pairs = [[33, 35], [16, 20], [80, 88], [6, 10]], loss = 0.5541414022445679\n",
            "Epoch = 14, pairs = [[9, 11], [28, 32], [2, 3], [76, 80]], loss = 0.3848746418952942\n",
            "Epoch = 14, pairs = [[12, 14], [36, 37], [39, 40], [56, 57]], loss = 0.33857041597366333\n",
            "Epoch = 14, pairs = [[28, 30], [20, 28], [83, 84], [64, 66]], loss = 0.41512376070022583\n",
            "Epoch = 14, pairs = [[86, 90], [66, 68], [59, 61], [17, 19]], loss = 0.5011281371116638\n",
            "Epoch = 14, pairs = [[37, 39], [76, 77], [72, 73], [24, 25]], loss = 0.2616521716117859\n",
            "Epoch = 14, pairs = [[48, 56], [64, 72], [25, 27], [9, 10]], loss = 0.569882869720459\n",
            "Epoch = 14, pairs = [[44, 48], [60, 61], [3, 5], [77, 79]], loss = 0.33578455448150635\n",
            "Epoch = 14, pairs = [[51, 53], [0, 8], [14, 18], [54, 56]], loss = 0.49460142850875854\n",
            "Epoch = 14, pairs = [[79, 80], [0, 2], [71, 72], [89, 90]], loss = 0.28349292278289795\n",
            "Epoch = 14, pairs = [[34, 35], [53, 54], [11, 13], [63, 64]], loss = 0.24323827028274536\n",
            "Epoch 14 took 85.20s.\n",
            "( 0,  1): reprojection: 0.235089, disparity: 0.049100\n",
            "( 0,  2): reprojection: 0.347658, disparity: 0.048719\n",
            "( 0,  4): reprojection: 0.385060, disparity: 0.056389\n",
            "( 0,  8): reprojection: 0.463786, disparity: 0.068587\n",
            "( 0, 16): reprojection: 0.736334, disparity: 0.075480\n",
            "( 0, 32): reprojection: 1.133031, disparity: 0.135476\n",
            "( 1,  2): reprojection: 0.195191, disparity: 0.043305\n",
            "( 1,  3): reprojection: 0.246805, disparity: 0.047097\n",
            "( 2,  3): reprojection: 0.107586, disparity: 0.041972\n",
            "( 2,  4): reprojection: 0.170206, disparity: 0.047378\n",
            "( 2,  6): reprojection: 0.334415, disparity: 0.055301\n",
            "( 3,  4): reprojection: 0.106901, disparity: 0.043724\n",
            "( 3,  5): reprojection: 0.193408, disparity: 0.047573\n",
            "( 4,  5): reprojection: 0.152368, disparity: 0.044116\n",
            "( 4,  6): reprojection: 0.213036, disparity: 0.044366\n",
            "( 4,  8): reprojection: 0.278838, disparity: 0.054433\n",
            "( 4, 12): reprojection: 0.383282, disparity: 0.068206\n",
            "( 5,  6): reprojection: 0.116889, disparity: 0.042447\n",
            "( 5,  7): reprojection: 0.151328, disparity: 0.046659\n",
            "( 6,  7): reprojection: 0.096316, disparity: 0.043191\n",
            "( 6,  8): reprojection: 0.161622, disparity: 0.046585\n",
            "( 6, 10): reprojection: 0.385671, disparity: 0.056419\n",
            "( 7,  8): reprojection: 0.088524, disparity: 0.042467\n",
            "( 7,  9): reprojection: 0.190091, disparity: 0.046924\n",
            "( 8,  9): reprojection: 0.161079, disparity: 0.044034\n",
            "( 8, 10): reprojection: 0.357290, disparity: 0.047950\n",
            "( 8, 12): reprojection: 0.503111, disparity: 0.057475\n",
            "( 8, 16): reprojection: 0.631034, disparity: 0.068655\n",
            "( 8, 24): reprojection: 0.772675, disparity: 0.088414\n",
            "( 9, 10): reprojection: 0.246720, disparity: 0.043055\n",
            "( 9, 11): reprojection: 0.366819, disparity: 0.048222\n",
            "(10, 11): reprojection: 0.160280, disparity: 0.040642\n",
            "(10, 12): reprojection: 0.200647, disparity: 0.046236\n",
            "(10, 14): reprojection: 0.331413, disparity: 0.060410\n",
            "(11, 12): reprojection: 0.134168, disparity: 0.040213\n",
            "(11, 13): reprojection: 0.210838, disparity: 0.048723\n",
            "(12, 13): reprojection: 0.136003, disparity: 0.041888\n",
            "(12, 14): reprojection: 0.264051, disparity: 0.049589\n",
            "(12, 16): reprojection: 0.436821, disparity: 0.067853\n",
            "(12, 20): reprojection: 0.684948, disparity: 0.070706\n",
            "(13, 14): reprojection: 0.180494, disparity: 0.044746\n",
            "(13, 15): reprojection: 0.242447, disparity: 0.050340\n",
            "(14, 15): reprojection: 0.188112, disparity: 0.045157\n",
            "(14, 16): reprojection: 0.425130, disparity: 0.053282\n",
            "(14, 18): reprojection: 0.348671, disparity: 0.060172\n",
            "(15, 16): reprojection: 0.296580, disparity: 0.045395\n",
            "(15, 17): reprojection: 0.375997, disparity: 0.053168\n",
            "(16, 17): reprojection: 0.136556, disparity: 0.044256\n",
            "(16, 18): reprojection: 0.314980, disparity: 0.057097\n",
            "(16, 20): reprojection: 0.544806, disparity: 0.067674\n",
            "(16, 24): reprojection: 0.809836, disparity: 0.083706\n",
            "(16, 32): reprojection: 1.233608, disparity: 0.143816\n",
            "(16, 48): reprojection: 1.545763, disparity: 0.345089\n",
            "(17, 18): reprojection: 0.313227, disparity: 0.049880\n",
            "(17, 19): reprojection: 0.479699, disparity: 0.060469\n",
            "(18, 19): reprojection: 0.210321, disparity: 0.046633\n",
            "(18, 20): reprojection: 0.287115, disparity: 0.052411\n",
            "(18, 22): reprojection: 0.487887, disparity: 0.062556\n",
            "(19, 20): reprojection: 0.108706, disparity: 0.044303\n",
            "(19, 21): reprojection: 0.242932, disparity: 0.052766\n",
            "(20, 21): reprojection: 0.191710, disparity: 0.047953\n",
            "(20, 22): reprojection: 0.290401, disparity: 0.055369\n",
            "(20, 24): reprojection: 0.382406, disparity: 0.062870\n",
            "(20, 28): reprojection: 0.684657, disparity: 0.093265\n",
            "(21, 22): reprojection: 0.157326, disparity: 0.045830\n",
            "(21, 23): reprojection: 0.246341, disparity: 0.051805\n",
            "(22, 23): reprojection: 0.122580, disparity: 0.046211\n",
            "(22, 24): reprojection: 0.166431, disparity: 0.056023\n",
            "(22, 26): reprojection: 0.311688, disparity: 0.072262\n",
            "(23, 24): reprojection: 0.099559, disparity: 0.048270\n",
            "(23, 25): reprojection: 0.174628, disparity: 0.057210\n",
            "(24, 25): reprojection: 0.134099, disparity: 0.049726\n",
            "(24, 26): reprojection: 0.181248, disparity: 0.057494\n",
            "(24, 28): reprojection: 0.294134, disparity: 0.076322\n",
            "(24, 32): reprojection: 0.590691, disparity: 0.097268\n",
            "(24, 40): reprojection: 0.910866, disparity: 0.164885\n",
            "(25, 26): reprojection: 0.124332, disparity: 0.052520\n",
            "(25, 27): reprojection: 0.220019, disparity: 0.060317\n",
            "(26, 27): reprojection: 0.143511, disparity: 0.051610\n",
            "(26, 28): reprojection: 0.170029, disparity: 0.061477\n",
            "(26, 30): reprojection: 0.310530, disparity: 0.073220\n",
            "(27, 28): reprojection: 0.139467, disparity: 0.049305\n",
            "(27, 29): reprojection: 0.250890, disparity: 0.057894\n",
            "(28, 29): reprojection: 0.180536, disparity: 0.052284\n",
            "(28, 30): reprojection: 0.236723, disparity: 0.059145\n",
            "(28, 32): reprojection: 0.450132, disparity: 0.066982\n",
            "(28, 36): reprojection: 0.725219, disparity: 0.092461\n",
            "(29, 30): reprojection: 0.129846, disparity: 0.049147\n",
            "(29, 31): reprojection: 0.273961, disparity: 0.053545\n",
            "(30, 31): reprojection: 0.188316, disparity: 0.049189\n",
            "(30, 32): reprojection: 0.272243, disparity: 0.055388\n",
            "(30, 34): reprojection: 0.499123, disparity: 0.063821\n",
            "(31, 32): reprojection: 0.131517, disparity: 0.049109\n",
            "(31, 33): reprojection: 0.313039, disparity: 0.055632\n",
            "(32, 33): reprojection: 0.250808, disparity: 0.050852\n",
            "(32, 34): reprojection: 0.258012, disparity: 0.054767\n",
            "(32, 36): reprojection: 0.363568, disparity: 0.068785\n",
            "(32, 40): reprojection: 0.515052, disparity: 0.096199\n",
            "(32, 48): reprojection: 1.042122, disparity: 0.134097\n",
            "(32, 64): reprojection: 1.434115, disparity: 0.207854\n",
            "(33, 34): reprojection: 0.151063, disparity: 0.048838\n",
            "(33, 35): reprojection: 0.260926, disparity: 0.053531\n",
            "(34, 35): reprojection: 0.187015, disparity: 0.047692\n",
            "(34, 36): reprojection: 0.286997, disparity: 0.057917\n",
            "(34, 38): reprojection: 0.367710, disparity: 0.065078\n",
            "(35, 36): reprojection: 0.234717, disparity: 0.051668\n",
            "(35, 37): reprojection: 0.276885, disparity: 0.063488\n",
            "(36, 37): reprojection: 0.259564, disparity: 0.054047\n",
            "(36, 38): reprojection: 0.369816, disparity: 0.060089\n",
            "(36, 40): reprojection: 0.324379, disparity: 0.076546\n",
            "(36, 44): reprojection: 0.704100, disparity: 0.114784\n",
            "(37, 38): reprojection: 0.219919, disparity: 0.051300\n",
            "(37, 39): reprojection: 0.242803, disparity: 0.056536\n",
            "(38, 39): reprojection: 0.196147, disparity: 0.051783\n",
            "(38, 40): reprojection: 0.373570, disparity: 0.065552\n",
            "(38, 42): reprojection: 0.665721, disparity: 0.083124\n",
            "(39, 40): reprojection: 0.283043, disparity: 0.058621\n",
            "(39, 41): reprojection: 0.498359, disparity: 0.071596\n",
            "(40, 41): reprojection: 0.292424, disparity: 0.057437\n",
            "(40, 42): reprojection: 0.416413, disparity: 0.063637\n",
            "(40, 44): reprojection: 0.567130, disparity: 0.082171\n",
            "(40, 48): reprojection: 0.794327, disparity: 0.100292\n",
            "(40, 56): reprojection: 0.767305, disparity: 0.127737\n",
            "(41, 42): reprojection: 0.191092, disparity: 0.053343\n",
            "(41, 43): reprojection: 0.305936, disparity: 0.061732\n",
            "(42, 43): reprojection: 0.179758, disparity: 0.054077\n",
            "(42, 44): reprojection: 0.333616, disparity: 0.067654\n",
            "(42, 46): reprojection: 0.534593, disparity: 0.080438\n",
            "(43, 44): reprojection: 0.212165, disparity: 0.054637\n",
            "(43, 45): reprojection: 0.306924, disparity: 0.066422\n",
            "(44, 45): reprojection: 0.149988, disparity: 0.054756\n",
            "(44, 46): reprojection: 0.285363, disparity: 0.065176\n",
            "(44, 48): reprojection: 0.419333, disparity: 0.078061\n",
            "(44, 52): reprojection: 0.610522, disparity: 0.090081\n",
            "(45, 46): reprojection: 0.204752, disparity: 0.057072\n",
            "(45, 47): reprojection: 0.332057, disparity: 0.067779\n",
            "(46, 47): reprojection: 0.194391, disparity: 0.056840\n",
            "(46, 48): reprojection: 0.295466, disparity: 0.066867\n",
            "(46, 50): reprojection: 0.399895, disparity: 0.071579\n",
            "(47, 48): reprojection: 0.202176, disparity: 0.056033\n",
            "(47, 49): reprojection: 0.257787, disparity: 0.062720\n",
            "(48, 49): reprojection: 0.169433, disparity: 0.061663\n",
            "(48, 50): reprojection: 0.244507, disparity: 0.064648\n",
            "(48, 52): reprojection: 0.351771, disparity: 0.077064\n",
            "(48, 56): reprojection: 0.662755, disparity: 0.083009\n",
            "(48, 64): reprojection: 0.957418, disparity: 0.153062\n",
            "(48, 80): reprojection: 1.998866, disparity: 0.116797\n",
            "(49, 50): reprojection: 0.148354, disparity: 0.059844\n",
            "(49, 51): reprojection: 0.242819, disparity: 0.063739\n",
            "(50, 51): reprojection: 0.148060, disparity: 0.058046\n",
            "(50, 52): reprojection: 0.246705, disparity: 0.065982\n",
            "(50, 54): reprojection: 0.389193, disparity: 0.079240\n",
            "(51, 52): reprojection: 0.166936, disparity: 0.056275\n",
            "(51, 53): reprojection: 0.298467, disparity: 0.062221\n",
            "(52, 53): reprojection: 0.170767, disparity: 0.055273\n",
            "(52, 54): reprojection: 0.254206, disparity: 0.064151\n",
            "(52, 56): reprojection: 0.512334, disparity: 0.067393\n",
            "(52, 60): reprojection: 0.606176, disparity: 0.089200\n",
            "(53, 54): reprojection: 0.137919, disparity: 0.056620\n",
            "(53, 55): reprojection: 0.323076, disparity: 0.064831\n",
            "(54, 55): reprojection: 0.297848, disparity: 0.056144\n",
            "(54, 56): reprojection: 0.554528, disparity: 0.063116\n",
            "(54, 58): reprojection: 0.374029, disparity: 0.065634\n",
            "(55, 56): reprojection: 0.346214, disparity: 0.060109\n",
            "(55, 57): reprojection: 0.381800, disparity: 0.061719\n",
            "(56, 57): reprojection: 0.301788, disparity: 0.058741\n",
            "(56, 58): reprojection: 0.544214, disparity: 0.069423\n",
            "(56, 60): reprojection: 0.668402, disparity: 0.092840\n",
            "(56, 64): reprojection: 0.883761, disparity: 0.103728\n",
            "(56, 72): reprojection: 1.137994, disparity: 0.132672\n",
            "(57, 58): reprojection: 0.351878, disparity: 0.063439\n",
            "(57, 59): reprojection: 0.471593, disparity: 0.073268\n",
            "(58, 59): reprojection: 0.240636, disparity: 0.064582\n",
            "(58, 60): reprojection: 0.363613, disparity: 0.070209\n",
            "(58, 62): reprojection: 0.454248, disparity: 0.092411\n",
            "(59, 60): reprojection: 0.189991, disparity: 0.063554\n",
            "(59, 61): reprojection: 0.303023, disparity: 0.069163\n",
            "(60, 61): reprojection: 0.215679, disparity: 0.067968\n",
            "(60, 62): reprojection: 0.295327, disparity: 0.071619\n",
            "(60, 64): reprojection: 0.483195, disparity: 0.081625\n",
            "(60, 68): reprojection: 0.633023, disparity: 0.108419\n",
            "(61, 62): reprojection: 0.204160, disparity: 0.065936\n",
            "(61, 63): reprojection: 0.298218, disparity: 0.075328\n",
            "(62, 63): reprojection: 0.223555, disparity: 0.064791\n",
            "(62, 64): reprojection: 0.337457, disparity: 0.070389\n",
            "(62, 66): reprojection: 0.529193, disparity: 0.087244\n",
            "(63, 64): reprojection: 0.213864, disparity: 0.065051\n",
            "(63, 65): reprojection: 0.340835, disparity: 0.068765\n",
            "(64, 65): reprojection: 0.221767, disparity: 0.066410\n",
            "(64, 66): reprojection: 0.368781, disparity: 0.067038\n",
            "(64, 68): reprojection: 0.476665, disparity: 0.084652\n",
            "(64, 72): reprojection: 0.823653, disparity: 0.096858\n",
            "(64, 80): reprojection: 1.284414, disparity: 0.101448\n",
            "(65, 66): reprojection: 0.271088, disparity: 0.060359\n",
            "(65, 67): reprojection: 0.413526, disparity: 0.068290\n",
            "(66, 67): reprojection: 0.258090, disparity: 0.063438\n",
            "(66, 68): reprojection: 0.371304, disparity: 0.064612\n",
            "(66, 70): reprojection: 0.469376, disparity: 0.074883\n",
            "(67, 68): reprojection: 0.341765, disparity: 0.059278\n",
            "(67, 69): reprojection: 0.332507, disparity: 0.062905\n",
            "(68, 69): reprojection: 0.204036, disparity: 0.057782\n",
            "(68, 70): reprojection: 0.439380, disparity: 0.063006\n",
            "(68, 72): reprojection: 0.530598, disparity: 0.072936\n",
            "(68, 76): reprojection: 0.704422, disparity: 0.090566\n",
            "(69, 70): reprojection: 0.298433, disparity: 0.057603\n",
            "(69, 71): reprojection: 0.424608, disparity: 0.061178\n",
            "(70, 71): reprojection: 0.213276, disparity: 0.058424\n",
            "(70, 72): reprojection: 0.275897, disparity: 0.061215\n",
            "(70, 74): reprojection: 0.419318, disparity: 0.073016\n",
            "(71, 72): reprojection: 0.225947, disparity: 0.059237\n",
            "(71, 73): reprojection: 0.341033, disparity: 0.064641\n",
            "(72, 73): reprojection: 0.224371, disparity: 0.059425\n",
            "(72, 74): reprojection: 0.305482, disparity: 0.065169\n",
            "(72, 76): reprojection: 0.386380, disparity: 0.076450\n",
            "(72, 80): reprojection: 0.683478, disparity: 0.091763\n",
            "(72, 88): reprojection: 0.883143, disparity: 0.096638\n",
            "(73, 74): reprojection: 0.318578, disparity: 0.060451\n",
            "(73, 75): reprojection: 0.433904, disparity: 0.069572\n",
            "(74, 75): reprojection: 0.236233, disparity: 0.060744\n",
            "(74, 76): reprojection: 0.262845, disparity: 0.062708\n",
            "(74, 78): reprojection: 0.388222, disparity: 0.073111\n",
            "(75, 76): reprojection: 0.225964, disparity: 0.056856\n",
            "(75, 77): reprojection: 0.347510, disparity: 0.061249\n",
            "(76, 77): reprojection: 0.227092, disparity: 0.056099\n",
            "(76, 78): reprojection: 0.343262, disparity: 0.057629\n",
            "(76, 80): reprojection: 0.421978, disparity: 0.067395\n",
            "(76, 84): reprojection: 0.597483, disparity: 0.085573\n",
            "(77, 78): reprojection: 0.229049, disparity: 0.056981\n",
            "(77, 79): reprojection: 0.257820, disparity: 0.059845\n",
            "(78, 79): reprojection: 0.194130, disparity: 0.057627\n",
            "(78, 80): reprojection: 0.245023, disparity: 0.062011\n",
            "(78, 82): reprojection: 0.387772, disparity: 0.068730\n",
            "(79, 80): reprojection: 0.146109, disparity: 0.055872\n",
            "(79, 81): reprojection: 0.244166, disparity: 0.058292\n",
            "(80, 81): reprojection: 0.148953, disparity: 0.053997\n",
            "(80, 82): reprojection: 0.261881, disparity: 0.060549\n",
            "(80, 84): reprojection: 0.439529, disparity: 0.063125\n",
            "(80, 88): reprojection: 0.714326, disparity: 0.078326\n",
            "(81, 82): reprojection: 0.182869, disparity: 0.053834\n",
            "(81, 83): reprojection: 0.277313, disparity: 0.058980\n",
            "(82, 83): reprojection: 0.142940, disparity: 0.052729\n",
            "(82, 84): reprojection: 0.243046, disparity: 0.058019\n",
            "(82, 86): reprojection: 0.434255, disparity: 0.068502\n",
            "(83, 84): reprojection: 0.138174, disparity: 0.052819\n",
            "(83, 85): reprojection: 0.296078, disparity: 0.060625\n",
            "(84, 85): reprojection: 0.173665, disparity: 0.052138\n",
            "(84, 86): reprojection: 0.258065, disparity: 0.058331\n",
            "(84, 88): reprojection: 0.498374, disparity: 0.065056\n",
            "(85, 86): reprojection: 0.165809, disparity: 0.051624\n",
            "(85, 87): reprojection: 0.282721, disparity: 0.057622\n",
            "(86, 87): reprojection: 0.233153, disparity: 0.053354\n",
            "(86, 88): reprojection: 0.331738, disparity: 0.060967\n",
            "(86, 90): reprojection: 0.493524, disparity: 0.065254\n",
            "(87, 88): reprojection: 0.177566, disparity: 0.052468\n",
            "(87, 89): reprojection: 0.280007, disparity: 0.058925\n",
            "(88, 89): reprojection: 0.141337, disparity: 0.051026\n",
            "(88, 90): reprojection: 0.308071, disparity: 0.055578\n",
            "(89, 90): reprojection: 0.214387, disparity: 0.049189\n",
            "(89, 91): reprojection: 0.421370, disparity: 0.059033\n",
            "(90, 91): reprojection: 0.332361, disparity: 0.052997\n",
            "Mean:     reprojection: 0.332361, disparity: 0.052997\n",
            "Done Validation for epoch 15 (3900 iterations)\n",
            "Epoch = 15, pairs = [[72, 88], [36, 38], [21, 22], [30, 31]], loss = 0.47214093804359436\n",
            "Epoch = 15, pairs = [[17, 18], [82, 86], [20, 28], [86, 88]], loss = 0.45040684938430786\n",
            "Epoch = 15, pairs = [[7, 8], [60, 61], [41, 42], [81, 82]], loss = 0.2239648848772049\n",
            "Epoch = 15, pairs = [[42, 44], [2, 4], [40, 44], [16, 18]], loss = 0.41182923316955566\n",
            "Epoch = 15, pairs = [[13, 15], [32, 64], [0, 16], [69, 71]], loss = 0.7033840417861938\n",
            "Epoch = 15, pairs = [[25, 26], [14, 16], [56, 72], [76, 84]], loss = 0.5277364253997803\n",
            "Epoch = 15, pairs = [[42, 46], [58, 60], [38, 42], [28, 36]], loss = 0.6471474766731262\n",
            "Epoch = 15, pairs = [[89, 91], [2, 3], [76, 77], [8, 16]], loss = 0.404661625623703\n",
            "Epoch = 15, pairs = [[43, 45], [16, 24], [28, 30], [32, 48]], loss = 0.67838454246521\n",
            "Epoch = 15, pairs = [[74, 76], [7, 9], [67, 69], [70, 74]], loss = 0.3527550995349884\n",
            "Epoch = 15, pairs = [[8, 12], [79, 81], [26, 30], [13, 14]], loss = 0.36699214577674866\n",
            "Epoch = 15, pairs = [[16, 20], [20, 22], [52, 54], [46, 50]], loss = 0.4374611973762512\n",
            "Epoch = 15, pairs = [[3, 4], [74, 75], [0, 32], [41, 43]], loss = 0.5090609192848206\n",
            "Epoch = 15, pairs = [[8, 10], [31, 33], [78, 79], [80, 88]], loss = 0.46019062399864197\n",
            "Epoch = 15, pairs = [[57, 59], [48, 64], [16, 32], [75, 77]], loss = 0.7665891051292419\n",
            "Epoch = 15, pairs = [[62, 66], [1, 3], [12, 13], [15, 17]], loss = 0.3713778257369995\n",
            "Epoch = 15, pairs = [[48, 80], [55, 57], [24, 32], [65, 67]], loss = 0.736811101436615\n",
            "Epoch = 15, pairs = [[50, 54], [78, 80], [40, 48], [28, 29]], loss = 0.4801284670829773\n",
            "Epoch = 15, pairs = [[4, 6], [48, 50], [4, 5], [50, 51]], loss = 0.2560262084007263\n",
            "Epoch = 15, pairs = [[76, 80], [57, 58], [83, 85], [80, 84]], loss = 0.4980010986328125\n",
            "Epoch = 15, pairs = [[79, 80], [77, 79], [56, 58], [62, 64]], loss = 0.43637996912002563\n",
            "Epoch = 15, pairs = [[84, 86], [73, 74], [40, 56], [85, 86]], loss = 0.6577211618423462\n",
            "Epoch = 15, pairs = [[84, 85], [22, 26], [5, 7], [66, 70]], loss = 0.383914053440094\n",
            "Epoch = 15, pairs = [[4, 12], [31, 32], [18, 20], [56, 64]], loss = 0.5324010252952576\n",
            "Epoch = 15, pairs = [[18, 22], [9, 10], [69, 70], [45, 47]], loss = 0.4194537103176117\n",
            "Epoch = 15, pairs = [[20, 24], [58, 62], [5, 6], [24, 40]], loss = 0.8356946706771851\n",
            "Epoch = 15, pairs = [[39, 41], [75, 76], [48, 56], [16, 48]], loss = 0.8899645805358887\n",
            "Epoch = 15, pairs = [[84, 88], [38, 40], [44, 48], [60, 62]], loss = 0.47292929887771606\n",
            "Epoch = 15, pairs = [[11, 12], [19, 21], [48, 52], [26, 28]], loss = 0.2901000678539276\n",
            "Epoch = 15, pairs = [[64, 68], [47, 49], [14, 15], [63, 65]], loss = 0.36981233954429626\n",
            "Epoch = 15, pairs = [[32, 34], [86, 87], [42, 43], [23, 25]], loss = 0.2755810022354126\n",
            "Epoch = 15, pairs = [[83, 84], [12, 14], [60, 64], [9, 11]], loss = 0.4163331389427185\n",
            "Epoch = 15, pairs = [[72, 80], [72, 76], [90, 91], [46, 47]], loss = 0.5378037095069885\n",
            "Epoch = 15, pairs = [[54, 55], [71, 73], [32, 40], [65, 66]], loss = 0.4743187427520752\n",
            "Epoch = 15, pairs = [[47, 48], [72, 74], [89, 90], [24, 26]], loss = 0.2880934476852417\n",
            "Epoch = 15, pairs = [[26, 27], [68, 76], [56, 60], [52, 60]], loss = 0.6528114676475525\n",
            "Epoch = 15, pairs = [[6, 8], [58, 59], [36, 40], [34, 38]], loss = 0.344109445810318\n",
            "Epoch = 15, pairs = [[64, 80], [55, 56], [45, 46], [3, 5]], loss = 0.6494258046150208\n",
            "Epoch = 15, pairs = [[88, 90], [48, 49], [43, 44], [37, 39]], loss = 0.29983019828796387\n",
            "Epoch = 15, pairs = [[11, 13], [88, 89], [76, 78], [34, 36]], loss = 0.3005395233631134\n",
            "Epoch = 15, pairs = [[32, 36], [52, 56], [12, 20], [82, 84]], loss = 0.5156540870666504\n",
            "Epoch = 15, pairs = [[30, 34], [49, 51], [59, 60], [28, 32]], loss = 0.39169198274612427\n",
            "Epoch = 15, pairs = [[61, 63], [67, 68], [29, 30], [87, 88]], loss = 0.3141633868217468\n",
            "Epoch = 15, pairs = [[10, 12], [52, 53], [35, 37], [68, 70]], loss = 0.34663450717926025\n",
            "Epoch = 15, pairs = [[4, 8], [72, 73], [54, 58], [74, 78]], loss = 0.41046708822250366\n",
            "Epoch = 15, pairs = [[0, 4], [2, 6], [6, 7], [30, 32]], loss = 0.3080623745918274\n",
            "Epoch = 15, pairs = [[12, 16], [23, 24], [86, 90], [44, 52]], loss = 0.48213720321655273\n",
            "Epoch = 15, pairs = [[27, 29], [17, 19], [59, 61], [44, 46]], loss = 0.39545005559921265\n",
            "Epoch = 15, pairs = [[1, 2], [36, 44], [68, 72], [15, 16]], loss = 0.512165904045105\n",
            "Epoch = 15, pairs = [[44, 45], [50, 52], [8, 9], [19, 20]], loss = 0.22067692875862122\n",
            "Epoch = 15, pairs = [[63, 64], [39, 40], [34, 35], [8, 24]], loss = 0.44605690240859985\n",
            "Epoch = 15, pairs = [[37, 38], [64, 72], [0, 2], [10, 14]], loss = 0.4562871754169464\n",
            "Epoch = 15, pairs = [[80, 81], [70, 71], [53, 54], [77, 78]], loss = 0.22931881248950958\n",
            "Epoch = 15, pairs = [[0, 1], [51, 52], [62, 63], [29, 31]], loss = 0.2825694680213928\n",
            "Epoch = 15, pairs = [[73, 75], [32, 33], [14, 18], [18, 19]], loss = 0.35833126306533813\n",
            "Epoch = 15, pairs = [[61, 62], [68, 69], [33, 35], [22, 24]], loss = 0.27611660957336426\n",
            "Epoch = 15, pairs = [[81, 83], [85, 87], [27, 28], [0, 8]], loss = 0.3690168261528015\n",
            "Epoch = 15, pairs = [[25, 27], [6, 10], [22, 23], [70, 72]], loss = 0.30890241265296936\n",
            "Epoch = 15, pairs = [[87, 89], [36, 37], [40, 42], [82, 83]], loss = 0.3322138786315918\n",
            "Epoch = 15, pairs = [[64, 66], [56, 57], [51, 53], [60, 68]], loss = 0.45847827196121216\n",
            "Epoch = 15, pairs = [[33, 34], [46, 48], [80, 82], [66, 67]], loss = 0.29775547981262207\n",
            "Epoch = 15, pairs = [[35, 36], [40, 41], [24, 25], [71, 72]], loss = 0.2770284414291382\n",
            "Epoch = 15, pairs = [[64, 65], [78, 82], [66, 68], [24, 28]], loss = 0.36948269605636597\n",
            "Epoch = 15, pairs = [[54, 56], [10, 11], [20, 21], [16, 17]], loss = 0.3156673312187195\n",
            "Epoch = 15, pairs = [[49, 50], [21, 23], [53, 55], [38, 39]], loss = 0.29218563437461853\n",
            "Epoch 15 took 84.54s.\n",
            "( 0,  1): reprojection: 0.234502, disparity: 0.048240\n",
            "( 0,  2): reprojection: 0.346389, disparity: 0.047544\n",
            "( 0,  4): reprojection: 0.381222, disparity: 0.055441\n",
            "( 0,  8): reprojection: 0.454871, disparity: 0.072460\n",
            "( 0, 16): reprojection: 0.694556, disparity: 0.083921\n",
            "( 0, 32): reprojection: 1.089498, disparity: 0.106266\n",
            "( 1,  2): reprojection: 0.194124, disparity: 0.042084\n",
            "( 1,  3): reprojection: 0.244450, disparity: 0.046434\n",
            "( 2,  3): reprojection: 0.106242, disparity: 0.042395\n",
            "( 2,  4): reprojection: 0.168497, disparity: 0.046512\n",
            "( 2,  6): reprojection: 0.334712, disparity: 0.056899\n",
            "( 3,  4): reprojection: 0.106650, disparity: 0.042584\n",
            "( 3,  5): reprojection: 0.192658, disparity: 0.046776\n",
            "( 4,  5): reprojection: 0.152910, disparity: 0.042741\n",
            "( 4,  6): reprojection: 0.214635, disparity: 0.045952\n",
            "( 4,  8): reprojection: 0.276935, disparity: 0.057409\n",
            "( 4, 12): reprojection: 0.403347, disparity: 0.069942\n",
            "( 5,  6): reprojection: 0.119530, disparity: 0.041883\n",
            "( 5,  7): reprojection: 0.148460, disparity: 0.045883\n",
            "( 6,  7): reprojection: 0.094470, disparity: 0.041690\n",
            "( 6,  8): reprojection: 0.157138, disparity: 0.046687\n",
            "( 6, 10): reprojection: 0.386754, disparity: 0.057276\n",
            "( 7,  8): reprojection: 0.086818, disparity: 0.041714\n",
            "( 7,  9): reprojection: 0.189742, disparity: 0.046931\n",
            "( 8,  9): reprojection: 0.161595, disparity: 0.043145\n",
            "( 8, 10): reprojection: 0.360301, disparity: 0.049161\n",
            "( 8, 12): reprojection: 0.510076, disparity: 0.063179\n",
            "( 8, 16): reprojection: 0.604217, disparity: 0.071287\n",
            "( 8, 24): reprojection: 0.714803, disparity: 0.077692\n",
            "( 9, 10): reprojection: 0.250862, disparity: 0.043514\n",
            "( 9, 11): reprojection: 0.374596, disparity: 0.051320\n",
            "(10, 11): reprojection: 0.163449, disparity: 0.040435\n",
            "(10, 12): reprojection: 0.207296, disparity: 0.046394\n",
            "(10, 14): reprojection: 0.336944, disparity: 0.061135\n",
            "(11, 12): reprojection: 0.134904, disparity: 0.039474\n",
            "(11, 13): reprojection: 0.211416, disparity: 0.048657\n",
            "(12, 13): reprojection: 0.134957, disparity: 0.041986\n",
            "(12, 14): reprojection: 0.261955, disparity: 0.050946\n",
            "(12, 16): reprojection: 0.410942, disparity: 0.070107\n",
            "(12, 20): reprojection: 0.647900, disparity: 0.068334\n",
            "(13, 14): reprojection: 0.181629, disparity: 0.044794\n",
            "(13, 15): reprojection: 0.230971, disparity: 0.051225\n",
            "(14, 15): reprojection: 0.184566, disparity: 0.044209\n",
            "(14, 16): reprojection: 0.413077, disparity: 0.052296\n",
            "(14, 18): reprojection: 0.330182, disparity: 0.060720\n",
            "(15, 16): reprojection: 0.290814, disparity: 0.044893\n",
            "(15, 17): reprojection: 0.364210, disparity: 0.054614\n",
            "(16, 17): reprojection: 0.133840, disparity: 0.045634\n",
            "(16, 18): reprojection: 0.328959, disparity: 0.053776\n",
            "(16, 20): reprojection: 0.560052, disparity: 0.064316\n",
            "(16, 24): reprojection: 0.816418, disparity: 0.075409\n",
            "(16, 32): reprojection: 1.161562, disparity: 0.110622\n",
            "(16, 48): reprojection: 1.310633, disparity: 0.219756\n",
            "(17, 18): reprojection: 0.323180, disparity: 0.047228\n",
            "(17, 19): reprojection: 0.495937, disparity: 0.056767\n",
            "(18, 19): reprojection: 0.215075, disparity: 0.045508\n",
            "(18, 20): reprojection: 0.290902, disparity: 0.053219\n",
            "(18, 22): reprojection: 0.495863, disparity: 0.061789\n",
            "(19, 20): reprojection: 0.104608, disparity: 0.045043\n",
            "(19, 21): reprojection: 0.250028, disparity: 0.052588\n",
            "(20, 21): reprojection: 0.198805, disparity: 0.046134\n",
            "(20, 22): reprojection: 0.298663, disparity: 0.054803\n",
            "(20, 24): reprojection: 0.366229, disparity: 0.059996\n",
            "(20, 28): reprojection: 0.593005, disparity: 0.084684\n",
            "(21, 22): reprojection: 0.155943, disparity: 0.046373\n",
            "(21, 23): reprojection: 0.229309, disparity: 0.054130\n",
            "(22, 23): reprojection: 0.117064, disparity: 0.046537\n",
            "(22, 24): reprojection: 0.163474, disparity: 0.054730\n",
            "(22, 26): reprojection: 0.286337, disparity: 0.067884\n",
            "(23, 24): reprojection: 0.104125, disparity: 0.047175\n",
            "(23, 25): reprojection: 0.185661, disparity: 0.056436\n",
            "(24, 25): reprojection: 0.137605, disparity: 0.049395\n",
            "(24, 26): reprojection: 0.175254, disparity: 0.057434\n",
            "(24, 28): reprojection: 0.273794, disparity: 0.072095\n",
            "(24, 32): reprojection: 0.556857, disparity: 0.094111\n",
            "(24, 40): reprojection: 0.924423, disparity: 0.153138\n",
            "(25, 26): reprojection: 0.122796, disparity: 0.051370\n",
            "(25, 27): reprojection: 0.195978, disparity: 0.058112\n",
            "(26, 27): reprojection: 0.132951, disparity: 0.050509\n",
            "(26, 28): reprojection: 0.162617, disparity: 0.058845\n",
            "(26, 30): reprojection: 0.312212, disparity: 0.070542\n",
            "(27, 28): reprojection: 0.147713, disparity: 0.048128\n",
            "(27, 29): reprojection: 0.264422, disparity: 0.057306\n",
            "(28, 29): reprojection: 0.188656, disparity: 0.051864\n",
            "(28, 30): reprojection: 0.244270, disparity: 0.059032\n",
            "(28, 32): reprojection: 0.427940, disparity: 0.067766\n",
            "(28, 36): reprojection: 0.676473, disparity: 0.101702\n",
            "(29, 30): reprojection: 0.132304, disparity: 0.047292\n",
            "(29, 31): reprojection: 0.269237, disparity: 0.053102\n",
            "(30, 31): reprojection: 0.189500, disparity: 0.048192\n",
            "(30, 32): reprojection: 0.262010, disparity: 0.056089\n",
            "(30, 34): reprojection: 0.480232, disparity: 0.070835\n",
            "(31, 32): reprojection: 0.131675, disparity: 0.049323\n",
            "(31, 33): reprojection: 0.300847, disparity: 0.058425\n",
            "(32, 33): reprojection: 0.246591, disparity: 0.050335\n",
            "(32, 34): reprojection: 0.248309, disparity: 0.056996\n",
            "(32, 36): reprojection: 0.366064, disparity: 0.077806\n",
            "(32, 40): reprojection: 0.492798, disparity: 0.094549\n",
            "(32, 48): reprojection: 0.968730, disparity: 0.155184\n",
            "(32, 64): reprojection: 1.123869, disparity: 0.134780\n",
            "(33, 34): reprojection: 0.154548, disparity: 0.047854\n",
            "(33, 35): reprojection: 0.270687, disparity: 0.054769\n",
            "(34, 35): reprojection: 0.193509, disparity: 0.046774\n",
            "(34, 36): reprojection: 0.293529, disparity: 0.058378\n",
            "(34, 38): reprojection: 0.359616, disparity: 0.064768\n",
            "(35, 36): reprojection: 0.236785, disparity: 0.051606\n",
            "(35, 37): reprojection: 0.270484, disparity: 0.062433\n",
            "(36, 37): reprojection: 0.257905, disparity: 0.054151\n",
            "(36, 38): reprojection: 0.365727, disparity: 0.057871\n",
            "(36, 40): reprojection: 0.340468, disparity: 0.065926\n",
            "(36, 44): reprojection: 0.676310, disparity: 0.117453\n",
            "(37, 38): reprojection: 0.217165, disparity: 0.049282\n",
            "(37, 39): reprojection: 0.246173, disparity: 0.054194\n",
            "(38, 39): reprojection: 0.198992, disparity: 0.051757\n",
            "(38, 40): reprojection: 0.379440, disparity: 0.061596\n",
            "(38, 42): reprojection: 0.669580, disparity: 0.083097\n",
            "(39, 40): reprojection: 0.286225, disparity: 0.053857\n",
            "(39, 41): reprojection: 0.499438, disparity: 0.066654\n",
            "(40, 41): reprojection: 0.292080, disparity: 0.057542\n",
            "(40, 42): reprojection: 0.417301, disparity: 0.071747\n",
            "(40, 44): reprojection: 0.551878, disparity: 0.106666\n",
            "(40, 48): reprojection: 0.749546, disparity: 0.123875\n",
            "(40, 56): reprojection: 0.733161, disparity: 0.130364\n",
            "(41, 42): reprojection: 0.196679, disparity: 0.056240\n",
            "(41, 43): reprojection: 0.299082, disparity: 0.071787\n",
            "(42, 43): reprojection: 0.174093, disparity: 0.057662\n",
            "(42, 44): reprojection: 0.323849, disparity: 0.082634\n",
            "(42, 46): reprojection: 0.515843, disparity: 0.083696\n",
            "(43, 44): reprojection: 0.209739, disparity: 0.059866\n",
            "(43, 45): reprojection: 0.304733, disparity: 0.064744\n",
            "(44, 45): reprojection: 0.153497, disparity: 0.053759\n",
            "(44, 46): reprojection: 0.282794, disparity: 0.062306\n",
            "(44, 48): reprojection: 0.404839, disparity: 0.083857\n",
            "(44, 52): reprojection: 0.560045, disparity: 0.091005\n",
            "(45, 46): reprojection: 0.201171, disparity: 0.055647\n",
            "(45, 47): reprojection: 0.323076, disparity: 0.073268\n",
            "(46, 47): reprojection: 0.190409, disparity: 0.058714\n",
            "(46, 48): reprojection: 0.284250, disparity: 0.075492\n",
            "(46, 50): reprojection: 0.390350, disparity: 0.084041\n",
            "(47, 48): reprojection: 0.199020, disparity: 0.056639\n",
            "(47, 49): reprojection: 0.256418, disparity: 0.068210\n",
            "(48, 49): reprojection: 0.172158, disparity: 0.061513\n",
            "(48, 50): reprojection: 0.252660, disparity: 0.068209\n",
            "(48, 52): reprojection: 0.339596, disparity: 0.078874\n",
            "(48, 56): reprojection: 0.645904, disparity: 0.090967\n",
            "(48, 64): reprojection: 0.896119, disparity: 0.134474\n",
            "(48, 80): reprojection: 1.127032, disparity: 0.095028\n",
            "(49, 50): reprojection: 0.152529, disparity: 0.057039\n",
            "(49, 51): reprojection: 0.234696, disparity: 0.063906\n",
            "(50, 51): reprojection: 0.148230, disparity: 0.056529\n",
            "(50, 52): reprojection: 0.236005, disparity: 0.062420\n",
            "(50, 54): reprojection: 0.371435, disparity: 0.075760\n",
            "(51, 52): reprojection: 0.160460, disparity: 0.055193\n",
            "(51, 53): reprojection: 0.283838, disparity: 0.061302\n",
            "(52, 53): reprojection: 0.167532, disparity: 0.054990\n",
            "(52, 54): reprojection: 0.245568, disparity: 0.065668\n",
            "(52, 56): reprojection: 0.513342, disparity: 0.065117\n",
            "(52, 60): reprojection: 0.587229, disparity: 0.107850\n",
            "(53, 54): reprojection: 0.140203, disparity: 0.057767\n",
            "(53, 55): reprojection: 0.325795, disparity: 0.063743\n",
            "(54, 55): reprojection: 0.294018, disparity: 0.055546\n",
            "(54, 56): reprojection: 0.549937, disparity: 0.066586\n",
            "(54, 58): reprojection: 0.384279, disparity: 0.067690\n",
            "(55, 56): reprojection: 0.342384, disparity: 0.061346\n",
            "(55, 57): reprojection: 0.382854, disparity: 0.064029\n",
            "(56, 57): reprojection: 0.301310, disparity: 0.062290\n",
            "(56, 58): reprojection: 0.535567, disparity: 0.077778\n",
            "(56, 60): reprojection: 0.649013, disparity: 0.111149\n",
            "(56, 64): reprojection: 0.816570, disparity: 0.117875\n",
            "(56, 72): reprojection: 0.921349, disparity: 0.106416\n",
            "(57, 58): reprojection: 0.348015, disparity: 0.064904\n",
            "(57, 59): reprojection: 0.463485, disparity: 0.081917\n",
            "(58, 59): reprojection: 0.234269, disparity: 0.067190\n",
            "(58, 60): reprojection: 0.341239, disparity: 0.076632\n",
            "(58, 62): reprojection: 0.415337, disparity: 0.098531\n",
            "(59, 60): reprojection: 0.181407, disparity: 0.066159\n",
            "(59, 61): reprojection: 0.291412, disparity: 0.071457\n",
            "(60, 61): reprojection: 0.215526, disparity: 0.066868\n",
            "(60, 62): reprojection: 0.288836, disparity: 0.070830\n",
            "(60, 64): reprojection: 0.456912, disparity: 0.080325\n",
            "(60, 68): reprojection: 0.583198, disparity: 0.099780\n",
            "(61, 62): reprojection: 0.190486, disparity: 0.065765\n",
            "(61, 63): reprojection: 0.283865, disparity: 0.071069\n",
            "(62, 63): reprojection: 0.220705, disparity: 0.061495\n",
            "(62, 64): reprojection: 0.323097, disparity: 0.068471\n",
            "(62, 66): reprojection: 0.462985, disparity: 0.084338\n",
            "(63, 64): reprojection: 0.214450, disparity: 0.064699\n",
            "(63, 65): reprojection: 0.323167, disparity: 0.066464\n",
            "(64, 65): reprojection: 0.209479, disparity: 0.065476\n",
            "(64, 66): reprojection: 0.343559, disparity: 0.064950\n",
            "(64, 68): reprojection: 0.431796, disparity: 0.078754\n",
            "(64, 72): reprojection: 0.655995, disparity: 0.097344\n",
            "(64, 80): reprojection: 0.970551, disparity: 0.096798\n",
            "(65, 66): reprojection: 0.257058, disparity: 0.060170\n",
            "(65, 67): reprojection: 0.393837, disparity: 0.066474\n",
            "(66, 67): reprojection: 0.252212, disparity: 0.062752\n",
            "(66, 68): reprojection: 0.359612, disparity: 0.063239\n",
            "(66, 70): reprojection: 0.430938, disparity: 0.073246\n",
            "(67, 68): reprojection: 0.328258, disparity: 0.061193\n",
            "(67, 69): reprojection: 0.297752, disparity: 0.063084\n",
            "(68, 69): reprojection: 0.192244, disparity: 0.057568\n",
            "(68, 70): reprojection: 0.419740, disparity: 0.069808\n",
            "(68, 72): reprojection: 0.494472, disparity: 0.078919\n",
            "(68, 76): reprojection: 0.648405, disparity: 0.084888\n",
            "(69, 70): reprojection: 0.292065, disparity: 0.060181\n",
            "(69, 71): reprojection: 0.403795, disparity: 0.069440\n",
            "(70, 71): reprojection: 0.205466, disparity: 0.059089\n",
            "(70, 72): reprojection: 0.263732, disparity: 0.060871\n",
            "(70, 74): reprojection: 0.395698, disparity: 0.069596\n",
            "(71, 72): reprojection: 0.224030, disparity: 0.057897\n",
            "(71, 73): reprojection: 0.331156, disparity: 0.064341\n",
            "(72, 73): reprojection: 0.218854, disparity: 0.060398\n",
            "(72, 74): reprojection: 0.288676, disparity: 0.064505\n",
            "(72, 76): reprojection: 0.388243, disparity: 0.074426\n",
            "(72, 80): reprojection: 0.620143, disparity: 0.089095\n",
            "(72, 88): reprojection: 0.831799, disparity: 0.096829\n",
            "(73, 74): reprojection: 0.309182, disparity: 0.059342\n",
            "(73, 75): reprojection: 0.422967, disparity: 0.062793\n",
            "(74, 75): reprojection: 0.235840, disparity: 0.057334\n",
            "(74, 76): reprojection: 0.270463, disparity: 0.059911\n",
            "(74, 78): reprojection: 0.401429, disparity: 0.068377\n",
            "(75, 76): reprojection: 0.225961, disparity: 0.055931\n",
            "(75, 77): reprojection: 0.350812, disparity: 0.061210\n",
            "(76, 77): reprojection: 0.229695, disparity: 0.055740\n",
            "(76, 78): reprojection: 0.348848, disparity: 0.058204\n",
            "(76, 80): reprojection: 0.412502, disparity: 0.066493\n",
            "(76, 84): reprojection: 0.596165, disparity: 0.079174\n",
            "(77, 78): reprojection: 0.230259, disparity: 0.055811\n",
            "(77, 79): reprojection: 0.252533, disparity: 0.058320\n",
            "(78, 79): reprojection: 0.192861, disparity: 0.057027\n",
            "(78, 80): reprojection: 0.236157, disparity: 0.060615\n",
            "(78, 82): reprojection: 0.370482, disparity: 0.066049\n",
            "(79, 80): reprojection: 0.142785, disparity: 0.054427\n",
            "(79, 81): reprojection: 0.235212, disparity: 0.057736\n",
            "(80, 81): reprojection: 0.146351, disparity: 0.053152\n",
            "(80, 82): reprojection: 0.257981, disparity: 0.057490\n",
            "(80, 84): reprojection: 0.441436, disparity: 0.061876\n",
            "(80, 88): reprojection: 0.662793, disparity: 0.080654\n",
            "(81, 82): reprojection: 0.180776, disparity: 0.052719\n",
            "(81, 83): reprojection: 0.277783, disparity: 0.057476\n",
            "(82, 83): reprojection: 0.143853, disparity: 0.052397\n",
            "(82, 84): reprojection: 0.251203, disparity: 0.057333\n",
            "(82, 86): reprojection: 0.417574, disparity: 0.066573\n",
            "(83, 84): reprojection: 0.144400, disparity: 0.051931\n",
            "(83, 85): reprojection: 0.295839, disparity: 0.061221\n",
            "(84, 85): reprojection: 0.169805, disparity: 0.052034\n",
            "(84, 86): reprojection: 0.240338, disparity: 0.059472\n",
            "(84, 88): reprojection: 0.465946, disparity: 0.065769\n",
            "(85, 86): reprojection: 0.158825, disparity: 0.051250\n",
            "(85, 87): reprojection: 0.264636, disparity: 0.055162\n",
            "(86, 87): reprojection: 0.228234, disparity: 0.052751\n",
            "(86, 88): reprojection: 0.322168, disparity: 0.060880\n",
            "(86, 90): reprojection: 0.488869, disparity: 0.064895\n",
            "(87, 88): reprojection: 0.173491, disparity: 0.051149\n",
            "(87, 89): reprojection: 0.277785, disparity: 0.056796\n",
            "(88, 89): reprojection: 0.143602, disparity: 0.048968\n",
            "(88, 90): reprojection: 0.309394, disparity: 0.055957\n",
            "(89, 90): reprojection: 0.213378, disparity: 0.049063\n",
            "(89, 91): reprojection: 0.413197, disparity: 0.067359\n",
            "(90, 91): reprojection: 0.328304, disparity: 0.056403\n",
            "Mean:     reprojection: 0.328304, disparity: 0.056403\n",
            "Done Validation for epoch 16 (4160 iterations)\n",
            "Epoch = 16, pairs = [[2, 4], [42, 46], [83, 84], [44, 52]], loss = 0.4174553155899048\n",
            "Epoch = 16, pairs = [[70, 71], [58, 59], [20, 24], [16, 32]], loss = 0.5326062440872192\n",
            "Epoch = 16, pairs = [[39, 41], [60, 62], [72, 73], [46, 50]], loss = 0.41707056760787964\n",
            "Epoch = 16, pairs = [[80, 84], [3, 5], [0, 1], [32, 40]], loss = 0.38485684990882874\n",
            "Epoch = 16, pairs = [[17, 19], [0, 4], [13, 15], [21, 22]], loss = 0.35510265827178955\n",
            "Epoch = 16, pairs = [[14, 16], [59, 60], [85, 87], [65, 66]], loss = 0.3534756302833557\n",
            "Epoch = 16, pairs = [[77, 79], [48, 80], [74, 78], [64, 72]], loss = 0.8184388875961304\n",
            "Epoch = 16, pairs = [[37, 38], [69, 71], [34, 35], [9, 11]], loss = 0.3501283526420593\n",
            "Epoch = 16, pairs = [[48, 64], [14, 15], [4, 12], [66, 70]], loss = 0.665706217288971\n",
            "Epoch = 16, pairs = [[44, 48], [90, 91], [0, 32], [79, 81]], loss = 0.9130202531814575\n",
            "Epoch = 16, pairs = [[20, 28], [55, 56], [28, 36], [16, 24]], loss = 0.9029487371444702\n",
            "Epoch = 16, pairs = [[46, 48], [82, 83], [0, 16], [77, 78]], loss = 0.42122772336006165\n",
            "Epoch = 16, pairs = [[89, 90], [8, 10], [18, 19], [84, 86]], loss = 0.3376050591468811\n",
            "Epoch = 16, pairs = [[8, 9], [2, 3], [56, 60], [82, 84]], loss = 0.39981216192245483\n",
            "Epoch = 16, pairs = [[68, 72], [47, 48], [60, 64], [4, 5]], loss = 0.5022392868995667\n",
            "Epoch = 16, pairs = [[86, 88], [48, 52], [58, 60], [17, 18]], loss = 0.493362158536911\n",
            "Epoch = 16, pairs = [[12, 14], [44, 46], [36, 38], [2, 6]], loss = 0.4120281934738159\n",
            "Epoch = 16, pairs = [[76, 78], [60, 61], [72, 88], [57, 58]], loss = 0.6538699865341187\n",
            "Epoch = 16, pairs = [[39, 40], [63, 65], [68, 70], [24, 32]], loss = 0.526974618434906\n",
            "Epoch = 16, pairs = [[87, 89], [22, 26], [72, 80], [78, 79]], loss = 0.4430387616157532\n",
            "Epoch = 16, pairs = [[76, 84], [20, 21], [78, 80], [62, 64]], loss = 0.4179837703704834\n",
            "Epoch = 16, pairs = [[67, 69], [12, 20], [80, 81], [29, 31]], loss = 0.45898568630218506\n",
            "Epoch = 16, pairs = [[85, 86], [88, 90], [43, 45], [24, 25]], loss = 0.3066471517086029\n",
            "Epoch = 16, pairs = [[52, 53], [72, 74], [67, 68], [48, 49]], loss = 0.3462950885295868\n",
            "Epoch = 16, pairs = [[80, 82], [15, 16], [8, 16], [40, 41]], loss = 0.5121345520019531\n",
            "Epoch = 16, pairs = [[19, 20], [88, 89], [49, 51], [51, 53]], loss = 0.28805285692214966\n",
            "Epoch = 16, pairs = [[0, 2], [48, 56], [45, 46], [36, 40]], loss = 0.6173442602157593\n",
            "Epoch = 16, pairs = [[45, 47], [56, 64], [26, 30], [14, 18]], loss = 0.6730073094367981\n",
            "Epoch = 16, pairs = [[8, 24], [42, 43], [78, 82], [65, 67]], loss = 0.5453366041183472\n",
            "Epoch = 16, pairs = [[66, 67], [10, 14], [52, 60], [56, 72]], loss = 0.626550555229187\n",
            "Epoch = 16, pairs = [[84, 85], [55, 57], [6, 8], [64, 68]], loss = 0.3586161136627197\n",
            "Epoch = 16, pairs = [[46, 47], [34, 38], [68, 69], [44, 45]], loss = 0.3031066656112671\n",
            "Epoch = 16, pairs = [[28, 32], [26, 28], [26, 27], [60, 68]], loss = 0.5084588527679443\n",
            "Epoch = 16, pairs = [[3, 4], [40, 48], [7, 8], [30, 31]], loss = 0.40800920128822327\n",
            "Epoch = 16, pairs = [[10, 11], [54, 55], [64, 66], [53, 54]], loss = 0.317413866519928\n",
            "Epoch = 16, pairs = [[74, 76], [61, 62], [11, 13], [42, 44]], loss = 0.33302754163742065\n",
            "Epoch = 16, pairs = [[16, 17], [74, 75], [6, 10], [89, 91]], loss = 0.3807939887046814\n",
            "Epoch = 16, pairs = [[34, 36], [57, 59], [30, 32], [28, 29]], loss = 0.36913037300109863\n",
            "Epoch = 16, pairs = [[10, 12], [29, 30], [16, 20], [31, 33]], loss = 0.36848098039627075\n",
            "Epoch = 16, pairs = [[35, 37], [54, 58], [50, 52], [50, 51]], loss = 0.37380456924438477\n",
            "Epoch = 16, pairs = [[18, 20], [43, 44], [12, 16], [58, 62]], loss = 0.4320192039012909\n",
            "Epoch = 16, pairs = [[32, 33], [18, 22], [38, 40], [40, 42]], loss = 0.4722406268119812\n",
            "Epoch = 16, pairs = [[35, 36], [33, 34], [11, 12], [52, 56]], loss = 0.32132628560066223\n",
            "Epoch = 16, pairs = [[40, 56], [71, 73], [73, 75], [15, 17]], loss = 0.5978502035140991\n",
            "Epoch = 16, pairs = [[73, 74], [25, 27], [51, 52], [16, 18]], loss = 0.3159773349761963\n",
            "Epoch = 16, pairs = [[41, 42], [48, 50], [86, 90], [87, 88]], loss = 0.33520573377609253\n",
            "Epoch = 16, pairs = [[61, 63], [7, 9], [12, 13], [66, 68]], loss = 0.30823662877082825\n",
            "Epoch = 16, pairs = [[68, 76], [76, 80], [64, 65], [27, 28]], loss = 0.4202467203140259\n",
            "Epoch = 16, pairs = [[32, 36], [72, 76], [23, 24], [22, 23]], loss = 0.2884526252746582\n",
            "Epoch = 16, pairs = [[5, 7], [24, 40], [4, 6], [56, 57]], loss = 0.47466427087783813\n",
            "Epoch = 16, pairs = [[32, 34], [32, 48], [36, 44], [20, 22]], loss = 0.6446496248245239\n",
            "Epoch = 16, pairs = [[27, 29], [81, 83], [8, 12], [13, 14]], loss = 0.37392619252204895\n",
            "Epoch = 16, pairs = [[50, 54], [6, 7], [75, 77], [30, 34]], loss = 0.3768468499183655\n",
            "Epoch = 16, pairs = [[38, 42], [31, 32], [38, 39], [64, 80]], loss = 0.5371273756027222\n",
            "Epoch = 16, pairs = [[1, 2], [9, 10], [24, 26], [54, 56]], loss = 0.3469994366168976\n",
            "Epoch = 16, pairs = [[71, 72], [84, 88], [70, 72], [4, 8]], loss = 0.3989567160606384\n",
            "Epoch = 16, pairs = [[63, 64], [1, 3], [23, 25], [53, 55]], loss = 0.3026188313961029\n",
            "Epoch = 16, pairs = [[5, 6], [0, 8], [52, 54], [86, 87]], loss = 0.3473505973815918\n",
            "Epoch = 16, pairs = [[40, 44], [62, 66], [25, 26], [36, 37]], loss = 0.40825438499450684\n",
            "Epoch = 16, pairs = [[37, 39], [32, 64], [59, 61], [79, 80]], loss = 0.5972578525543213\n",
            "Epoch = 16, pairs = [[49, 50], [76, 77], [47, 49], [19, 21]], loss = 0.2829616963863373\n",
            "Epoch = 16, pairs = [[83, 85], [22, 24], [75, 76], [81, 82]], loss = 0.2890528738498688\n",
            "Epoch = 16, pairs = [[16, 48], [21, 23], [24, 28], [56, 58]], loss = 0.6544917821884155\n",
            "Epoch = 16, pairs = [[28, 30], [82, 86], [62, 63], [80, 88]], loss = 0.47870558500289917\n",
            "Epoch = 16, pairs = [[69, 70], [41, 43], [33, 35], [70, 74]], loss = 0.3654775023460388\n",
            "Epoch 16 took 85.32s.\n",
            "( 0,  1): reprojection: 0.235469, disparity: 0.049685\n",
            "( 0,  2): reprojection: 0.347900, disparity: 0.050715\n",
            "( 0,  4): reprojection: 0.383837, disparity: 0.060213\n",
            "( 0,  8): reprojection: 0.447352, disparity: 0.073398\n",
            "( 0, 16): reprojection: 0.768547, disparity: 0.077506\n",
            "( 0, 32): reprojection: 1.163990, disparity: 0.119158\n",
            "( 1,  2): reprojection: 0.193106, disparity: 0.043672\n",
            "( 1,  3): reprojection: 0.243404, disparity: 0.048486\n",
            "( 2,  3): reprojection: 0.108440, disparity: 0.044042\n",
            "( 2,  4): reprojection: 0.170601, disparity: 0.050208\n",
            "( 2,  6): reprojection: 0.338324, disparity: 0.060312\n",
            "( 3,  4): reprojection: 0.107880, disparity: 0.045080\n",
            "( 3,  5): reprojection: 0.193550, disparity: 0.051407\n",
            "( 4,  5): reprojection: 0.153295, disparity: 0.045447\n",
            "( 4,  6): reprojection: 0.214860, disparity: 0.048096\n",
            "( 4,  8): reprojection: 0.271270, disparity: 0.061372\n",
            "( 4, 12): reprojection: 0.405325, disparity: 0.070952\n",
            "( 5,  6): reprojection: 0.119042, disparity: 0.043908\n",
            "( 5,  7): reprojection: 0.150683, disparity: 0.048396\n",
            "( 6,  7): reprojection: 0.095081, disparity: 0.043614\n",
            "( 6,  8): reprojection: 0.157224, disparity: 0.050469\n",
            "( 6, 10): reprojection: 0.387455, disparity: 0.058968\n",
            "( 7,  8): reprojection: 0.087568, disparity: 0.044690\n",
            "( 7,  9): reprojection: 0.188889, disparity: 0.049936\n",
            "( 8,  9): reprojection: 0.163736, disparity: 0.044857\n",
            "( 8, 10): reprojection: 0.366368, disparity: 0.050428\n",
            "( 8, 12): reprojection: 0.515529, disparity: 0.060011\n",
            "( 8, 16): reprojection: 0.665854, disparity: 0.076960\n",
            "( 8, 24): reprojection: 0.836083, disparity: 0.101311\n",
            "( 9, 10): reprojection: 0.253886, disparity: 0.045404\n",
            "( 9, 11): reprojection: 0.383201, disparity: 0.051579\n",
            "(10, 11): reprojection: 0.168158, disparity: 0.041604\n",
            "(10, 12): reprojection: 0.213563, disparity: 0.048726\n",
            "(10, 14): reprojection: 0.352852, disparity: 0.064912\n",
            "(11, 12): reprojection: 0.137479, disparity: 0.041431\n",
            "(11, 13): reprojection: 0.219517, disparity: 0.051160\n",
            "(12, 13): reprojection: 0.144088, disparity: 0.042709\n",
            "(12, 14): reprojection: 0.274660, disparity: 0.052297\n",
            "(12, 16): reprojection: 0.450122, disparity: 0.071351\n",
            "(12, 20): reprojection: 0.716108, disparity: 0.078367\n",
            "(13, 14): reprojection: 0.183297, disparity: 0.046296\n",
            "(13, 15): reprojection: 0.248945, disparity: 0.052818\n",
            "(14, 15): reprojection: 0.193979, disparity: 0.045481\n",
            "(14, 16): reprojection: 0.429934, disparity: 0.054618\n",
            "(14, 18): reprojection: 0.366174, disparity: 0.065044\n",
            "(15, 16): reprojection: 0.297434, disparity: 0.047365\n",
            "(15, 17): reprojection: 0.378389, disparity: 0.057191\n",
            "(16, 17): reprojection: 0.140782, disparity: 0.046195\n",
            "(16, 18): reprojection: 0.328905, disparity: 0.056879\n",
            "(16, 20): reprojection: 0.552299, disparity: 0.066913\n",
            "(16, 24): reprojection: 0.824895, disparity: 0.081787\n",
            "(16, 32): reprojection: 1.239154, disparity: 0.110870\n",
            "(16, 48): reprojection: 1.237323, disparity: 0.127451\n",
            "(17, 18): reprojection: 0.323712, disparity: 0.049431\n",
            "(17, 19): reprojection: 0.491810, disparity: 0.059051\n",
            "(18, 19): reprojection: 0.217002, disparity: 0.047399\n",
            "(18, 20): reprojection: 0.289758, disparity: 0.055844\n",
            "(18, 22): reprojection: 0.505350, disparity: 0.064121\n",
            "(19, 20): reprojection: 0.110246, disparity: 0.046591\n",
            "(19, 21): reprojection: 0.249918, disparity: 0.055541\n",
            "(20, 21): reprojection: 0.198697, disparity: 0.047902\n",
            "(20, 22): reprojection: 0.300487, disparity: 0.058325\n",
            "(20, 24): reprojection: 0.379846, disparity: 0.065876\n",
            "(20, 28): reprojection: 0.645671, disparity: 0.086366\n",
            "(21, 22): reprojection: 0.161547, disparity: 0.048322\n",
            "(21, 23): reprojection: 0.246128, disparity: 0.056267\n",
            "(22, 23): reprojection: 0.125908, disparity: 0.048759\n",
            "(22, 24): reprojection: 0.166717, disparity: 0.058723\n",
            "(22, 26): reprojection: 0.303249, disparity: 0.069853\n",
            "(23, 24): reprojection: 0.101609, disparity: 0.050228\n",
            "(23, 25): reprojection: 0.180371, disparity: 0.059113\n",
            "(24, 25): reprojection: 0.137589, disparity: 0.051142\n",
            "(24, 26): reprojection: 0.177161, disparity: 0.058514\n",
            "(24, 28): reprojection: 0.294248, disparity: 0.070522\n",
            "(24, 32): reprojection: 0.614689, disparity: 0.085628\n",
            "(24, 40): reprojection: 0.892291, disparity: 0.139859\n",
            "(25, 26): reprojection: 0.119175, disparity: 0.053144\n",
            "(25, 27): reprojection: 0.206840, disparity: 0.060530\n",
            "(26, 27): reprojection: 0.144965, disparity: 0.052676\n",
            "(26, 28): reprojection: 0.172089, disparity: 0.062416\n",
            "(26, 30): reprojection: 0.335061, disparity: 0.073470\n",
            "(27, 28): reprojection: 0.144974, disparity: 0.050918\n",
            "(27, 29): reprojection: 0.261811, disparity: 0.059385\n",
            "(28, 29): reprojection: 0.188210, disparity: 0.053178\n",
            "(28, 30): reprojection: 0.249039, disparity: 0.061463\n",
            "(28, 32): reprojection: 0.453008, disparity: 0.070418\n",
            "(28, 36): reprojection: 0.727682, disparity: 0.090577\n",
            "(29, 30): reprojection: 0.138596, disparity: 0.050685\n",
            "(29, 31): reprojection: 0.277816, disparity: 0.057802\n",
            "(30, 31): reprojection: 0.195986, disparity: 0.050989\n",
            "(30, 32): reprojection: 0.272342, disparity: 0.059059\n",
            "(30, 34): reprojection: 0.484208, disparity: 0.073861\n",
            "(31, 32): reprojection: 0.130710, disparity: 0.050481\n",
            "(31, 33): reprojection: 0.305658, disparity: 0.060315\n",
            "(32, 33): reprojection: 0.250556, disparity: 0.055489\n",
            "(32, 34): reprojection: 0.255777, disparity: 0.058888\n",
            "(32, 36): reprojection: 0.359654, disparity: 0.073383\n",
            "(32, 40): reprojection: 0.528108, disparity: 0.101647\n",
            "(32, 48): reprojection: 0.962034, disparity: 0.129898\n",
            "(32, 64): reprojection: 1.166191, disparity: 0.139324\n",
            "(33, 34): reprojection: 0.155013, disparity: 0.050254\n",
            "(33, 35): reprojection: 0.259132, disparity: 0.057937\n",
            "(34, 35): reprojection: 0.188030, disparity: 0.051570\n",
            "(34, 36): reprojection: 0.285042, disparity: 0.060325\n",
            "(34, 38): reprojection: 0.375694, disparity: 0.072545\n",
            "(35, 36): reprojection: 0.239252, disparity: 0.053642\n",
            "(35, 37): reprojection: 0.281707, disparity: 0.065131\n",
            "(36, 37): reprojection: 0.259122, disparity: 0.057561\n",
            "(36, 38): reprojection: 0.377888, disparity: 0.064565\n",
            "(36, 40): reprojection: 0.337039, disparity: 0.077424\n",
            "(36, 44): reprojection: 0.673482, disparity: 0.106222\n",
            "(37, 38): reprojection: 0.220807, disparity: 0.052732\n",
            "(37, 39): reprojection: 0.241254, disparity: 0.060639\n",
            "(38, 39): reprojection: 0.197976, disparity: 0.053919\n",
            "(38, 40): reprojection: 0.378241, disparity: 0.066134\n",
            "(38, 42): reprojection: 0.671368, disparity: 0.082352\n",
            "(39, 40): reprojection: 0.286075, disparity: 0.055549\n",
            "(39, 41): reprojection: 0.501835, disparity: 0.067000\n",
            "(40, 41): reprojection: 0.295733, disparity: 0.057603\n",
            "(40, 42): reprojection: 0.425846, disparity: 0.067582\n",
            "(40, 44): reprojection: 0.552130, disparity: 0.081999\n",
            "(40, 48): reprojection: 0.751535, disparity: 0.093557\n",
            "(40, 56): reprojection: 0.762153, disparity: 0.126536\n",
            "(41, 42): reprojection: 0.197811, disparity: 0.056261\n",
            "(41, 43): reprojection: 0.296747, disparity: 0.068387\n",
            "(42, 43): reprojection: 0.176407, disparity: 0.058340\n",
            "(42, 44): reprojection: 0.322472, disparity: 0.068799\n",
            "(42, 46): reprojection: 0.511102, disparity: 0.076185\n",
            "(43, 44): reprojection: 0.205644, disparity: 0.054923\n",
            "(43, 45): reprojection: 0.298713, disparity: 0.067547\n",
            "(44, 45): reprojection: 0.150145, disparity: 0.056314\n",
            "(44, 46): reprojection: 0.278277, disparity: 0.066198\n",
            "(44, 48): reprojection: 0.413623, disparity: 0.079614\n",
            "(44, 52): reprojection: 0.588604, disparity: 0.093735\n",
            "(45, 46): reprojection: 0.200184, disparity: 0.060294\n",
            "(45, 47): reprojection: 0.324638, disparity: 0.075912\n",
            "(46, 47): reprojection: 0.190311, disparity: 0.057184\n",
            "(46, 48): reprojection: 0.291809, disparity: 0.070890\n",
            "(46, 50): reprojection: 0.396624, disparity: 0.080593\n",
            "(47, 48): reprojection: 0.201829, disparity: 0.058147\n",
            "(47, 49): reprojection: 0.258290, disparity: 0.067325\n",
            "(48, 49): reprojection: 0.169762, disparity: 0.061055\n",
            "(48, 50): reprojection: 0.251285, disparity: 0.070458\n",
            "(48, 52): reprojection: 0.357199, disparity: 0.079786\n",
            "(48, 56): reprojection: 0.672962, disparity: 0.090824\n",
            "(48, 64): reprojection: 0.949121, disparity: 0.153380\n",
            "(48, 80): reprojection: 1.082316, disparity: 0.092641\n",
            "(49, 50): reprojection: 0.148857, disparity: 0.059736\n",
            "(49, 51): reprojection: 0.240913, disparity: 0.067869\n",
            "(50, 51): reprojection: 0.150106, disparity: 0.060532\n",
            "(50, 52): reprojection: 0.242123, disparity: 0.067128\n",
            "(50, 54): reprojection: 0.383140, disparity: 0.078079\n",
            "(51, 52): reprojection: 0.167514, disparity: 0.057834\n",
            "(51, 53): reprojection: 0.285664, disparity: 0.065955\n",
            "(52, 53): reprojection: 0.171351, disparity: 0.058687\n",
            "(52, 54): reprojection: 0.244247, disparity: 0.066059\n",
            "(52, 56): reprojection: 0.525599, disparity: 0.069837\n",
            "(52, 60): reprojection: 0.564893, disparity: 0.097588\n",
            "(53, 54): reprojection: 0.140891, disparity: 0.061097\n",
            "(53, 55): reprojection: 0.330867, disparity: 0.067219\n",
            "(54, 55): reprojection: 0.298879, disparity: 0.057176\n",
            "(54, 56): reprojection: 0.563684, disparity: 0.063256\n",
            "(54, 58): reprojection: 0.359682, disparity: 0.071555\n",
            "(55, 56): reprojection: 0.347249, disparity: 0.060338\n",
            "(55, 57): reprojection: 0.376032, disparity: 0.066343\n",
            "(56, 57): reprojection: 0.299136, disparity: 0.062408\n",
            "(56, 58): reprojection: 0.527555, disparity: 0.073678\n",
            "(56, 60): reprojection: 0.652150, disparity: 0.090976\n",
            "(56, 64): reprojection: 0.784390, disparity: 0.103944\n",
            "(56, 72): reprojection: 1.051407, disparity: 0.121388\n",
            "(57, 58): reprojection: 0.342358, disparity: 0.065292\n",
            "(57, 59): reprojection: 0.462539, disparity: 0.076898\n",
            "(58, 59): reprojection: 0.235770, disparity: 0.063465\n",
            "(58, 60): reprojection: 0.347316, disparity: 0.073216\n",
            "(58, 62): reprojection: 0.422426, disparity: 0.097950\n",
            "(59, 60): reprojection: 0.179643, disparity: 0.066342\n",
            "(59, 61): reprojection: 0.281162, disparity: 0.072267\n",
            "(60, 61): reprojection: 0.214687, disparity: 0.073310\n",
            "(60, 62): reprojection: 0.277897, disparity: 0.081655\n",
            "(60, 64): reprojection: 0.457831, disparity: 0.085495\n",
            "(60, 68): reprojection: 0.625655, disparity: 0.115558\n",
            "(61, 62): reprojection: 0.192361, disparity: 0.067227\n",
            "(61, 63): reprojection: 0.287930, disparity: 0.070069\n",
            "(62, 63): reprojection: 0.225064, disparity: 0.063243\n",
            "(62, 64): reprojection: 0.328162, disparity: 0.070640\n",
            "(62, 66): reprojection: 0.507436, disparity: 0.096573\n",
            "(63, 64): reprojection: 0.214182, disparity: 0.064536\n",
            "(63, 65): reprojection: 0.343809, disparity: 0.072700\n",
            "(64, 65): reprojection: 0.216806, disparity: 0.066602\n",
            "(64, 66): reprojection: 0.370278, disparity: 0.072662\n",
            "(64, 68): reprojection: 0.445340, disparity: 0.093821\n",
            "(64, 72): reprojection: 0.713321, disparity: 0.111111\n",
            "(64, 80): reprojection: 0.961434, disparity: 0.102454\n",
            "(65, 66): reprojection: 0.265486, disparity: 0.062891\n",
            "(65, 67): reprojection: 0.411743, disparity: 0.071735\n",
            "(66, 67): reprojection: 0.258800, disparity: 0.065496\n",
            "(66, 68): reprojection: 0.361439, disparity: 0.067749\n",
            "(66, 70): reprojection: 0.462364, disparity: 0.082352\n",
            "(67, 68): reprojection: 0.331703, disparity: 0.062251\n",
            "(67, 69): reprojection: 0.322780, disparity: 0.066404\n",
            "(68, 69): reprojection: 0.203243, disparity: 0.060261\n",
            "(68, 70): reprojection: 0.440417, disparity: 0.065044\n",
            "(68, 72): reprojection: 0.513445, disparity: 0.077476\n",
            "(68, 76): reprojection: 0.630261, disparity: 0.092235\n",
            "(69, 70): reprojection: 0.307331, disparity: 0.060489\n",
            "(69, 71): reprojection: 0.431221, disparity: 0.063700\n",
            "(70, 71): reprojection: 0.218529, disparity: 0.061811\n",
            "(70, 72): reprojection: 0.265929, disparity: 0.064611\n",
            "(70, 74): reprojection: 0.369658, disparity: 0.075670\n",
            "(71, 72): reprojection: 0.225945, disparity: 0.061058\n",
            "(71, 73): reprojection: 0.325163, disparity: 0.067473\n",
            "(72, 73): reprojection: 0.215475, disparity: 0.062118\n",
            "(72, 74): reprojection: 0.284196, disparity: 0.067440\n",
            "(72, 76): reprojection: 0.369959, disparity: 0.076742\n",
            "(72, 80): reprojection: 0.568396, disparity: 0.088467\n",
            "(72, 88): reprojection: 0.795661, disparity: 0.090239\n",
            "(73, 74): reprojection: 0.312464, disparity: 0.060647\n",
            "(73, 75): reprojection: 0.429492, disparity: 0.068562\n",
            "(74, 75): reprojection: 0.238966, disparity: 0.062437\n",
            "(74, 76): reprojection: 0.252353, disparity: 0.067132\n",
            "(74, 78): reprojection: 0.385472, disparity: 0.069038\n",
            "(75, 76): reprojection: 0.228002, disparity: 0.059093\n",
            "(75, 77): reprojection: 0.351301, disparity: 0.064342\n",
            "(76, 77): reprojection: 0.229605, disparity: 0.058174\n",
            "(76, 78): reprojection: 0.344713, disparity: 0.060779\n",
            "(76, 80): reprojection: 0.405880, disparity: 0.067002\n",
            "(76, 84): reprojection: 0.573279, disparity: 0.077344\n",
            "(77, 78): reprojection: 0.229024, disparity: 0.058868\n",
            "(77, 79): reprojection: 0.246164, disparity: 0.061373\n",
            "(78, 79): reprojection: 0.189833, disparity: 0.059347\n",
            "(78, 80): reprojection: 0.225586, disparity: 0.062949\n",
            "(78, 82): reprojection: 0.359021, disparity: 0.066825\n",
            "(79, 80): reprojection: 0.140605, disparity: 0.056476\n",
            "(79, 81): reprojection: 0.229301, disparity: 0.060160\n",
            "(80, 81): reprojection: 0.146020, disparity: 0.054438\n",
            "(80, 82): reprojection: 0.256785, disparity: 0.058789\n",
            "(80, 84): reprojection: 0.430838, disparity: 0.064732\n",
            "(80, 88): reprojection: 0.671192, disparity: 0.077095\n",
            "(81, 82): reprojection: 0.178487, disparity: 0.054817\n",
            "(81, 83): reprojection: 0.276192, disparity: 0.060855\n",
            "(82, 83): reprojection: 0.142246, disparity: 0.053932\n",
            "(82, 84): reprojection: 0.241306, disparity: 0.060858\n",
            "(82, 86): reprojection: 0.412101, disparity: 0.068381\n",
            "(83, 84): reprojection: 0.139089, disparity: 0.055004\n",
            "(83, 85): reprojection: 0.296132, disparity: 0.063835\n",
            "(84, 85): reprojection: 0.170931, disparity: 0.053375\n",
            "(84, 86): reprojection: 0.248885, disparity: 0.060112\n",
            "(84, 88): reprojection: 0.484268, disparity: 0.064998\n",
            "(85, 86): reprojection: 0.164816, disparity: 0.051976\n",
            "(85, 87): reprojection: 0.271056, disparity: 0.057235\n",
            "(86, 87): reprojection: 0.230027, disparity: 0.053110\n",
            "(86, 88): reprojection: 0.332347, disparity: 0.063001\n",
            "(86, 90): reprojection: 0.487185, disparity: 0.064188\n",
            "(87, 88): reprojection: 0.180397, disparity: 0.053958\n",
            "(87, 89): reprojection: 0.279068, disparity: 0.058993\n",
            "(88, 89): reprojection: 0.141228, disparity: 0.050354\n",
            "(88, 90): reprojection: 0.306265, disparity: 0.055993\n",
            "(89, 90): reprojection: 0.213347, disparity: 0.048296\n",
            "(89, 91): reprojection: 0.417128, disparity: 0.063839\n",
            "(90, 91): reprojection: 0.334981, disparity: 0.057224\n",
            "Mean:     reprojection: 0.334981, disparity: 0.057224\n",
            "Done Validation for epoch 17 (4420 iterations)\n",
            "Epoch = 17, pairs = [[86, 88], [12, 14], [38, 40], [86, 90]], loss = 0.4275105893611908\n",
            "Epoch = 17, pairs = [[69, 71], [52, 56], [13, 15], [51, 53]], loss = 0.43473097681999207\n",
            "Epoch = 17, pairs = [[33, 34], [57, 59], [16, 17], [74, 76]], loss = 0.31425994634628296\n",
            "Epoch = 17, pairs = [[54, 56], [82, 84], [22, 23], [80, 88]], loss = 0.46173059940338135\n",
            "Epoch = 17, pairs = [[76, 80], [83, 84], [44, 45], [63, 64]], loss = 0.27891361713409424\n",
            "Epoch = 17, pairs = [[75, 77], [80, 82], [76, 78], [36, 40]], loss = 0.37763088941574097\n",
            "Epoch = 17, pairs = [[83, 85], [4, 8], [21, 23], [6, 10]], loss = 0.3474110960960388\n",
            "Epoch = 17, pairs = [[1, 2], [42, 44], [16, 18], [50, 51]], loss = 0.30365195870399475\n",
            "Epoch = 17, pairs = [[48, 49], [85, 87], [38, 42], [30, 31]], loss = 0.3759816884994507\n",
            "Epoch = 17, pairs = [[44, 48], [70, 74], [71, 72], [73, 75]], loss = 0.43399110436439514\n",
            "Epoch = 17, pairs = [[8, 24], [36, 38], [24, 40], [76, 84]], loss = 0.6968210935592651\n",
            "Epoch = 17, pairs = [[25, 27], [10, 11], [40, 48], [55, 57]], loss = 0.4345337152481079\n",
            "Epoch = 17, pairs = [[48, 50], [64, 72], [34, 35], [44, 46]], loss = 0.4363992214202881\n",
            "Epoch = 17, pairs = [[47, 48], [1, 3], [24, 32], [61, 62]], loss = 0.32977402210235596\n",
            "Epoch = 17, pairs = [[2, 3], [49, 51], [48, 80], [78, 79]], loss = 0.45346665382385254\n",
            "Epoch = 17, pairs = [[16, 24], [29, 30], [18, 19], [71, 73]], loss = 0.45119839906692505\n",
            "Epoch = 17, pairs = [[53, 55], [26, 27], [2, 4], [10, 12]], loss = 0.27476170659065247\n",
            "Epoch = 17, pairs = [[54, 55], [32, 48], [68, 72], [85, 86]], loss = 0.5362199544906616\n",
            "Epoch = 17, pairs = [[79, 81], [0, 2], [18, 22], [77, 79]], loss = 0.39643174409866333\n",
            "Epoch = 17, pairs = [[48, 56], [26, 30], [14, 16], [82, 86]], loss = 0.5038411617279053\n",
            "Epoch = 17, pairs = [[40, 42], [12, 20], [24, 25], [84, 86]], loss = 0.42552563548088074\n",
            "Epoch = 17, pairs = [[22, 24], [59, 60], [19, 20], [68, 69]], loss = 0.228419229388237\n",
            "Epoch = 17, pairs = [[33, 35], [8, 16], [51, 52], [43, 44]], loss = 0.3880470395088196\n",
            "Epoch = 17, pairs = [[44, 52], [52, 60], [27, 28], [48, 52]], loss = 0.5426181554794312\n",
            "Epoch = 17, pairs = [[40, 56], [60, 62], [73, 74], [46, 50]], loss = 0.5613747239112854\n",
            "Epoch = 17, pairs = [[0, 32], [72, 76], [7, 8], [79, 80]], loss = 0.47004181146621704\n",
            "Epoch = 17, pairs = [[46, 48], [43, 45], [47, 49], [72, 73]], loss = 0.32531052827835083\n",
            "Epoch = 17, pairs = [[9, 11], [14, 15], [5, 6], [26, 28]], loss = 0.2775593101978302\n",
            "Epoch = 17, pairs = [[58, 59], [88, 90], [19, 21], [65, 67]], loss = 0.3887639343738556\n",
            "Epoch = 17, pairs = [[72, 80], [64, 66], [66, 68], [64, 80]], loss = 0.6808494329452515\n",
            "Epoch = 17, pairs = [[32, 33], [72, 74], [87, 88], [7, 9]], loss = 0.28400737047195435\n",
            "Epoch = 17, pairs = [[80, 81], [3, 4], [35, 37], [40, 44]], loss = 0.3429391086101532\n",
            "Epoch = 17, pairs = [[17, 18], [23, 24], [41, 42], [65, 66]], loss = 0.28655558824539185\n",
            "Epoch = 17, pairs = [[15, 16], [56, 58], [12, 13], [27, 29]], loss = 0.3656098544597626\n",
            "Epoch = 17, pairs = [[14, 18], [90, 91], [20, 28], [64, 65]], loss = 0.41139328479766846\n",
            "Epoch = 17, pairs = [[0, 1], [66, 67], [48, 64], [29, 31]], loss = 0.5091602802276611\n",
            "Epoch = 17, pairs = [[62, 66], [50, 54], [3, 5], [81, 82]], loss = 0.3716774582862854\n",
            "Epoch = 17, pairs = [[28, 29], [12, 16], [6, 7], [50, 52]], loss = 0.3007544279098511\n",
            "Epoch = 17, pairs = [[58, 62], [60, 68], [69, 70], [32, 34]], loss = 0.5151688456535339\n",
            "Epoch = 17, pairs = [[32, 40], [84, 85], [0, 4], [80, 84]], loss = 0.4522443413734436\n",
            "Epoch = 17, pairs = [[56, 57], [62, 63], [8, 12], [46, 47]], loss = 0.3497268557548523\n",
            "Epoch = 17, pairs = [[52, 53], [66, 70], [32, 64], [17, 19]], loss = 0.6354073286056519\n",
            "Epoch = 17, pairs = [[76, 77], [22, 26], [45, 46], [55, 56]], loss = 0.33640849590301514\n",
            "Epoch = 17, pairs = [[49, 50], [4, 6], [67, 69], [4, 12]], loss = 0.3274366855621338\n",
            "Epoch = 17, pairs = [[15, 17], [59, 61], [11, 12], [30, 32]], loss = 0.324503630399704\n",
            "Epoch = 17, pairs = [[4, 5], [2, 6], [5, 7], [31, 32]], loss = 0.24707412719726562\n",
            "Epoch = 17, pairs = [[6, 8], [63, 65], [40, 41], [34, 38]], loss = 0.3504236936569214\n",
            "Epoch = 17, pairs = [[10, 14], [39, 41], [0, 8], [28, 30]], loss = 0.46785399317741394\n",
            "Epoch = 17, pairs = [[57, 58], [70, 71], [74, 78], [56, 60]], loss = 0.5010061860084534\n",
            "Epoch = 17, pairs = [[56, 72], [30, 34], [89, 91], [84, 88]], loss = 0.696287989616394\n",
            "Epoch = 17, pairs = [[68, 76], [78, 80], [82, 83], [28, 36]], loss = 0.4723341166973114\n",
            "Epoch = 17, pairs = [[77, 78], [18, 20], [37, 38], [32, 36]], loss = 0.3412596583366394\n",
            "Epoch = 17, pairs = [[28, 32], [54, 58], [61, 63], [23, 25]], loss = 0.4232638478279114\n",
            "Epoch = 17, pairs = [[86, 87], [8, 9], [45, 47], [58, 60]], loss = 0.3427223563194275\n",
            "Epoch = 17, pairs = [[60, 64], [72, 88], [16, 48], [36, 37]], loss = 1.1605712175369263\n",
            "Epoch = 17, pairs = [[41, 43], [70, 72], [75, 76], [13, 14]], loss = 0.3009176254272461\n",
            "Epoch = 17, pairs = [[8, 10], [87, 89], [25, 26], [62, 64]], loss = 0.35031843185424805\n",
            "Epoch = 17, pairs = [[0, 16], [36, 44], [20, 22], [60, 61]], loss = 0.6692427396774292\n",
            "Epoch = 17, pairs = [[24, 28], [11, 13], [16, 20], [16, 32]], loss = 0.8141553401947021\n",
            "Epoch = 17, pairs = [[74, 75], [20, 24], [9, 10], [38, 39]], loss = 0.3247772455215454\n",
            "Epoch = 17, pairs = [[35, 36], [67, 68], [68, 70], [53, 54]], loss = 0.3575872778892517\n",
            "Epoch = 17, pairs = [[89, 90], [78, 82], [37, 39], [31, 33]], loss = 0.3378235697746277\n",
            "Epoch = 17, pairs = [[21, 22], [39, 40], [56, 64], [34, 36]], loss = 0.47154632210731506\n",
            "Epoch = 17, pairs = [[64, 68], [24, 26], [88, 89], [81, 83]], loss = 0.3387676477432251\n",
            "Epoch = 17, pairs = [[42, 46], [42, 43], [52, 54], [20, 21]], loss = 0.3825526535511017\n",
            "Epoch 17 took 84.52s.\n",
            "( 0,  1): reprojection: 0.239803, disparity: 0.052399\n",
            "( 0,  2): reprojection: 0.360603, disparity: 0.054809\n",
            "( 0,  4): reprojection: 0.415221, disparity: 0.068121\n",
            "( 0,  8): reprojection: 0.508078, disparity: 0.090309\n",
            "( 0, 16): reprojection: 1.001820, disparity: 0.091039\n",
            "( 0, 32): reprojection: 1.724519, disparity: 0.121152\n",
            "( 1,  2): reprojection: 0.198820, disparity: 0.046035\n",
            "( 1,  3): reprojection: 0.254367, disparity: 0.051831\n",
            "( 2,  3): reprojection: 0.111870, disparity: 0.045708\n",
            "( 2,  4): reprojection: 0.176458, disparity: 0.054174\n",
            "( 2,  6): reprojection: 0.335561, disparity: 0.064987\n",
            "( 3,  4): reprojection: 0.115638, disparity: 0.047794\n",
            "( 3,  5): reprojection: 0.191124, disparity: 0.056122\n",
            "( 4,  5): reprojection: 0.149738, disparity: 0.047738\n",
            "( 4,  6): reprojection: 0.211184, disparity: 0.053013\n",
            "( 4,  8): reprojection: 0.296587, disparity: 0.065042\n",
            "( 4, 12): reprojection: 0.386055, disparity: 0.077950\n",
            "( 5,  6): reprojection: 0.125111, disparity: 0.046479\n",
            "( 5,  7): reprojection: 0.173337, disparity: 0.052942\n",
            "( 6,  7): reprojection: 0.106696, disparity: 0.046408\n",
            "( 6,  8): reprojection: 0.181332, disparity: 0.052653\n",
            "( 6, 10): reprojection: 0.399625, disparity: 0.064560\n",
            "( 7,  8): reprojection: 0.100051, disparity: 0.046684\n",
            "( 7,  9): reprojection: 0.203410, disparity: 0.054600\n",
            "( 8,  9): reprojection: 0.171474, disparity: 0.048212\n",
            "( 8, 10): reprojection: 0.357447, disparity: 0.054793\n",
            "( 8, 12): reprojection: 0.487024, disparity: 0.067962\n",
            "( 8, 16): reprojection: 0.737446, disparity: 0.080190\n",
            "( 8, 24): reprojection: 1.159932, disparity: 0.102619\n",
            "( 9, 10): reprojection: 0.243903, disparity: 0.047382\n",
            "( 9, 11): reprojection: 0.359288, disparity: 0.056448\n",
            "(10, 11): reprojection: 0.156133, disparity: 0.045092\n",
            "(10, 12): reprojection: 0.208701, disparity: 0.052958\n",
            "(10, 14): reprojection: 0.394617, disparity: 0.068320\n",
            "(11, 12): reprojection: 0.139702, disparity: 0.044517\n",
            "(11, 13): reprojection: 0.240786, disparity: 0.054445\n",
            "(12, 13): reprojection: 0.162305, disparity: 0.045196\n",
            "(12, 14): reprojection: 0.310062, disparity: 0.055228\n",
            "(12, 16): reprojection: 0.571738, disparity: 0.075986\n",
            "(12, 20): reprojection: 0.909402, disparity: 0.082401\n",
            "(13, 14): reprojection: 0.192028, disparity: 0.049828\n",
            "(13, 15): reprojection: 0.311425, disparity: 0.056823\n",
            "(14, 15): reprojection: 0.219935, disparity: 0.048046\n",
            "(14, 16): reprojection: 0.465629, disparity: 0.059911\n",
            "(14, 18): reprojection: 0.475576, disparity: 0.068717\n",
            "(15, 16): reprojection: 0.313100, disparity: 0.050980\n",
            "(15, 17): reprojection: 0.407517, disparity: 0.061571\n",
            "(16, 17): reprojection: 0.154247, disparity: 0.048682\n",
            "(16, 18): reprojection: 0.345780, disparity: 0.060313\n",
            "(16, 20): reprojection: 0.562947, disparity: 0.073672\n",
            "(16, 24): reprojection: 0.947247, disparity: 0.093266\n",
            "(16, 32): reprojection: 1.695309, disparity: 0.120965\n",
            "(16, 48): reprojection: 1.303112, disparity: 0.171692\n",
            "(17, 18): reprojection: 0.324179, disparity: 0.052551\n",
            "(17, 19): reprojection: 0.499868, disparity: 0.063693\n",
            "(18, 19): reprojection: 0.218785, disparity: 0.050984\n",
            "(18, 20): reprojection: 0.288676, disparity: 0.061024\n",
            "(18, 22): reprojection: 0.571054, disparity: 0.072987\n",
            "(19, 20): reprojection: 0.122032, disparity: 0.050651\n",
            "(19, 21): reprojection: 0.271749, disparity: 0.061651\n",
            "(20, 21): reprojection: 0.216345, disparity: 0.052618\n",
            "(20, 22): reprojection: 0.341913, disparity: 0.063975\n",
            "(20, 24): reprojection: 0.518419, disparity: 0.073887\n",
            "(20, 28): reprojection: 0.958707, disparity: 0.103895\n",
            "(21, 22): reprojection: 0.190014, disparity: 0.052536\n",
            "(21, 23): reprojection: 0.325024, disparity: 0.060803\n",
            "(22, 23): reprojection: 0.150432, disparity: 0.051909\n",
            "(22, 24): reprojection: 0.229341, disparity: 0.062242\n",
            "(22, 26): reprojection: 0.422359, disparity: 0.078474\n",
            "(23, 24): reprojection: 0.123552, disparity: 0.053438\n",
            "(23, 25): reprojection: 0.223240, disparity: 0.064425\n",
            "(24, 25): reprojection: 0.168822, disparity: 0.055117\n",
            "(24, 26): reprojection: 0.237413, disparity: 0.064996\n",
            "(24, 28): reprojection: 0.410159, disparity: 0.086641\n",
            "(24, 32): reprojection: 0.850697, disparity: 0.095489\n",
            "(24, 40): reprojection: 1.370326, disparity: 0.143794\n",
            "(25, 26): reprojection: 0.137927, disparity: 0.058435\n",
            "(25, 27): reprojection: 0.264121, disparity: 0.068778\n",
            "(26, 27): reprojection: 0.171001, disparity: 0.057127\n",
            "(26, 28): reprojection: 0.220296, disparity: 0.071786\n",
            "(26, 30): reprojection: 0.416766, disparity: 0.078311\n",
            "(27, 28): reprojection: 0.168216, disparity: 0.054516\n",
            "(27, 29): reprojection: 0.291280, disparity: 0.065478\n",
            "(28, 29): reprojection: 0.204758, disparity: 0.056420\n",
            "(28, 30): reprojection: 0.282287, disparity: 0.065402\n",
            "(28, 32): reprojection: 0.585170, disparity: 0.074089\n",
            "(28, 36): reprojection: 0.937272, disparity: 0.090479\n",
            "(29, 30): reprojection: 0.158012, disparity: 0.054254\n",
            "(29, 31): reprojection: 0.343493, disparity: 0.060888\n",
            "(30, 31): reprojection: 0.218320, disparity: 0.054973\n",
            "(30, 32): reprojection: 0.328468, disparity: 0.062269\n",
            "(30, 34): reprojection: 0.627115, disparity: 0.079593\n",
            "(31, 32): reprojection: 0.153356, disparity: 0.053518\n",
            "(31, 33): reprojection: 0.364051, disparity: 0.063126\n",
            "(32, 33): reprojection: 0.280919, disparity: 0.057809\n",
            "(32, 34): reprojection: 0.308332, disparity: 0.063218\n",
            "(32, 36): reprojection: 0.478045, disparity: 0.076495\n",
            "(32, 40): reprojection: 0.711373, disparity: 0.117572\n",
            "(32, 48): reprojection: 1.343457, disparity: 0.166235\n",
            "(32, 64): reprojection: 1.330404, disparity: 0.214566\n",
            "(33, 34): reprojection: 0.154299, disparity: 0.051540\n",
            "(33, 35): reprojection: 0.293914, disparity: 0.058112\n",
            "(34, 35): reprojection: 0.208288, disparity: 0.050888\n",
            "(34, 36): reprojection: 0.332644, disparity: 0.061928\n",
            "(34, 38): reprojection: 0.476012, disparity: 0.076196\n",
            "(35, 36): reprojection: 0.255424, disparity: 0.055114\n",
            "(35, 37): reprojection: 0.330345, disparity: 0.069042\n",
            "(36, 37): reprojection: 0.288821, disparity: 0.063149\n",
            "(36, 38): reprojection: 0.390145, disparity: 0.068791\n",
            "(36, 40): reprojection: 0.407710, disparity: 0.087903\n",
            "(36, 44): reprojection: 0.807051, disparity: 0.125073\n",
            "(37, 38): reprojection: 0.223305, disparity: 0.053090\n",
            "(37, 39): reprojection: 0.270411, disparity: 0.063626\n",
            "(38, 39): reprojection: 0.207801, disparity: 0.057196\n",
            "(38, 40): reprojection: 0.393331, disparity: 0.071860\n",
            "(38, 42): reprojection: 0.697462, disparity: 0.097095\n",
            "(39, 40): reprojection: 0.294612, disparity: 0.059635\n",
            "(39, 41): reprojection: 0.508000, disparity: 0.075541\n",
            "(40, 41): reprojection: 0.290045, disparity: 0.063785\n",
            "(40, 42): reprojection: 0.416989, disparity: 0.076640\n",
            "(40, 44): reprojection: 0.559511, disparity: 0.103534\n",
            "(40, 48): reprojection: 0.831450, disparity: 0.141409\n",
            "(40, 56): reprojection: 1.039972, disparity: 0.190897\n",
            "(41, 42): reprojection: 0.206069, disparity: 0.060135\n",
            "(41, 43): reprojection: 0.308346, disparity: 0.075769\n",
            "(42, 43): reprojection: 0.170112, disparity: 0.062206\n",
            "(42, 44): reprojection: 0.324399, disparity: 0.079596\n",
            "(42, 46): reprojection: 0.533396, disparity: 0.108382\n",
            "(43, 44): reprojection: 0.221590, disparity: 0.060321\n",
            "(43, 45): reprojection: 0.328496, disparity: 0.083411\n",
            "(44, 45): reprojection: 0.169071, disparity: 0.064918\n",
            "(44, 46): reprojection: 0.299044, disparity: 0.082239\n",
            "(44, 48): reprojection: 0.461784, disparity: 0.101718\n",
            "(44, 52): reprojection: 0.667074, disparity: 0.114177\n",
            "(45, 46): reprojection: 0.207388, disparity: 0.061601\n",
            "(45, 47): reprojection: 0.346748, disparity: 0.077074\n",
            "(46, 47): reprojection: 0.205715, disparity: 0.062056\n",
            "(46, 48): reprojection: 0.316376, disparity: 0.082856\n",
            "(46, 50): reprojection: 0.474939, disparity: 0.096545\n",
            "(47, 48): reprojection: 0.213621, disparity: 0.062856\n",
            "(47, 49): reprojection: 0.301979, disparity: 0.074085\n",
            "(48, 49): reprojection: 0.189738, disparity: 0.065499\n",
            "(48, 50): reprojection: 0.303888, disparity: 0.077456\n",
            "(48, 52): reprojection: 0.401231, disparity: 0.094408\n",
            "(48, 56): reprojection: 0.777044, disparity: 0.112695\n",
            "(48, 64): reprojection: 1.079654, disparity: 0.175984\n",
            "(48, 80): reprojection: 1.252892, disparity: 0.158477\n",
            "(49, 50): reprojection: 0.177819, disparity: 0.061666\n",
            "(49, 51): reprojection: 0.278861, disparity: 0.071760\n",
            "(50, 51): reprojection: 0.163542, disparity: 0.061241\n",
            "(50, 52): reprojection: 0.256470, disparity: 0.071346\n",
            "(50, 54): reprojection: 0.379466, disparity: 0.095850\n",
            "(51, 52): reprojection: 0.156679, disparity: 0.060520\n",
            "(51, 53): reprojection: 0.271378, disparity: 0.072530\n",
            "(52, 53): reprojection: 0.163092, disparity: 0.062599\n",
            "(52, 54): reprojection: 0.248029, disparity: 0.075874\n",
            "(52, 56): reprojection: 0.552150, disparity: 0.081877\n",
            "(52, 60): reprojection: 0.705640, disparity: 0.110360\n",
            "(53, 54): reprojection: 0.150398, disparity: 0.062916\n",
            "(53, 55): reprojection: 0.357767, disparity: 0.070703\n",
            "(54, 55): reprojection: 0.297064, disparity: 0.058714\n",
            "(54, 56): reprojection: 0.570129, disparity: 0.066178\n",
            "(54, 58): reprojection: 0.431782, disparity: 0.076657\n",
            "(55, 56): reprojection: 0.348889, disparity: 0.065362\n",
            "(55, 57): reprojection: 0.405831, disparity: 0.071187\n",
            "(56, 57): reprojection: 0.309073, disparity: 0.066999\n",
            "(56, 58): reprojection: 0.533783, disparity: 0.081303\n",
            "(56, 60): reprojection: 0.680436, disparity: 0.093684\n",
            "(56, 64): reprojection: 0.922916, disparity: 0.107498\n",
            "(56, 72): reprojection: 1.184256, disparity: 0.120857\n",
            "(57, 58): reprojection: 0.346214, disparity: 0.066987\n",
            "(57, 59): reprojection: 0.466862, disparity: 0.076259\n",
            "(58, 59): reprojection: 0.231327, disparity: 0.066078\n",
            "(58, 60): reprojection: 0.353581, disparity: 0.077725\n",
            "(58, 62): reprojection: 0.432255, disparity: 0.092757\n",
            "(59, 60): reprojection: 0.188722, disparity: 0.067939\n",
            "(59, 61): reprojection: 0.282242, disparity: 0.083264\n",
            "(60, 61): reprojection: 0.215194, disparity: 0.074325\n",
            "(60, 62): reprojection: 0.289384, disparity: 0.081061\n",
            "(60, 64): reprojection: 0.464016, disparity: 0.092411\n",
            "(60, 68): reprojection: 0.727479, disparity: 0.115059\n",
            "(61, 62): reprojection: 0.175018, disparity: 0.069503\n",
            "(61, 63): reprojection: 0.289391, disparity: 0.075296\n",
            "(62, 63): reprojection: 0.228292, disparity: 0.066841\n",
            "(62, 64): reprojection: 0.344199, disparity: 0.077104\n",
            "(62, 66): reprojection: 0.595317, disparity: 0.105665\n",
            "(63, 64): reprojection: 0.226381, disparity: 0.068954\n",
            "(63, 65): reprojection: 0.372019, disparity: 0.075347\n",
            "(64, 65): reprojection: 0.245506, disparity: 0.070076\n",
            "(64, 66): reprojection: 0.414868, disparity: 0.076023\n",
            "(64, 68): reprojection: 0.570361, disparity: 0.095429\n",
            "(64, 72): reprojection: 0.871368, disparity: 0.114471\n",
            "(64, 80): reprojection: 1.331442, disparity: 0.121835\n",
            "(65, 66): reprojection: 0.268225, disparity: 0.066521\n",
            "(65, 67): reprojection: 0.426722, disparity: 0.076847\n",
            "(66, 67): reprojection: 0.266057, disparity: 0.069118\n",
            "(66, 68): reprojection: 0.386315, disparity: 0.070628\n",
            "(66, 70): reprojection: 0.514716, disparity: 0.083809\n",
            "(67, 68): reprojection: 0.333782, disparity: 0.065816\n",
            "(67, 69): reprojection: 0.308540, disparity: 0.070171\n",
            "(68, 69): reprojection: 0.206702, disparity: 0.062637\n",
            "(68, 70): reprojection: 0.434665, disparity: 0.068856\n",
            "(68, 72): reprojection: 0.577206, disparity: 0.080460\n",
            "(68, 76): reprojection: 0.791595, disparity: 0.095611\n",
            "(69, 70): reprojection: 0.296075, disparity: 0.062527\n",
            "(69, 71): reprojection: 0.409142, disparity: 0.068203\n",
            "(70, 71): reprojection: 0.222815, disparity: 0.062254\n",
            "(70, 72): reprojection: 0.309119, disparity: 0.067089\n",
            "(70, 74): reprojection: 0.482030, disparity: 0.076921\n",
            "(71, 72): reprojection: 0.234053, disparity: 0.063441\n",
            "(71, 73): reprojection: 0.344317, disparity: 0.071138\n",
            "(72, 73): reprojection: 0.222943, disparity: 0.063781\n",
            "(72, 74): reprojection: 0.321549, disparity: 0.069509\n",
            "(72, 76): reprojection: 0.444869, disparity: 0.079415\n",
            "(72, 80): reprojection: 0.803618, disparity: 0.100707\n",
            "(72, 88): reprojection: 0.949883, disparity: 0.116237\n",
            "(73, 74): reprojection: 0.322804, disparity: 0.063886\n",
            "(73, 75): reprojection: 0.451631, disparity: 0.072286\n",
            "(74, 75): reprojection: 0.250443, disparity: 0.062583\n",
            "(74, 76): reprojection: 0.295212, disparity: 0.065862\n",
            "(74, 78): reprojection: 0.478077, disparity: 0.079213\n",
            "(75, 76): reprojection: 0.233422, disparity: 0.059340\n",
            "(75, 77): reprojection: 0.375541, disparity: 0.066789\n",
            "(76, 77): reprojection: 0.246328, disparity: 0.059330\n",
            "(76, 78): reprojection: 0.397000, disparity: 0.062642\n",
            "(76, 80): reprojection: 0.500318, disparity: 0.071209\n",
            "(76, 84): reprojection: 0.670565, disparity: 0.084475\n",
            "(77, 78): reprojection: 0.252501, disparity: 0.059048\n",
            "(77, 79): reprojection: 0.277636, disparity: 0.063878\n",
            "(78, 79): reprojection: 0.194780, disparity: 0.060646\n",
            "(78, 80): reprojection: 0.242438, disparity: 0.065853\n",
            "(78, 82): reprojection: 0.377906, disparity: 0.072302\n",
            "(79, 80): reprojection: 0.149485, disparity: 0.057072\n",
            "(79, 81): reprojection: 0.254736, disparity: 0.060553\n",
            "(80, 81): reprojection: 0.159290, disparity: 0.055278\n",
            "(80, 82): reprojection: 0.277667, disparity: 0.060589\n",
            "(80, 84): reprojection: 0.432160, disparity: 0.066093\n",
            "(80, 88): reprojection: 0.680088, disparity: 0.084423\n",
            "(81, 82): reprojection: 0.178945, disparity: 0.055569\n",
            "(81, 83): reprojection: 0.267773, disparity: 0.062299\n",
            "(82, 83): reprojection: 0.138864, disparity: 0.055878\n",
            "(82, 84): reprojection: 0.248420, disparity: 0.062505\n",
            "(82, 86): reprojection: 0.405787, disparity: 0.074659\n",
            "(83, 84): reprojection: 0.142338, disparity: 0.055575\n",
            "(83, 85): reprojection: 0.295057, disparity: 0.066357\n",
            "(84, 85): reprojection: 0.170730, disparity: 0.054321\n",
            "(84, 86): reprojection: 0.242824, disparity: 0.063217\n",
            "(84, 88): reprojection: 0.469891, disparity: 0.071176\n",
            "(85, 86): reprojection: 0.158506, disparity: 0.054337\n",
            "(85, 87): reprojection: 0.260430, disparity: 0.060844\n",
            "(86, 87): reprojection: 0.224612, disparity: 0.056032\n",
            "(86, 88): reprojection: 0.327001, disparity: 0.064851\n",
            "(86, 90): reprojection: 0.499688, disparity: 0.068009\n",
            "(87, 88): reprojection: 0.179163, disparity: 0.053863\n",
            "(87, 89): reprojection: 0.289243, disparity: 0.060017\n",
            "(88, 89): reprojection: 0.148730, disparity: 0.051282\n",
            "(88, 90): reprojection: 0.310523, disparity: 0.056581\n",
            "(89, 90): reprojection: 0.212559, disparity: 0.050439\n",
            "(89, 91): reprojection: 0.417096, disparity: 0.065715\n",
            "(90, 91): reprojection: 0.329437, disparity: 0.058207\n",
            "Mean:     reprojection: 0.329437, disparity: 0.058207\n",
            "Done Validation for epoch 18 (4680 iterations)\n",
            "Epoch = 18, pairs = [[80, 88], [14, 16], [50, 51], [36, 40]], loss = 0.5034987330436707\n",
            "Epoch = 18, pairs = [[51, 52], [21, 22], [62, 64], [16, 48]], loss = 0.6020263433456421\n",
            "Epoch = 18, pairs = [[63, 65], [88, 89], [20, 24], [17, 19]], loss = 0.4111465513706207\n",
            "Epoch = 18, pairs = [[20, 21], [29, 31], [62, 66], [83, 85]], loss = 0.37342745065689087\n",
            "Epoch = 18, pairs = [[18, 19], [56, 57], [38, 39], [13, 14]], loss = 0.28088200092315674\n",
            "Epoch = 18, pairs = [[18, 22], [44, 48], [40, 56], [56, 58]], loss = 0.6827975511550903\n",
            "Epoch = 18, pairs = [[48, 49], [52, 60], [30, 34], [38, 40]], loss = 0.5098000168800354\n",
            "Epoch = 18, pairs = [[56, 60], [64, 66], [4, 12], [42, 43]], loss = 0.49463650584220886\n",
            "Epoch = 18, pairs = [[20, 22], [18, 20], [32, 64], [52, 56]], loss = 0.6766412258148193\n",
            "Epoch = 18, pairs = [[44, 46], [72, 80], [17, 18], [59, 60]], loss = 0.38846391439437866\n",
            "Epoch = 18, pairs = [[25, 27], [10, 14], [13, 15], [11, 12]], loss = 0.28851452469825745\n",
            "Epoch = 18, pairs = [[28, 36], [11, 13], [23, 25], [34, 36]], loss = 0.48949337005615234\n",
            "Epoch = 18, pairs = [[36, 38], [59, 61], [33, 34], [41, 42]], loss = 0.3222653567790985\n",
            "Epoch = 18, pairs = [[12, 16], [57, 59], [44, 52], [60, 62]], loss = 0.5990933179855347\n",
            "Epoch = 18, pairs = [[8, 9], [64, 65], [43, 44], [63, 64]], loss = 0.2650126814842224\n",
            "Epoch = 18, pairs = [[74, 75], [0, 2], [2, 6], [53, 55]], loss = 0.3700646460056305\n",
            "Epoch = 18, pairs = [[77, 78], [58, 62], [48, 64], [22, 23]], loss = 0.6522094011306763\n",
            "Epoch = 18, pairs = [[32, 36], [24, 28], [78, 82], [16, 32]], loss = 0.6551909446716309\n",
            "Epoch = 18, pairs = [[27, 29], [82, 84], [48, 52], [8, 16]], loss = 0.44287508726119995\n",
            "Epoch = 18, pairs = [[1, 2], [60, 61], [71, 72], [5, 7]], loss = 0.24983221292495728\n",
            "Epoch = 18, pairs = [[55, 56], [64, 68], [76, 80], [8, 10]], loss = 0.4357072412967682\n",
            "Epoch = 18, pairs = [[80, 81], [14, 15], [68, 72], [12, 20]], loss = 0.4454774856567383\n",
            "Epoch = 18, pairs = [[49, 50], [15, 17], [72, 74], [73, 75]], loss = 0.36342036724090576\n",
            "Epoch = 18, pairs = [[22, 24], [66, 70], [9, 11], [16, 20]], loss = 0.46012750267982483\n",
            "Epoch = 18, pairs = [[41, 43], [86, 88], [2, 3], [66, 68]], loss = 0.32786792516708374\n",
            "Epoch = 18, pairs = [[78, 79], [42, 44], [71, 73], [80, 82]], loss = 0.3370944857597351\n",
            "Epoch = 18, pairs = [[50, 52], [57, 58], [76, 84], [88, 90]], loss = 0.4577462673187256\n",
            "Epoch = 18, pairs = [[32, 40], [64, 72], [69, 71], [40, 42]], loss = 0.6514682173728943\n",
            "Epoch = 18, pairs = [[25, 26], [15, 16], [54, 58], [29, 30]], loss = 0.30860665440559387\n",
            "Epoch = 18, pairs = [[50, 54], [54, 56], [42, 46], [22, 26]], loss = 0.5295331478118896\n",
            "Epoch = 18, pairs = [[24, 25], [19, 20], [31, 33], [37, 38]], loss = 0.24207285046577454\n",
            "Epoch = 18, pairs = [[37, 39], [0, 4], [77, 79], [46, 47]], loss = 0.3272259831428528\n",
            "Epoch = 18, pairs = [[40, 41], [30, 32], [27, 28], [48, 50]], loss = 0.3081367015838623\n",
            "Epoch = 18, pairs = [[28, 32], [43, 45], [84, 86], [12, 13]], loss = 0.3400349020957947\n",
            "Epoch = 18, pairs = [[72, 73], [39, 40], [0, 16], [53, 54]], loss = 0.433136910200119\n",
            "Epoch = 18, pairs = [[55, 57], [76, 78], [21, 23], [26, 28]], loss = 0.34263062477111816\n",
            "Epoch = 18, pairs = [[31, 32], [74, 78], [86, 87], [0, 1]], loss = 0.2970486283302307\n",
            "Epoch = 18, pairs = [[70, 72], [32, 33], [16, 17], [76, 77]], loss = 0.2636580169200897\n",
            "Epoch = 18, pairs = [[75, 77], [46, 50], [65, 67], [3, 5]], loss = 0.4040432870388031\n",
            "Epoch = 18, pairs = [[35, 36], [4, 6], [1, 3], [89, 91]], loss = 0.32970085740089417\n",
            "Epoch = 18, pairs = [[64, 80], [52, 54], [46, 48], [35, 37]], loss = 0.6296761631965637\n",
            "Epoch = 18, pairs = [[79, 81], [24, 26], [2, 4], [48, 80]], loss = 0.5877612829208374\n",
            "Epoch = 18, pairs = [[65, 66], [60, 68], [67, 68], [28, 30]], loss = 0.4697738289833069\n",
            "Epoch = 18, pairs = [[4, 5], [16, 18], [8, 24], [40, 44]], loss = 0.5061821937561035\n",
            "Epoch = 18, pairs = [[12, 14], [7, 8], [14, 18], [32, 48]], loss = 0.4972938001155853\n",
            "Epoch = 18, pairs = [[28, 29], [58, 59], [47, 48], [85, 86]], loss = 0.25910574197769165\n",
            "Epoch = 18, pairs = [[26, 27], [7, 9], [86, 90], [20, 28]], loss = 0.4048463702201843\n",
            "Epoch = 18, pairs = [[23, 24], [48, 56], [61, 63], [87, 88]], loss = 0.4200628697872162\n",
            "Epoch = 18, pairs = [[36, 37], [52, 53], [6, 10], [70, 71]], loss = 0.3170459270477295\n",
            "Epoch = 18, pairs = [[81, 82], [10, 11], [87, 89], [4, 8]], loss = 0.2899807393550873\n",
            "Epoch = 18, pairs = [[90, 91], [56, 64], [66, 67], [67, 69]], loss = 0.550859808921814\n",
            "Epoch = 18, pairs = [[49, 51], [8, 12], [0, 8], [10, 12]], loss = 0.43784099817276\n",
            "Epoch = 18, pairs = [[84, 88], [0, 32], [72, 76], [24, 40]], loss = 1.0159075260162354\n",
            "Epoch = 18, pairs = [[61, 62], [54, 55], [81, 83], [45, 46]], loss = 0.31518256664276123\n",
            "Epoch = 18, pairs = [[82, 86], [74, 76], [5, 6], [80, 84]], loss = 0.41940683126449585\n",
            "Epoch = 18, pairs = [[6, 8], [56, 72], [79, 80], [16, 24]], loss = 0.7617640495300293\n",
            "Epoch = 18, pairs = [[9, 10], [51, 53], [34, 38], [45, 47]], loss = 0.37859368324279785\n",
            "Epoch = 18, pairs = [[75, 76], [78, 80], [33, 35], [84, 85]], loss = 0.2961108386516571\n",
            "Epoch = 18, pairs = [[72, 88], [44, 45], [3, 4], [85, 87]], loss = 0.39862096309661865\n",
            "Epoch = 18, pairs = [[68, 69], [30, 31], [34, 35], [70, 74]], loss = 0.3015146851539612\n",
            "Epoch = 18, pairs = [[47, 49], [19, 21], [73, 74], [69, 70]], loss = 0.3476867973804474\n",
            "Epoch = 18, pairs = [[89, 90], [68, 76], [83, 84], [24, 32]], loss = 0.4758766293525696\n",
            "Epoch = 18, pairs = [[32, 34], [62, 63], [38, 42], [39, 41]], loss = 0.5144819021224976\n",
            "Epoch = 18, pairs = [[36, 44], [58, 60], [6, 7], [60, 64]], loss = 0.5256798267364502\n",
            "Epoch = 18, pairs = [[26, 30], [40, 48], [82, 83], [68, 70]], loss = 0.5127531290054321\n",
            "Epoch 18 took 85.36s.\n",
            "( 0,  1): reprojection: 0.233501, disparity: 0.057340\n",
            "( 0,  2): reprojection: 0.345336, disparity: 0.056242\n",
            "( 0,  4): reprojection: 0.383679, disparity: 0.065227\n",
            "( 0,  8): reprojection: 0.471803, disparity: 0.077830\n",
            "( 0, 16): reprojection: 0.761051, disparity: 0.093590\n",
            "( 0, 32): reprojection: 1.227199, disparity: 0.181688\n",
            "( 1,  2): reprojection: 0.195674, disparity: 0.043919\n",
            "( 1,  3): reprojection: 0.248290, disparity: 0.048249\n",
            "( 2,  3): reprojection: 0.108680, disparity: 0.042003\n",
            "( 2,  4): reprojection: 0.173919, disparity: 0.047022\n",
            "( 2,  6): reprojection: 0.342464, disparity: 0.059136\n",
            "( 3,  4): reprojection: 0.106438, disparity: 0.042389\n",
            "( 3,  5): reprojection: 0.198854, disparity: 0.047602\n",
            "( 4,  5): reprojection: 0.153949, disparity: 0.045229\n",
            "( 4,  6): reprojection: 0.218696, disparity: 0.047591\n",
            "( 4,  8): reprojection: 0.287949, disparity: 0.060650\n",
            "( 4, 12): reprojection: 0.397314, disparity: 0.072525\n",
            "( 5,  6): reprojection: 0.115697, disparity: 0.044261\n",
            "( 5,  7): reprojection: 0.149389, disparity: 0.046484\n",
            "( 6,  7): reprojection: 0.094611, disparity: 0.043605\n",
            "( 6,  8): reprojection: 0.162900, disparity: 0.047923\n",
            "( 6, 10): reprojection: 0.384995, disparity: 0.057196\n",
            "( 7,  8): reprojection: 0.086735, disparity: 0.043807\n",
            "( 7,  9): reprojection: 0.189933, disparity: 0.049326\n",
            "( 8,  9): reprojection: 0.159014, disparity: 0.044236\n",
            "( 8, 10): reprojection: 0.354163, disparity: 0.048677\n",
            "( 8, 12): reprojection: 0.503046, disparity: 0.056242\n",
            "( 8, 16): reprojection: 0.618230, disparity: 0.072144\n",
            "( 8, 24): reprojection: 0.727276, disparity: 0.101964\n",
            "( 9, 10): reprojection: 0.244536, disparity: 0.042902\n",
            "( 9, 11): reprojection: 0.362675, disparity: 0.049580\n",
            "(10, 11): reprojection: 0.159101, disparity: 0.041156\n",
            "(10, 12): reprojection: 0.193583, disparity: 0.048857\n",
            "(10, 14): reprojection: 0.315061, disparity: 0.065459\n",
            "(11, 12): reprojection: 0.131431, disparity: 0.042654\n",
            "(11, 13): reprojection: 0.207570, disparity: 0.052518\n",
            "(12, 13): reprojection: 0.128820, disparity: 0.042862\n",
            "(12, 14): reprojection: 0.255110, disparity: 0.052782\n",
            "(12, 16): reprojection: 0.427222, disparity: 0.070511\n",
            "(12, 20): reprojection: 0.678117, disparity: 0.079181\n",
            "(13, 14): reprojection: 0.182173, disparity: 0.047511\n",
            "(13, 15): reprojection: 0.229731, disparity: 0.053978\n",
            "(14, 15): reprojection: 0.183062, disparity: 0.044993\n",
            "(14, 16): reprojection: 0.419663, disparity: 0.054100\n",
            "(14, 18): reprojection: 0.339625, disparity: 0.060966\n",
            "(15, 16): reprojection: 0.293030, disparity: 0.045902\n",
            "(15, 17): reprojection: 0.371906, disparity: 0.053677\n",
            "(16, 17): reprojection: 0.137068, disparity: 0.045634\n",
            "(16, 18): reprojection: 0.314474, disparity: 0.055771\n",
            "(16, 20): reprojection: 0.559313, disparity: 0.067400\n",
            "(16, 24): reprojection: 0.825166, disparity: 0.086370\n",
            "(16, 32): reprojection: 1.215826, disparity: 0.144482\n",
            "(16, 48): reprojection: 1.772002, disparity: 0.340410\n",
            "(17, 18): reprojection: 0.311286, disparity: 0.050481\n",
            "(17, 19): reprojection: 0.479077, disparity: 0.061589\n",
            "(18, 19): reprojection: 0.213779, disparity: 0.047923\n",
            "(18, 20): reprojection: 0.302699, disparity: 0.056101\n",
            "(18, 22): reprojection: 0.493057, disparity: 0.066327\n",
            "(19, 20): reprojection: 0.114974, disparity: 0.046277\n",
            "(19, 21): reprojection: 0.254591, disparity: 0.055329\n",
            "(20, 21): reprojection: 0.188422, disparity: 0.049352\n",
            "(20, 22): reprojection: 0.287321, disparity: 0.057246\n",
            "(20, 24): reprojection: 0.393321, disparity: 0.066744\n",
            "(20, 28): reprojection: 0.666097, disparity: 0.098247\n",
            "(21, 22): reprojection: 0.154545, disparity: 0.048149\n",
            "(21, 23): reprojection: 0.240116, disparity: 0.055827\n",
            "(22, 23): reprojection: 0.117585, disparity: 0.047000\n",
            "(22, 24): reprojection: 0.176713, disparity: 0.057349\n",
            "(22, 26): reprojection: 0.313714, disparity: 0.073481\n",
            "(23, 24): reprojection: 0.106158, disparity: 0.049384\n",
            "(23, 25): reprojection: 0.183813, disparity: 0.058970\n",
            "(24, 25): reprojection: 0.134587, disparity: 0.050498\n",
            "(24, 26): reprojection: 0.187160, disparity: 0.059543\n",
            "(24, 28): reprojection: 0.294211, disparity: 0.078411\n",
            "(24, 32): reprojection: 0.569924, disparity: 0.102391\n",
            "(24, 40): reprojection: 0.954813, disparity: 0.194614\n",
            "(25, 26): reprojection: 0.130858, disparity: 0.053523\n",
            "(25, 27): reprojection: 0.229358, disparity: 0.061624\n",
            "(26, 27): reprojection: 0.140939, disparity: 0.052424\n",
            "(26, 28): reprojection: 0.177885, disparity: 0.062525\n",
            "(26, 30): reprojection: 0.306086, disparity: 0.073345\n",
            "(27, 28): reprojection: 0.144917, disparity: 0.050288\n",
            "(27, 29): reprojection: 0.257517, disparity: 0.057886\n",
            "(28, 29): reprojection: 0.179456, disparity: 0.052946\n",
            "(28, 30): reprojection: 0.238471, disparity: 0.062268\n",
            "(28, 32): reprojection: 0.445406, disparity: 0.071875\n",
            "(28, 36): reprojection: 0.719888, disparity: 0.107959\n",
            "(29, 30): reprojection: 0.128377, disparity: 0.050455\n",
            "(29, 31): reprojection: 0.270143, disparity: 0.056135\n",
            "(30, 31): reprojection: 0.183310, disparity: 0.050707\n",
            "(30, 32): reprojection: 0.269701, disparity: 0.059007\n",
            "(30, 34): reprojection: 0.509590, disparity: 0.070270\n",
            "(31, 32): reprojection: 0.131657, disparity: 0.052528\n",
            "(31, 33): reprojection: 0.314481, disparity: 0.059852\n",
            "(32, 33): reprojection: 0.250005, disparity: 0.050704\n",
            "(32, 34): reprojection: 0.256124, disparity: 0.056923\n",
            "(32, 36): reprojection: 0.372701, disparity: 0.078457\n",
            "(32, 40): reprojection: 0.545097, disparity: 0.118863\n",
            "(32, 48): reprojection: 1.098165, disparity: 0.139542\n",
            "(32, 64): reprojection: 1.742450, disparity: 0.232510\n",
            "(33, 34): reprojection: 0.149397, disparity: 0.049014\n",
            "(33, 35): reprojection: 0.267448, disparity: 0.056135\n",
            "(34, 35): reprojection: 0.188538, disparity: 0.050076\n",
            "(34, 36): reprojection: 0.293053, disparity: 0.063053\n",
            "(34, 38): reprojection: 0.383555, disparity: 0.074114\n",
            "(35, 36): reprojection: 0.234924, disparity: 0.054793\n",
            "(35, 37): reprojection: 0.277467, disparity: 0.071484\n",
            "(36, 37): reprojection: 0.260926, disparity: 0.055762\n",
            "(36, 38): reprojection: 0.370850, disparity: 0.065404\n",
            "(36, 40): reprojection: 0.325093, disparity: 0.083427\n",
            "(36, 44): reprojection: 0.709005, disparity: 0.101447\n",
            "(37, 38): reprojection: 0.221247, disparity: 0.051579\n",
            "(37, 39): reprojection: 0.250575, disparity: 0.059800\n",
            "(38, 39): reprojection: 0.197639, disparity: 0.055775\n",
            "(38, 40): reprojection: 0.374611, disparity: 0.071929\n",
            "(38, 42): reprojection: 0.665176, disparity: 0.084452\n",
            "(39, 40): reprojection: 0.280896, disparity: 0.058558\n",
            "(39, 41): reprojection: 0.494705, disparity: 0.066871\n",
            "(40, 41): reprojection: 0.294212, disparity: 0.055749\n",
            "(40, 42): reprojection: 0.420359, disparity: 0.062552\n",
            "(40, 44): reprojection: 0.577137, disparity: 0.076410\n",
            "(40, 48): reprojection: 0.817424, disparity: 0.102174\n",
            "(40, 56): reprojection: 0.915548, disparity: 0.132969\n",
            "(41, 42): reprojection: 0.193372, disparity: 0.055902\n",
            "(41, 43): reprojection: 0.314873, disparity: 0.063387\n",
            "(42, 43): reprojection: 0.181834, disparity: 0.056011\n",
            "(42, 44): reprojection: 0.340074, disparity: 0.067377\n",
            "(42, 46): reprojection: 0.552319, disparity: 0.081626\n",
            "(43, 44): reprojection: 0.214116, disparity: 0.059220\n",
            "(43, 45): reprojection: 0.305982, disparity: 0.069927\n",
            "(44, 45): reprojection: 0.149905, disparity: 0.056991\n",
            "(44, 46): reprojection: 0.290053, disparity: 0.071899\n",
            "(44, 48): reprojection: 0.425922, disparity: 0.087167\n",
            "(44, 52): reprojection: 0.632630, disparity: 0.106602\n",
            "(45, 46): reprojection: 0.207748, disparity: 0.059781\n",
            "(45, 47): reprojection: 0.337265, disparity: 0.081536\n",
            "(46, 47): reprojection: 0.196196, disparity: 0.059827\n",
            "(46, 48): reprojection: 0.296941, disparity: 0.072811\n",
            "(46, 50): reprojection: 0.408077, disparity: 0.081849\n",
            "(47, 48): reprojection: 0.199901, disparity: 0.060136\n",
            "(47, 49): reprojection: 0.258592, disparity: 0.065398\n",
            "(48, 49): reprojection: 0.172077, disparity: 0.063862\n",
            "(48, 50): reprojection: 0.247723, disparity: 0.072272\n",
            "(48, 52): reprojection: 0.358835, disparity: 0.086253\n",
            "(48, 56): reprojection: 0.670816, disparity: 0.095318\n",
            "(48, 64): reprojection: 1.070586, disparity: 0.142769\n",
            "(48, 80): reprojection: 2.156334, disparity: 0.159885\n",
            "(49, 50): reprojection: 0.149310, disparity: 0.060132\n",
            "(49, 51): reprojection: 0.243878, disparity: 0.068430\n",
            "(50, 51): reprojection: 0.148962, disparity: 0.059733\n",
            "(50, 52): reprojection: 0.251426, disparity: 0.067106\n",
            "(50, 54): reprojection: 0.387226, disparity: 0.079981\n",
            "(51, 52): reprojection: 0.164073, disparity: 0.058370\n",
            "(51, 53): reprojection: 0.302117, disparity: 0.065005\n",
            "(52, 53): reprojection: 0.170670, disparity: 0.057503\n",
            "(52, 54): reprojection: 0.253084, disparity: 0.068189\n",
            "(52, 56): reprojection: 0.536145, disparity: 0.070921\n",
            "(52, 60): reprojection: 0.698782, disparity: 0.097107\n",
            "(53, 54): reprojection: 0.134043, disparity: 0.058627\n",
            "(53, 55): reprojection: 0.324166, disparity: 0.072658\n",
            "(54, 55): reprojection: 0.298494, disparity: 0.058600\n",
            "(54, 56): reprojection: 0.557496, disparity: 0.064706\n",
            "(54, 58): reprojection: 0.410518, disparity: 0.073786\n",
            "(55, 56): reprojection: 0.348002, disparity: 0.063609\n",
            "(55, 57): reprojection: 0.402727, disparity: 0.064393\n",
            "(56, 57): reprojection: 0.303996, disparity: 0.061682\n",
            "(56, 58): reprojection: 0.552606, disparity: 0.071126\n",
            "(56, 60): reprojection: 0.703552, disparity: 0.090192\n",
            "(56, 64): reprojection: 0.974972, disparity: 0.093374\n",
            "(56, 72): reprojection: 1.407687, disparity: 0.120694\n",
            "(57, 58): reprojection: 0.353533, disparity: 0.064383\n",
            "(57, 59): reprojection: 0.474921, disparity: 0.072225\n",
            "(58, 59): reprojection: 0.242032, disparity: 0.065516\n",
            "(58, 60): reprojection: 0.374440, disparity: 0.078082\n",
            "(58, 62): reprojection: 0.499866, disparity: 0.091620\n",
            "(59, 60): reprojection: 0.199412, disparity: 0.066078\n",
            "(59, 61): reprojection: 0.314158, disparity: 0.077345\n",
            "(60, 61): reprojection: 0.219046, disparity: 0.069125\n",
            "(60, 62): reprojection: 0.319108, disparity: 0.073539\n",
            "(60, 64): reprojection: 0.507715, disparity: 0.078554\n",
            "(60, 68): reprojection: 0.713409, disparity: 0.106488\n",
            "(61, 62): reprojection: 0.212275, disparity: 0.067667\n",
            "(61, 63): reprojection: 0.314380, disparity: 0.071235\n",
            "(62, 63): reprojection: 0.227862, disparity: 0.065622\n",
            "(62, 64): reprojection: 0.346263, disparity: 0.072498\n",
            "(62, 66): reprojection: 0.527778, disparity: 0.098848\n",
            "(63, 64): reprojection: 0.214176, disparity: 0.066909\n",
            "(63, 65): reprojection: 0.350508, disparity: 0.069266\n",
            "(64, 65): reprojection: 0.224145, disparity: 0.065806\n",
            "(64, 66): reprojection: 0.372097, disparity: 0.068977\n",
            "(64, 68): reprojection: 0.475051, disparity: 0.090277\n",
            "(64, 72): reprojection: 0.772554, disparity: 0.110738\n",
            "(64, 80): reprojection: 1.345503, disparity: 0.149009\n",
            "(65, 66): reprojection: 0.270336, disparity: 0.063419\n",
            "(65, 67): reprojection: 0.410431, disparity: 0.072213\n",
            "(66, 67): reprojection: 0.255586, disparity: 0.067819\n",
            "(66, 68): reprojection: 0.372126, disparity: 0.069254\n",
            "(66, 70): reprojection: 0.468501, disparity: 0.086082\n",
            "(67, 68): reprojection: 0.337465, disparity: 0.060540\n",
            "(67, 69): reprojection: 0.332312, disparity: 0.065725\n",
            "(68, 69): reprojection: 0.200068, disparity: 0.060463\n",
            "(68, 70): reprojection: 0.432026, disparity: 0.066573\n",
            "(68, 72): reprojection: 0.548789, disparity: 0.077796\n",
            "(68, 76): reprojection: 0.744557, disparity: 0.091870\n",
            "(69, 70): reprojection: 0.291963, disparity: 0.060269\n",
            "(69, 71): reprojection: 0.420565, disparity: 0.064955\n",
            "(70, 71): reprojection: 0.209144, disparity: 0.060501\n",
            "(70, 72): reprojection: 0.286081, disparity: 0.063622\n",
            "(70, 74): reprojection: 0.480436, disparity: 0.070320\n",
            "(71, 72): reprojection: 0.231412, disparity: 0.060456\n",
            "(71, 73): reprojection: 0.353953, disparity: 0.065518\n",
            "(72, 73): reprojection: 0.226723, disparity: 0.061519\n",
            "(72, 74): reprojection: 0.318777, disparity: 0.065747\n",
            "(72, 76): reprojection: 0.415288, disparity: 0.075040\n",
            "(72, 80): reprojection: 0.754587, disparity: 0.109476\n",
            "(72, 88): reprojection: 0.952839, disparity: 0.116045\n",
            "(73, 74): reprojection: 0.313584, disparity: 0.061096\n",
            "(73, 75): reprojection: 0.433050, disparity: 0.067602\n",
            "(74, 75): reprojection: 0.235161, disparity: 0.060438\n",
            "(74, 76): reprojection: 0.276913, disparity: 0.064179\n",
            "(74, 78): reprojection: 0.399263, disparity: 0.078507\n",
            "(75, 76): reprojection: 0.223558, disparity: 0.058543\n",
            "(75, 77): reprojection: 0.347378, disparity: 0.065153\n",
            "(76, 77): reprojection: 0.227233, disparity: 0.057233\n",
            "(76, 78): reprojection: 0.344386, disparity: 0.063977\n",
            "(76, 80): reprojection: 0.455476, disparity: 0.080514\n",
            "(76, 84): reprojection: 0.672431, disparity: 0.091810\n",
            "(77, 78): reprojection: 0.228963, disparity: 0.059683\n",
            "(77, 79): reprojection: 0.264636, disparity: 0.067659\n",
            "(78, 79): reprojection: 0.196351, disparity: 0.059498\n",
            "(78, 80): reprojection: 0.258581, disparity: 0.067749\n",
            "(78, 82): reprojection: 0.411662, disparity: 0.070569\n",
            "(79, 80): reprojection: 0.152933, disparity: 0.058702\n",
            "(79, 81): reprojection: 0.257755, disparity: 0.060448\n",
            "(80, 81): reprojection: 0.153069, disparity: 0.057495\n",
            "(80, 82): reprojection: 0.268093, disparity: 0.064992\n",
            "(80, 84): reprojection: 0.461223, disparity: 0.070621\n",
            "(80, 88): reprojection: 0.724926, disparity: 0.084076\n",
            "(81, 82): reprojection: 0.185254, disparity: 0.055839\n",
            "(81, 83): reprojection: 0.283239, disparity: 0.061637\n",
            "(82, 83): reprojection: 0.145294, disparity: 0.055729\n",
            "(82, 84): reprojection: 0.255521, disparity: 0.061019\n",
            "(82, 86): reprojection: 0.436232, disparity: 0.075540\n",
            "(83, 84): reprojection: 0.143490, disparity: 0.053356\n",
            "(83, 85): reprojection: 0.295850, disparity: 0.061300\n",
            "(84, 85): reprojection: 0.170332, disparity: 0.051588\n",
            "(84, 86): reprojection: 0.251073, disparity: 0.059465\n",
            "(84, 88): reprojection: 0.482131, disparity: 0.062702\n",
            "(85, 86): reprojection: 0.162535, disparity: 0.052043\n",
            "(85, 87): reprojection: 0.278888, disparity: 0.056554\n",
            "(86, 87): reprojection: 0.231936, disparity: 0.053724\n",
            "(86, 88): reprojection: 0.318433, disparity: 0.064038\n",
            "(86, 90): reprojection: 0.493395, disparity: 0.067492\n",
            "(87, 88): reprojection: 0.170129, disparity: 0.053723\n",
            "(87, 89): reprojection: 0.280691, disparity: 0.060092\n",
            "(88, 89): reprojection: 0.145455, disparity: 0.049904\n",
            "(88, 90): reprojection: 0.313219, disparity: 0.054473\n",
            "(89, 90): reprojection: 0.214135, disparity: 0.048725\n",
            "(89, 91): reprojection: 0.410801, disparity: 0.061745\n",
            "(90, 91): reprojection: 0.322751, disparity: 0.056116\n",
            "Mean:     reprojection: 0.322751, disparity: 0.056116\n",
            "Done Validation for epoch 19 (4940 iterations)\n",
            "Epoch = 19, pairs = [[32, 64], [11, 13], [76, 80], [68, 76]], loss = 0.8832598924636841\n",
            "Epoch = 19, pairs = [[40, 41], [29, 30], [88, 89], [84, 86]], loss = 0.2650686204433441\n",
            "Epoch = 19, pairs = [[37, 38], [36, 38], [4, 6], [80, 88]], loss = 0.4348997473716736\n",
            "Epoch = 19, pairs = [[12, 13], [27, 28], [83, 84], [6, 10]], loss = 0.2677370607852936\n",
            "Epoch = 19, pairs = [[52, 54], [10, 12], [73, 74], [8, 9]], loss = 0.31289052963256836\n",
            "Epoch = 19, pairs = [[58, 62], [36, 40], [13, 14], [16, 32]], loss = 0.6960235238075256\n",
            "Epoch = 19, pairs = [[10, 11], [16, 24], [64, 65], [75, 76]], loss = 0.4687264561653137\n",
            "Epoch = 19, pairs = [[9, 11], [42, 43], [46, 50], [8, 16]], loss = 0.4979921579360962\n",
            "Epoch = 19, pairs = [[72, 88], [10, 14], [72, 80], [56, 58]], loss = 0.7320870161056519\n",
            "Epoch = 19, pairs = [[36, 44], [89, 91], [60, 64], [85, 87]], loss = 0.5281895995140076\n",
            "Epoch = 19, pairs = [[21, 23], [46, 47], [86, 88], [80, 84]], loss = 0.36483174562454224\n",
            "Epoch = 19, pairs = [[56, 60], [12, 14], [17, 18], [0, 32]], loss = 0.8703930377960205\n",
            "Epoch = 19, pairs = [[21, 22], [64, 66], [24, 40], [52, 60]], loss = 0.6658583879470825\n",
            "Epoch = 19, pairs = [[59, 61], [38, 42], [84, 85], [19, 20]], loss = 0.40931472182273865\n",
            "Epoch = 19, pairs = [[3, 4], [33, 35], [28, 30], [23, 25]], loss = 0.28976595401763916\n",
            "Epoch = 19, pairs = [[64, 72], [64, 68], [51, 52], [54, 55]], loss = 0.9032127857208252\n",
            "Epoch = 19, pairs = [[70, 74], [62, 66], [11, 12], [56, 57]], loss = 0.5203694701194763\n",
            "Epoch = 19, pairs = [[44, 52], [16, 20], [52, 53], [32, 48]], loss = 0.7231305837631226\n",
            "Epoch = 19, pairs = [[18, 20], [87, 89], [56, 64], [8, 24]], loss = 0.6916139125823975\n",
            "Epoch = 19, pairs = [[44, 45], [53, 55], [0, 1], [48, 52]], loss = 0.3620661795139313\n",
            "Epoch = 19, pairs = [[30, 34], [18, 19], [22, 24], [84, 88]], loss = 0.4824365973472595\n",
            "Epoch = 19, pairs = [[14, 15], [19, 21], [4, 5], [16, 48]], loss = 0.9241421222686768\n",
            "Epoch = 19, pairs = [[78, 82], [32, 34], [85, 86], [48, 50]], loss = 0.45352011919021606\n",
            "Epoch = 19, pairs = [[14, 16], [90, 91], [22, 23], [61, 62]], loss = 0.36832955479621887\n",
            "Epoch = 19, pairs = [[49, 50], [20, 28], [50, 52], [25, 26]], loss = 0.6443307995796204\n",
            "Epoch = 19, pairs = [[13, 15], [44, 48], [18, 22], [57, 59]], loss = 0.6391971111297607\n",
            "Epoch = 19, pairs = [[2, 3], [89, 90], [39, 40], [53, 54]], loss = 0.2471102774143219\n",
            "Epoch = 19, pairs = [[16, 17], [47, 48], [72, 74], [41, 42]], loss = 0.3143397867679596\n",
            "Epoch = 19, pairs = [[55, 56], [17, 19], [22, 26], [76, 84]], loss = 0.6916115283966064\n",
            "Epoch = 19, pairs = [[33, 34], [4, 8], [79, 81], [65, 67]], loss = 0.369733601808548\n",
            "Epoch = 19, pairs = [[88, 90], [70, 72], [82, 84], [38, 39]], loss = 0.3753109574317932\n",
            "Epoch = 19, pairs = [[60, 62], [82, 83], [40, 44], [69, 70]], loss = 0.44783079624176025\n",
            "Epoch = 19, pairs = [[71, 73], [45, 46], [6, 8], [50, 54]], loss = 0.3890138864517212\n",
            "Epoch = 19, pairs = [[60, 68], [39, 41], [47, 49], [58, 60]], loss = 0.5612231492996216\n",
            "Epoch = 19, pairs = [[2, 4], [20, 22], [7, 9], [80, 81]], loss = 0.27670031785964966\n",
            "Epoch = 19, pairs = [[49, 51], [42, 46], [1, 2], [48, 80]], loss = 0.7934708595275879\n",
            "Epoch = 19, pairs = [[68, 72], [81, 83], [38, 40], [76, 77]], loss = 0.44095420837402344\n",
            "Epoch = 19, pairs = [[83, 85], [51, 53], [27, 29], [34, 35]], loss = 0.3393101990222931\n",
            "Epoch = 19, pairs = [[24, 32], [66, 67], [2, 6], [36, 37]], loss = 0.462546706199646\n",
            "Epoch = 19, pairs = [[43, 45], [8, 12], [20, 21], [28, 29]], loss = 0.38741278648376465\n",
            "Epoch = 19, pairs = [[24, 26], [61, 63], [44, 46], [24, 28]], loss = 0.39056628942489624\n",
            "Epoch = 19, pairs = [[45, 47], [16, 18], [74, 76], [40, 42]], loss = 0.4289865493774414\n",
            "Epoch = 19, pairs = [[66, 70], [65, 66], [35, 36], [48, 49]], loss = 0.3997069299221039\n",
            "Epoch = 19, pairs = [[30, 31], [54, 58], [20, 24], [67, 69]], loss = 0.43104565143585205\n",
            "Epoch = 19, pairs = [[23, 24], [29, 31], [58, 59], [80, 82]], loss = 0.2870931029319763\n",
            "Epoch = 19, pairs = [[86, 90], [32, 33], [28, 36], [68, 70]], loss = 0.5830708742141724\n",
            "Epoch = 19, pairs = [[34, 36], [76, 78], [0, 4], [50, 51]], loss = 0.3554372787475586\n",
            "Epoch = 19, pairs = [[43, 44], [26, 30], [35, 37], [0, 16]], loss = 0.4892912805080414\n",
            "Epoch = 19, pairs = [[5, 6], [75, 77], [41, 43], [14, 18]], loss = 0.35918325185775757\n",
            "Epoch = 19, pairs = [[25, 27], [55, 57], [26, 27], [78, 79]], loss = 0.3042335510253906\n",
            "Epoch = 19, pairs = [[74, 78], [42, 44], [69, 71], [79, 80]], loss = 0.4246305823326111\n",
            "Epoch = 19, pairs = [[1, 3], [86, 87], [60, 61], [57, 58]], loss = 0.3319379687309265\n",
            "Epoch = 19, pairs = [[34, 38], [9, 10], [63, 65], [7, 8]], loss = 0.34023523330688477\n",
            "Epoch = 19, pairs = [[31, 32], [70, 71], [52, 56], [30, 32]], loss = 0.3569544553756714\n",
            "Epoch = 19, pairs = [[77, 79], [48, 56], [68, 69], [54, 56]], loss = 0.5112143158912659\n",
            "Epoch = 19, pairs = [[0, 8], [12, 16], [6, 7], [74, 75]], loss = 0.34413570165634155\n",
            "Epoch = 19, pairs = [[77, 78], [15, 16], [71, 72], [62, 63]], loss = 0.3108713626861572\n",
            "Epoch = 19, pairs = [[48, 64], [40, 56], [67, 68], [81, 82]], loss = 0.7324692606925964\n",
            "Epoch = 19, pairs = [[46, 48], [32, 40], [15, 17], [66, 68]], loss = 0.46870771050453186\n",
            "Epoch = 19, pairs = [[72, 73], [28, 32], [78, 80], [40, 48]], loss = 0.49228549003601074\n",
            "Epoch = 19, pairs = [[3, 5], [24, 25], [72, 76], [32, 36]], loss = 0.3338007628917694\n",
            "Epoch = 19, pairs = [[26, 28], [0, 2], [62, 64], [37, 39]], loss = 0.337421178817749\n",
            "Epoch = 19, pairs = [[31, 33], [56, 72], [12, 20], [64, 80]], loss = 0.9181865453720093\n",
            "Epoch = 19, pairs = [[82, 86], [63, 64], [87, 88], [4, 12]], loss = 0.38052529096603394\n",
            "Epoch = 19, pairs = [[5, 7], [8, 10], [73, 75], [59, 60]], loss = 0.3455626666545868\n",
            "Epoch 19 took 85.36s.\n",
            "( 0,  1): reprojection: 0.239238, disparity: 0.053219\n",
            "( 0,  2): reprojection: 0.357756, disparity: 0.053973\n",
            "( 0,  4): reprojection: 0.404281, disparity: 0.063573\n",
            "( 0,  8): reprojection: 0.485747, disparity: 0.073095\n",
            "( 0, 16): reprojection: 0.850911, disparity: 0.078108\n",
            "( 0, 32): reprojection: 1.589512, disparity: 0.105342\n",
            "( 1,  2): reprojection: 0.196535, disparity: 0.045635\n",
            "( 1,  3): reprojection: 0.249020, disparity: 0.049854\n",
            "( 2,  3): reprojection: 0.109878, disparity: 0.044839\n",
            "( 2,  4): reprojection: 0.173102, disparity: 0.051542\n",
            "( 2,  6): reprojection: 0.333851, disparity: 0.060064\n",
            "( 3,  4): reprojection: 0.112621, disparity: 0.046161\n",
            "( 3,  5): reprojection: 0.192041, disparity: 0.052177\n",
            "( 4,  5): reprojection: 0.151034, disparity: 0.046645\n",
            "( 4,  6): reprojection: 0.211883, disparity: 0.049363\n",
            "( 4,  8): reprojection: 0.288629, disparity: 0.059925\n",
            "( 4, 12): reprojection: 0.378730, disparity: 0.073566\n",
            "( 5,  6): reprojection: 0.124109, disparity: 0.045642\n",
            "( 5,  7): reprojection: 0.166777, disparity: 0.049758\n",
            "( 6,  7): reprojection: 0.103397, disparity: 0.044672\n",
            "( 6,  8): reprojection: 0.171264, disparity: 0.049377\n",
            "( 6, 10): reprojection: 0.400609, disparity: 0.060154\n",
            "( 7,  8): reprojection: 0.095973, disparity: 0.044417\n",
            "( 7,  9): reprojection: 0.199713, disparity: 0.049865\n",
            "( 8,  9): reprojection: 0.168878, disparity: 0.045889\n",
            "( 8, 10): reprojection: 0.361832, disparity: 0.050924\n",
            "( 8, 12): reprojection: 0.487497, disparity: 0.061732\n",
            "( 8, 16): reprojection: 0.670684, disparity: 0.073758\n",
            "( 8, 24): reprojection: 0.948268, disparity: 0.088189\n",
            "( 9, 10): reprojection: 0.247426, disparity: 0.045897\n",
            "( 9, 11): reprojection: 0.369164, disparity: 0.052401\n",
            "(10, 11): reprojection: 0.159031, disparity: 0.043423\n",
            "(10, 12): reprojection: 0.200927, disparity: 0.050478\n",
            "(10, 14): reprojection: 0.347464, disparity: 0.063694\n",
            "(11, 12): reprojection: 0.131451, disparity: 0.043541\n",
            "(11, 13): reprojection: 0.214853, disparity: 0.050860\n",
            "(12, 13): reprojection: 0.148883, disparity: 0.043340\n",
            "(12, 14): reprojection: 0.286888, disparity: 0.050904\n",
            "(12, 16): reprojection: 0.504493, disparity: 0.070584\n",
            "(12, 20): reprojection: 0.805808, disparity: 0.074004\n",
            "(13, 14): reprojection: 0.183629, disparity: 0.046261\n",
            "(13, 15): reprojection: 0.277655, disparity: 0.052796\n",
            "(14, 15): reprojection: 0.209111, disparity: 0.045843\n",
            "(14, 16): reprojection: 0.450806, disparity: 0.054580\n",
            "(14, 18): reprojection: 0.425275, disparity: 0.064600\n",
            "(15, 16): reprojection: 0.306068, disparity: 0.048401\n",
            "(15, 17): reprojection: 0.395909, disparity: 0.056810\n",
            "(16, 17): reprojection: 0.151290, disparity: 0.047805\n",
            "(16, 18): reprojection: 0.343124, disparity: 0.057816\n",
            "(16, 20): reprojection: 0.563272, disparity: 0.067532\n",
            "(16, 24): reprojection: 0.899667, disparity: 0.080720\n",
            "(16, 32): reprojection: 1.558077, disparity: 0.104723\n",
            "(16, 48): reprojection: 1.571644, disparity: 0.217335\n",
            "(17, 18): reprojection: 0.325159, disparity: 0.052263\n",
            "(17, 19): reprojection: 0.497298, disparity: 0.061426\n",
            "(18, 19): reprojection: 0.217565, disparity: 0.048388\n",
            "(18, 20): reprojection: 0.290116, disparity: 0.054954\n",
            "(18, 22): reprojection: 0.534914, disparity: 0.063880\n",
            "(19, 20): reprojection: 0.117347, disparity: 0.046460\n",
            "(19, 21): reprojection: 0.257804, disparity: 0.055155\n",
            "(20, 21): reprojection: 0.210691, disparity: 0.049932\n",
            "(20, 22): reprojection: 0.327015, disparity: 0.058611\n",
            "(20, 24): reprojection: 0.447288, disparity: 0.065927\n",
            "(20, 28): reprojection: 0.824836, disparity: 0.084410\n",
            "(21, 22): reprojection: 0.181718, disparity: 0.049346\n",
            "(21, 23): reprojection: 0.287024, disparity: 0.056740\n",
            "(22, 23): reprojection: 0.143577, disparity: 0.049097\n",
            "(22, 24): reprojection: 0.208935, disparity: 0.057674\n",
            "(22, 26): reprojection: 0.353851, disparity: 0.072194\n",
            "(23, 24): reprojection: 0.118749, disparity: 0.049696\n",
            "(23, 25): reprojection: 0.206268, disparity: 0.058522\n",
            "(24, 25): reprojection: 0.161401, disparity: 0.051613\n",
            "(24, 26): reprojection: 0.209416, disparity: 0.061447\n",
            "(24, 28): reprojection: 0.352412, disparity: 0.074114\n",
            "(24, 32): reprojection: 0.757577, disparity: 0.083752\n",
            "(24, 40): reprojection: 1.229100, disparity: 0.123767\n",
            "(25, 26): reprojection: 0.135482, disparity: 0.054824\n",
            "(25, 27): reprojection: 0.238010, disparity: 0.061677\n",
            "(26, 27): reprojection: 0.160866, disparity: 0.051725\n",
            "(26, 28): reprojection: 0.205114, disparity: 0.060277\n",
            "(26, 30): reprojection: 0.383340, disparity: 0.068636\n",
            "(27, 28): reprojection: 0.159932, disparity: 0.051150\n",
            "(27, 29): reprojection: 0.280596, disparity: 0.058852\n",
            "(28, 29): reprojection: 0.199346, disparity: 0.053163\n",
            "(28, 30): reprojection: 0.262414, disparity: 0.059431\n",
            "(28, 32): reprojection: 0.523067, disparity: 0.068264\n",
            "(28, 36): reprojection: 0.883676, disparity: 0.085277\n",
            "(29, 30): reprojection: 0.143639, disparity: 0.050112\n",
            "(29, 31): reprojection: 0.306373, disparity: 0.057556\n",
            "(30, 31): reprojection: 0.208871, disparity: 0.051879\n",
            "(30, 32): reprojection: 0.309653, disparity: 0.058240\n",
            "(30, 34): reprojection: 0.557980, disparity: 0.070451\n",
            "(31, 32): reprojection: 0.149043, disparity: 0.050018\n",
            "(31, 33): reprojection: 0.343359, disparity: 0.060970\n",
            "(32, 33): reprojection: 0.267696, disparity: 0.053941\n",
            "(32, 34): reprojection: 0.290059, disparity: 0.061241\n",
            "(32, 36): reprojection: 0.438990, disparity: 0.068346\n",
            "(32, 40): reprojection: 0.718783, disparity: 0.089043\n",
            "(32, 48): reprojection: 1.266899, disparity: 0.138981\n",
            "(32, 64): reprojection: 1.388515, disparity: 0.210002\n",
            "(33, 34): reprojection: 0.153042, disparity: 0.050856\n",
            "(33, 35): reprojection: 0.279581, disparity: 0.055696\n",
            "(34, 35): reprojection: 0.198489, disparity: 0.049635\n",
            "(34, 36): reprojection: 0.321034, disparity: 0.061825\n",
            "(34, 38): reprojection: 0.424022, disparity: 0.068270\n",
            "(35, 36): reprojection: 0.254643, disparity: 0.054102\n",
            "(35, 37): reprojection: 0.325752, disparity: 0.064573\n",
            "(36, 37): reprojection: 0.275641, disparity: 0.055973\n",
            "(36, 38): reprojection: 0.383397, disparity: 0.067062\n",
            "(36, 40): reprojection: 0.371625, disparity: 0.075250\n",
            "(36, 44): reprojection: 0.769950, disparity: 0.099651\n",
            "(37, 38): reprojection: 0.219856, disparity: 0.052177\n",
            "(37, 39): reprojection: 0.255550, disparity: 0.060738\n",
            "(38, 39): reprojection: 0.204554, disparity: 0.056370\n",
            "(38, 40): reprojection: 0.392843, disparity: 0.067703\n",
            "(38, 42): reprojection: 0.695994, disparity: 0.083962\n",
            "(39, 40): reprojection: 0.292953, disparity: 0.059164\n",
            "(39, 41): reprojection: 0.504079, disparity: 0.069611\n",
            "(40, 41): reprojection: 0.287212, disparity: 0.059710\n",
            "(40, 42): reprojection: 0.423663, disparity: 0.070468\n",
            "(40, 44): reprojection: 0.570899, disparity: 0.088845\n",
            "(40, 48): reprojection: 0.834756, disparity: 0.116458\n",
            "(40, 56): reprojection: 0.980789, disparity: 0.177153\n",
            "(41, 42): reprojection: 0.208250, disparity: 0.058914\n",
            "(41, 43): reprojection: 0.322097, disparity: 0.068891\n",
            "(42, 43): reprojection: 0.178333, disparity: 0.059690\n",
            "(42, 44): reprojection: 0.336452, disparity: 0.072018\n",
            "(42, 46): reprojection: 0.540634, disparity: 0.089052\n",
            "(43, 44): reprojection: 0.222920, disparity: 0.058659\n",
            "(43, 45): reprojection: 0.328923, disparity: 0.075111\n",
            "(44, 45): reprojection: 0.159826, disparity: 0.057697\n",
            "(44, 46): reprojection: 0.291991, disparity: 0.071568\n",
            "(44, 48): reprojection: 0.461121, disparity: 0.091227\n",
            "(44, 52): reprojection: 0.704811, disparity: 0.129988\n",
            "(45, 46): reprojection: 0.210412, disparity: 0.060283\n",
            "(45, 47): reprojection: 0.355408, disparity: 0.078469\n",
            "(46, 47): reprojection: 0.207656, disparity: 0.062445\n",
            "(46, 48): reprojection: 0.304360, disparity: 0.074639\n",
            "(46, 50): reprojection: 0.430351, disparity: 0.090370\n",
            "(47, 48): reprojection: 0.205672, disparity: 0.060989\n",
            "(47, 49): reprojection: 0.277896, disparity: 0.070823\n",
            "(48, 49): reprojection: 0.187589, disparity: 0.063435\n",
            "(48, 50): reprojection: 0.279943, disparity: 0.075286\n",
            "(48, 52): reprojection: 0.386109, disparity: 0.099049\n",
            "(48, 56): reprojection: 0.819261, disparity: 0.125388\n",
            "(48, 64): reprojection: 1.058050, disparity: 0.140958\n",
            "(48, 80): reprojection: 1.721486, disparity: 0.140948\n",
            "(49, 50): reprojection: 0.158637, disparity: 0.060398\n",
            "(49, 51): reprojection: 0.257539, disparity: 0.068755\n",
            "(50, 51): reprojection: 0.160454, disparity: 0.058931\n",
            "(50, 52): reprojection: 0.252020, disparity: 0.070509\n",
            "(50, 54): reprojection: 0.385189, disparity: 0.087224\n",
            "(51, 52): reprojection: 0.163518, disparity: 0.059919\n",
            "(51, 53): reprojection: 0.289262, disparity: 0.071265\n",
            "(52, 53): reprojection: 0.167918, disparity: 0.058562\n",
            "(52, 54): reprojection: 0.255166, disparity: 0.069176\n",
            "(52, 56): reprojection: 0.541514, disparity: 0.076972\n",
            "(52, 60): reprojection: 0.680416, disparity: 0.099389\n",
            "(53, 54): reprojection: 0.145403, disparity: 0.060469\n",
            "(53, 55): reprojection: 0.341775, disparity: 0.067269\n",
            "(54, 55): reprojection: 0.294960, disparity: 0.058760\n",
            "(54, 56): reprojection: 0.569066, disparity: 0.063909\n",
            "(54, 58): reprojection: 0.433658, disparity: 0.077526\n",
            "(55, 56): reprojection: 0.353310, disparity: 0.060062\n",
            "(55, 57): reprojection: 0.406013, disparity: 0.066091\n",
            "(56, 57): reprojection: 0.310774, disparity: 0.062640\n",
            "(56, 58): reprojection: 0.545574, disparity: 0.079560\n",
            "(56, 60): reprojection: 0.683369, disparity: 0.091624\n",
            "(56, 64): reprojection: 0.959251, disparity: 0.105737\n",
            "(56, 72): reprojection: 1.238744, disparity: 0.130852\n",
            "(57, 58): reprojection: 0.354777, disparity: 0.068861\n",
            "(57, 59): reprojection: 0.476108, disparity: 0.076546\n",
            "(58, 59): reprojection: 0.237421, disparity: 0.066298\n",
            "(58, 60): reprojection: 0.376686, disparity: 0.073807\n",
            "(58, 62): reprojection: 0.428975, disparity: 0.089838\n",
            "(59, 60): reprojection: 0.196589, disparity: 0.067204\n",
            "(59, 61): reprojection: 0.315308, disparity: 0.074494\n",
            "(60, 61): reprojection: 0.215803, disparity: 0.072519\n",
            "(60, 62): reprojection: 0.294035, disparity: 0.076896\n",
            "(60, 64): reprojection: 0.472311, disparity: 0.086387\n",
            "(60, 68): reprojection: 0.663393, disparity: 0.108256\n",
            "(61, 62): reprojection: 0.186751, disparity: 0.066822\n",
            "(61, 63): reprojection: 0.295923, disparity: 0.072011\n",
            "(62, 63): reprojection: 0.226667, disparity: 0.063398\n",
            "(62, 64): reprojection: 0.351956, disparity: 0.069952\n",
            "(62, 66): reprojection: 0.606495, disparity: 0.085081\n",
            "(63, 64): reprojection: 0.224039, disparity: 0.063880\n",
            "(63, 65): reprojection: 0.374322, disparity: 0.070375\n",
            "(64, 65): reprojection: 0.250334, disparity: 0.067644\n",
            "(64, 66): reprojection: 0.409390, disparity: 0.070612\n",
            "(64, 68): reprojection: 0.560663, disparity: 0.082124\n",
            "(64, 72): reprojection: 1.005802, disparity: 0.097049\n",
            "(64, 80): reprojection: 1.504437, disparity: 0.112543\n",
            "(65, 66): reprojection: 0.271496, disparity: 0.061665\n",
            "(65, 67): reprojection: 0.425113, disparity: 0.068418\n",
            "(66, 67): reprojection: 0.265395, disparity: 0.065459\n",
            "(66, 68): reprojection: 0.383380, disparity: 0.064811\n",
            "(66, 70): reprojection: 0.490889, disparity: 0.079502\n",
            "(67, 68): reprojection: 0.337151, disparity: 0.061880\n",
            "(67, 69): reprojection: 0.324926, disparity: 0.065493\n",
            "(68, 69): reprojection: 0.210515, disparity: 0.060631\n",
            "(68, 70): reprojection: 0.447903, disparity: 0.067282\n",
            "(68, 72): reprojection: 0.580989, disparity: 0.073893\n",
            "(68, 76): reprojection: 0.815565, disparity: 0.083246\n",
            "(69, 70): reprojection: 0.301880, disparity: 0.061415\n",
            "(69, 71): reprojection: 0.426849, disparity: 0.062419\n",
            "(70, 71): reprojection: 0.226081, disparity: 0.058926\n",
            "(70, 72): reprojection: 0.308921, disparity: 0.062367\n",
            "(70, 74): reprojection: 0.484540, disparity: 0.071993\n",
            "(71, 72): reprojection: 0.229032, disparity: 0.060457\n",
            "(71, 73): reprojection: 0.339376, disparity: 0.067105\n",
            "(72, 73): reprojection: 0.223499, disparity: 0.061284\n",
            "(72, 74): reprojection: 0.330647, disparity: 0.066430\n",
            "(72, 76): reprojection: 0.442400, disparity: 0.074267\n",
            "(72, 80): reprojection: 0.825639, disparity: 0.087586\n",
            "(72, 88): reprojection: 1.001778, disparity: 0.089860\n",
            "(73, 74): reprojection: 0.330355, disparity: 0.061536\n",
            "(73, 75): reprojection: 0.461306, disparity: 0.065611\n",
            "(74, 75): reprojection: 0.250841, disparity: 0.059759\n",
            "(74, 76): reprojection: 0.282715, disparity: 0.062456\n",
            "(74, 78): reprojection: 0.452596, disparity: 0.069168\n",
            "(75, 76): reprojection: 0.230925, disparity: 0.057935\n",
            "(75, 77): reprojection: 0.363480, disparity: 0.061619\n",
            "(76, 77): reprojection: 0.239498, disparity: 0.057716\n",
            "(76, 78): reprojection: 0.378731, disparity: 0.058281\n",
            "(76, 80): reprojection: 0.495577, disparity: 0.067749\n",
            "(76, 84): reprojection: 0.723655, disparity: 0.085755\n",
            "(77, 78): reprojection: 0.244275, disparity: 0.058317\n",
            "(77, 79): reprojection: 0.272600, disparity: 0.061518\n",
            "(78, 79): reprojection: 0.196097, disparity: 0.059673\n",
            "(78, 80): reprojection: 0.249941, disparity: 0.064694\n",
            "(78, 82): reprojection: 0.398104, disparity: 0.068460\n",
            "(79, 80): reprojection: 0.150891, disparity: 0.057890\n",
            "(79, 81): reprojection: 0.258344, disparity: 0.061238\n",
            "(80, 81): reprojection: 0.159930, disparity: 0.056394\n",
            "(80, 82): reprojection: 0.280561, disparity: 0.062062\n",
            "(80, 84): reprojection: 0.447215, disparity: 0.066742\n",
            "(80, 88): reprojection: 0.729261, disparity: 0.082362\n",
            "(81, 82): reprojection: 0.179564, disparity: 0.055291\n",
            "(81, 83): reprojection: 0.275402, disparity: 0.062061\n",
            "(82, 83): reprojection: 0.143998, disparity: 0.055040\n",
            "(82, 84): reprojection: 0.246608, disparity: 0.062657\n",
            "(82, 86): reprojection: 0.425771, disparity: 0.069536\n",
            "(83, 84): reprojection: 0.140161, disparity: 0.055386\n",
            "(83, 85): reprojection: 0.292466, disparity: 0.063600\n",
            "(84, 85): reprojection: 0.171614, disparity: 0.054535\n",
            "(84, 86): reprojection: 0.253431, disparity: 0.060973\n",
            "(84, 88): reprojection: 0.494192, disparity: 0.067575\n",
            "(85, 86): reprojection: 0.164812, disparity: 0.052902\n",
            "(85, 87): reprojection: 0.279087, disparity: 0.058062\n",
            "(86, 87): reprojection: 0.231185, disparity: 0.054286\n",
            "(86, 88): reprojection: 0.338073, disparity: 0.064686\n",
            "(86, 90): reprojection: 0.511046, disparity: 0.069204\n",
            "(87, 88): reprojection: 0.182854, disparity: 0.054920\n",
            "(87, 89): reprojection: 0.290942, disparity: 0.060905\n",
            "(88, 89): reprojection: 0.146334, disparity: 0.051334\n",
            "(88, 90): reprojection: 0.310082, disparity: 0.056806\n",
            "(89, 90): reprojection: 0.214203, disparity: 0.050136\n",
            "(89, 91): reprojection: 0.418329, disparity: 0.060846\n",
            "(90, 91): reprojection: 0.329909, disparity: 0.055277\n",
            "Mean:     reprojection: 0.329909, disparity: 0.055277\n",
            "Done Validation for epoch 20 (5200 iterations)\n",
            "Finished Training\n",
            "\n",
            "*******************************\n",
            "****  Compute final depth  ****\n",
            "*******************************\n",
            "\n",
            "***************************************\n",
            "****  Export visualization videos  ****\n",
            "***************************************\n",
            "2021-07-15 05:50:08,877 - INFO - Make videos Namespace(align=16, batch_size=4, camera_model='SIMPLE_PINHOLE', camera_params='1671.770118, 540, 960', colmap_bin_path='colmap', color_dir='results/ayush/color_down_png', configure='default', dense_frame_ratio=0.95, dense_pixel_ratio=0.3, depth_dirs=['results/ayush/depth_mc', 'results/ayush/depth_colmap_dense', 'results/ayush/R_hierarchical2_mc/B0.1_R1.0_PL1-0_LR0.0004_BS4_Oadam/depth'], display_freq=100, ext='.mp4', ffmpeg='ffmpeg', flow_checkpoint='FlowNet2', flow_ops=['hierarchical2'], frame_fmt='frame_%06d.png', frame_range=NamedOptionalSet(name='', set=<utils.frame_range.OptionalSet object at 0x7fe793046400>), initialize_pose=False, lambda_parameter=0, lambda_reprojection=1.0, lambda_view_baseline=0.1, learning_rate=0.0004, log_dir=None, make_video=True, matcher='exhaustive', model_type='mc', num_epochs=20, op='all', optimizer='Adam', out_dir='results/ayush/R_hierarchical2_mc/videos', overlap_ratio=0.2, path='results/ayush', print_freq=1, refine_intrinsics=False, save_epoch_freq=1, size=384, sparse=False, val_epoch_freq=1, video3d_dir=None, video_file='data/videos/ayush.mp4')\n",
            "ffmpeg version 3.4 Copyright (c) 2000-2017 the FFmpeg developers\n",
            "  built with gcc 7.2.0 (crosstool-NG fa8859cb)\n",
            "  configuration: --prefix=/opt/conda/conda-bld/ffmpeg_1530807717919/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh --disable-doc --enable-shared --extra-cflags='-fPIC -I/opt/conda/conda-bld/ffmpeg_1530807717919/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh/include' --extra-cxxflags='=-fPIC' --extra-libs='-L/opt/conda/conda-bld/ffmpeg_1530807717919/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh/lib -lz' --enable-pic --disable-static --disable-gpl --disable-nonfree --disable-openssl --enable-libvpx --cc=/opt/conda/conda-bld/ffmpeg_1530807717919/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --cxx=/opt/conda/conda-bld/ffmpeg_1530807717919/_build_env/bin/x86_64-conda_cos6-linux-gnu-c++ --enable-libopus\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "Input #0, image2, from 'results/ayush/color_down_png/frame_%06d.png':\n",
            "  Duration: 00:00:03.68, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 224x384, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "\u001b[4;31mUnknown encoder 'libx264'\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"main.py\", line 13, in <module>\n",
            "    dp.process(params)\n",
            "  File \"/content/consistent_depth/process.py\", line 117, in process\n",
            "    return self.pipeline(params)\n",
            "  File \"/content/consistent_depth/process.py\", line 97, in pipeline\n",
            "    self.make_videos(params, ft.out_dir)\n",
            "  File \"/content/consistent_depth/process.py\", line 141, in make_videos\n",
            "    mkvid.main(vid_params)\n",
            "  File \"/content/consistent_depth/tools/make_video.py\", line 249, in main\n",
            "    ext=args.ext,\n",
            "  File \"/content/consistent_depth/tools/make_video.py\", line 141, in make_video\n",
            "    print(subprocess.run(cmd, check=True))\n",
            "  File \"/usr/local/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-r', '30', '-i', 'results/ayush/color_down_png/frame_%06d.png', '-vcodec', 'libx264', '-pix_fmt', 'yuv420p', '-crf', '1', '-vf', 'pad=ceil(iw/2)*2:ceil(ih/2)*2', 'results/ayush/R_hierarchical2_mc/videos/color.mp4']' returned non-zero exit status 1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "FA2CB9dWwCaN",
        "outputId": "3964b3c5-b54d-4f0c-c200-643857f09837"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "image_folder = 'image'\n",
        "video_name = 'video.avi'\n",
        "\n",
        "images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
        "frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
        "height, width, layers = frame.shape\n",
        "\n",
        "video = cv2.VideoWriter(video_name, 0, 1, (width,height))\n",
        "\n",
        "for image in images:\n",
        "    video.write(cv2.imread(os.path.join(image_folder, image)))\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "video.release()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-cb8dd5f0f687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvideo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'video.avi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSZhxQDUzN6k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}